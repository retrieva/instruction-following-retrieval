/home/pj25000118/ku50001652/works/2025_internship/.venv/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2025-09-19 11:01:48,583][accelerate.utils.other][WARNING] - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2025-09-19 11:01:50,374] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-19 11:01:50,491][root][INFO] - cc -pthread -fno-strict-overflow -Wsign-compare -Wunreachable-code -DNDEBUG -g -O3 -Wall -fPIC -fPIC -c /tmp/3682034_SjAxLWNvbi1iYXNpYy5zaAo=/tmpmhafz8k6/test.c -o /tmp/3682034_SjAxLWNvbi1iYXNpYy5zaAo=/tmpmhafz8k6/test.o
[2025-09-19 11:01:50,509][root][INFO] - cc -pthread /tmp/3682034_SjAxLWNvbi1iYXNpYy5zaAo=/tmpmhafz8k6/test.o -laio -o /tmp/3682034_SjAxLWNvbi1iYXNpYy5zaAo=/tmpmhafz8k6/a.out
[2025-09-19 11:01:50,866][root][INFO] - cc -pthread -fno-strict-overflow -Wsign-compare -Wunreachable-code -DNDEBUG -g -O3 -Wall -fPIC -fPIC -c /tmp/3682034_SjAxLWNvbi1iYXNpYy5zaAo=/tmpqw3men5l/test.c -o /tmp/3682034_SjAxLWNvbi1iYXNpYy5zaAo=/tmpqw3men5l/test.o
[2025-09-19 11:01:50,885][root][INFO] - cc -pthread /tmp/3682034_SjAxLWNvbi1iYXNpYy5zaAo=/tmpqw3men5l/test.o -L/home/app/cuda/12.2.2 -L/home/app/cuda/12.2.2/lib64 -lcufile -o /tmp/3682034_SjAxLWNvbi1iYXNpYy5zaAo=/tmpqw3men5l/a.out
[2025-09-19 11:01:52,962] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|          | 10/3270 [00:06<34:18,  1.58it/s] 
対照学習損失: 1.399559497833252
対照学習損失: 1.1182777881622314
{'loss': 1.2589, 'grad_norm': 2.730574131011963, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
対照学習損失: 1.1397135257720947
対照学習損失: 1.0324218273162842
{'loss': 1.0861, 'grad_norm': 2.549834728240967, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
対照学習損失: 1.252151370048523
対照学習損失: 1.4072529077529907
{'loss': 1.3297, 'grad_norm': 2.826798915863037, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
対照学習損失: 1.2444790601730347
対照学習損失: 1.3796732425689697
{'loss': 1.3121, 'grad_norm': 2.7828731536865234, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
対照学習損失: 1.1391918659210205
対照学習損失: 1.2886838912963867
{'loss': 1.2139, 'grad_norm': 2.675480365753174, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
対照学習損失: 1.250387191772461
対照学習損失: 1.5938682556152344
{'loss': 1.4221, 'grad_norm': 2.806443214416504, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.01}
対照学習損失: 0.9187692403793335
対照学習損失: 1.169844150543213
{'loss': 1.0443, 'grad_norm': 2.4965903759002686, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.01}
対照学習損失: 1.816049337387085
対照学習損失: 1.1975986957550049
{'loss': 1.5068, 'grad_norm': 3.1620538234710693, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.01}
対照学習損失: 1.4503833055496216
対照学習損失: 1.2639410495758057
{'loss': 1.3572, 'grad_norm': 2.813535690307617, 'learning_rate': 9e-07, 'epoch': 0.01}
対照学習損失: 1.0611343383789062
対照学習損失: 1.144648790359497
{'loss': 1.1029, 'grad_norm': 2.552473783493042, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}
対照学習損失: 1.2424131631851196
対照学習損失: 1.1595677137374878
{'loss': 1.201, 'grad_norm': 2.603012800216675, 'learning_rate': 1.1e-06, 'epoch': 0.01}
対照学習損失: 1.1877264976501465
対照学習損失: 1.3019999265670776
{'loss': 1.2449, 'grad_norm': 2.5704238414764404, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.01}
対照学習損失: 1.426574945449829
対照学習損失: 1.2697844505310059
{'loss': 1.3482, 'grad_norm': 2.662963628768921, 'learning_rate': 1.3e-06, 'epoch': 0.01}
対照学習損失: 1.3166619539260864
対照学習損失: 1.0720717906951904
{'loss': 1.1944, 'grad_norm': 2.513702392578125, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.01}
対照学習損失: 1.0267672538757324
対照学習損失: 1.4536871910095215
{'loss': 1.2402, 'grad_norm': 3.29575514793396, 'learning_rate': 1.5e-06, 'epoch': 0.01}
対照学習損失: 1.1380040645599365
対照学習損失: 1.2281584739685059
{'loss': 1.1831, 'grad_norm': 2.9831230640411377, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.01}
対照学習損失: 1.2263553142547607
対照学習損失: 1.388610601425171
{'loss': 1.3075, 'grad_norm': 2.712742567062378, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.02}
対照学習損失: 1.2773411273956299
対照学習損失: 1.1948515176773071
{'loss': 1.2361, 'grad_norm': 2.6752307415008545, 'learning_rate': 1.8e-06, 'epoch': 0.02}
対照学習損失: 1.0428862571716309
対照学習損失: 1.5003299713134766
{'loss': 1.2716, 'grad_norm': 2.663748264312744, 'learning_rate': 1.9e-06, 'epoch': 0.02}
対照学習損失: 1.3461105823516846
対照学習損失: 1.4343717098236084
{'loss': 1.3902, 'grad_norm': 2.7082552909851074, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02}
対照学習損失: 1.0056843757629395
対照学習損失: 1.1953253746032715
{'loss': 1.1005, 'grad_norm': 2.6707065105438232, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.02}
対照学習損失: 1.0904152393341064
対照学習損失: 1.201817274093628
{'loss': 1.1461, 'grad_norm': 2.5391054153442383, 'learning_rate': 2.2e-06, 'epoch': 0.02}
対照学習損失: 1.3721299171447754
対照学習損失: 1.120086431503296
{'loss': 1.2461, 'grad_norm': 3.0995683670043945, 'learning_rate': 2.3e-06, 'epoch': 0.02}
対照学習損失: 1.2019015550613403
対照学習損失: 1.2712528705596924
{'loss': 1.2366, 'grad_norm': 2.768120050430298, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.02}
対照学習損失: 1.4927732944488525
対照学習損失: 1.0071349143981934
{'loss': 1.25, 'grad_norm': 2.694119691848755, 'learning_rate': 2.5e-06, 'epoch': 0.02}
対照学習損失: 0.9775912165641785
対照学習損失: 1.2441929578781128
{'loss': 1.1109, 'grad_norm': 2.502614736557007, 'learning_rate': 2.6e-06, 'epoch': 0.02}
対照学習損失: 1.2119863033294678
対照学習損失: 1.2257676124572754
{'loss': 1.2189, 'grad_norm': 2.7150518894195557, 'learning_rate': 2.7e-06, 'epoch': 0.02}
対照学習損失: 1.2075095176696777
対照学習損失: 1.276063323020935
{'loss': 1.2418, 'grad_norm': 2.7303826808929443, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.03}
対照学習損失: 1.2790191173553467
対照学習損失: 1.052269697189331
{'loss': 1.1656, 'grad_norm': 2.7686426639556885, 'learning_rate': 2.9e-06, 'epoch': 0.03}
対照学習損失: 1.4323478937149048
対照学習損失: 1.3018434047698975
{'loss': 1.3671, 'grad_norm': 3.0359089374542236, 'learning_rate': 3e-06, 'epoch': 0.03}
対照学習損失: 1.0780181884765625
対照学習損失: 0.9249382019042969
{'loss': 1.0015, 'grad_norm': 2.6579883098602295, 'learning_rate': 3.1e-06, 'epoch': 0.03}
対照学習損失: 1.42218017578125
対照学習損失: 1.1308335065841675
{'loss': 1.2765, 'grad_norm': 2.696305513381958, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.03}
対照学習損失: 1.202742099761963
対照学習損失: 1.2619014978408813
{'loss': 1.2323, 'grad_norm': 2.674642562866211, 'learning_rate': 3.3e-06, 'epoch': 0.03}
対照学習損失: 1.3085728883743286
対照学習損失: 1.3539161682128906
{'loss': 1.3312, 'grad_norm': 2.992870330810547, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.03}
対照学習損失: 1.0782225131988525
対照学習損失: 1.0740673542022705
{'loss': 1.0761, 'grad_norm': 2.7334625720977783, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.03}
対照学習損失: 1.094738245010376
対照学習損失: 1.140844464302063
{'loss': 1.1178, 'grad_norm': 2.767519235610962, 'learning_rate': 3.6e-06, 'epoch': 0.03}
対照学習損失: 1.3060803413391113
対照学習損失: 1.1153966188430786
{'loss': 1.2107, 'grad_norm': 2.912461996078491, 'learning_rate': 3.7e-06, 'epoch': 0.03}
対照学習損失: 1.085820198059082
対照学習損失: 1.235434889793396
{'loss': 1.1606, 'grad_norm': 2.8037095069885254, 'learning_rate': 3.8e-06, 'epoch': 0.03}
対照学習損失: 1.2233375310897827
対照学習損失: 0.983298659324646
{'loss': 1.1033, 'grad_norm': 2.7218868732452393, 'learning_rate': 3.9e-06, 'epoch': 0.04}
対照学習損失: 0.9206498265266418
対照学習損失: 0.9967361092567444
{'loss': 0.9587, 'grad_norm': 2.6858205795288086, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.04}
対照学習損失: 1.0888795852661133
対照学習損失: 1.7089241743087769
{'loss': 1.3989, 'grad_norm': 3.0728704929351807, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.04}
対照学習損失: 0.9912278056144714
対照学習損失: 0.9510301351547241
{'loss': 0.9711, 'grad_norm': 2.608973264694214, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.04}
対照学習損失: 1.3499999046325684
対照学習損失: 1.1176280975341797
{'loss': 1.2338, 'grad_norm': 3.1976687908172607, 'learning_rate': 4.2999999999999995e-06, 'epoch': 0.04}
対照学習損失: 1.004761815071106
対照学習損失: 1.2341784238815308
{'loss': 1.1195, 'grad_norm': 2.88187575340271, 'learning_rate': 4.4e-06, 'epoch': 0.04}
対照学習損失: 0.987378716468811
対照学習損失: 1.2196733951568604
{'loss': 1.1035, 'grad_norm': 2.5025718212127686, 'learning_rate': 4.5e-06, 'epoch': 0.04}
対照学習損失: 1.1429214477539062
対照学習損失: 0.7805085182189941
{'loss': 0.9617, 'grad_norm': 2.8058791160583496, 'learning_rate': 4.6e-06, 'epoch': 0.04}
対照学習損失: 1.0846675634384155
対照学習損失: 1.0385284423828125
{'loss': 1.0616, 'grad_norm': 2.77034854888916, 'learning_rate': 4.7e-06, 'epoch': 0.04}
対照学習損失: 1.1576619148254395
対照学習損失: 1.2919511795043945
{'loss': 1.2248, 'grad_norm': 3.050865888595581, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.04}
対照学習損失: 0.8905770778656006
対照学習損失: 1.1429303884506226
{'loss': 1.0168, 'grad_norm': 2.924914836883545, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.04}
対照学習損失: 0.9309489130973816
対照学習損失: 1.3669036626815796
{'loss': 1.1489, 'grad_norm': 3.305464267730713, 'learning_rate': 5e-06, 'epoch': 0.05}
対照学習損失: 1.011513113975525
対照学習損失: 1.2904164791107178
{'loss': 1.151, 'grad_norm': 3.5810766220092773, 'learning_rate': 5.1e-06, 'epoch': 0.05}
対照学習損失: 0.9336264729499817
対照学習損失: 0.9410024881362915
{'loss': 0.9373, 'grad_norm': 2.770979881286621, 'learning_rate': 5.2e-06, 'epoch': 0.05}
対照学習損失: 0.8579462170600891
対照学習損失: 1.097715139389038
{'loss': 0.9778, 'grad_norm': 2.9173080921173096, 'learning_rate': 5.3e-06, 'epoch': 0.05}
対照学習損失: 1.1270642280578613
対照学習損失: 0.9288211464881897
{'loss': 1.0279, 'grad_norm': 2.969257116317749, 'learning_rate': 5.4e-06, 'epoch': 0.05}
対照学習損失: 1.0035839080810547
対照学習損失: 1.1118558645248413
{'loss': 1.0577, 'grad_norm': 3.107539653778076, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.05}
対照学習損失: 0.8339488506317139
対照学習損失: 1.029860019683838
{'loss': 0.9319, 'grad_norm': 3.2399349212646484, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.05}
対照学習損失: 0.9637677073478699
対照学習損失: 0.9651039838790894
{'loss': 0.9644, 'grad_norm': 3.0343997478485107, 'learning_rate': 5.7000000000000005e-06, 'epoch': 0.05}
対照学習損失: 0.833702564239502
対照学習損失: 1.130195140838623
{'loss': 0.9819, 'grad_norm': 2.846433639526367, 'learning_rate': 5.8e-06, 'epoch': 0.05}
対照学習損失: 1.0259729623794556
対照学習損失: 1.0315474271774292
{'loss': 1.0288, 'grad_norm': 3.4758877754211426, 'learning_rate': 5.9e-06, 'epoch': 0.05}
対照学習損失: 1.0329046249389648
対照学習損失: 0.9205542802810669
{'loss': 0.9767, 'grad_norm': 3.1628873348236084, 'learning_rate': 6e-06, 'epoch': 0.06}
対照学習損失: 1.2344337701797485
対照学習損失: 0.8561386466026306
{'loss': 1.0453, 'grad_norm': 3.099050521850586, 'learning_rate': 6.1e-06, 'epoch': 0.06}
対照学習損失: 0.8551174402236938
対照学習損失: 1.201311469078064
{'loss': 1.0282, 'grad_norm': 3.103250503540039, 'learning_rate': 6.2e-06, 'epoch': 0.06}
対照学習損失: 0.8929555416107178
対照学習損失: 1.195396900177002
{'loss': 1.0442, 'grad_norm': 3.207897901535034, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.06}
対照学習損失: 1.0339126586914062
対照学習損失: 0.8631076812744141
{'loss': 0.9485, 'grad_norm': 3.068641424179077, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.06}
対照学習損失: 0.7689439058303833
対照学習損失: 0.7173997163772583
{'loss': 0.7432, 'grad_norm': 2.6216580867767334, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.06}
対照学習損失: 0.816234827041626
対照学習損失: 0.6886277794837952
{'loss': 0.7524, 'grad_norm': 2.9374234676361084, 'learning_rate': 6.6e-06, 'epoch': 0.06}
対照学習損失: 0.9643138647079468
対照学習損失: 0.6660585403442383
{'loss': 0.8152, 'grad_norm': 3.359467029571533, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.06}
対照学習損失: 0.8612920045852661
対照学習損失: 0.6274926662445068
{'loss': 0.7444, 'grad_norm': 3.3440401554107666, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.06}
対照学習損失: 0.8532348871231079
対照学習損失: 0.8317983150482178
{'loss': 0.8425, 'grad_norm': 3.025806427001953, 'learning_rate': 6.900000000000001e-06, 'epoch': 0.06}
対照学習損失: 0.6953887343406677
対照学習損失: 1.209862232208252
{'loss': 0.9526, 'grad_norm': 3.2382984161376953, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.06}
対照学習損失: 0.6609858870506287
対照学習損失: 0.6506120562553406
{'loss': 0.6558, 'grad_norm': 3.022270441055298, 'learning_rate': 7.1e-06, 'epoch': 0.07}
対照学習損失: 0.6576931476593018
対照学習損失: 0.6806740164756775
{'loss': 0.6692, 'grad_norm': 2.82692289352417, 'learning_rate': 7.2e-06, 'epoch': 0.07}
対照学習損失: 0.7177614569664001
対照学習損失: 0.9213008880615234
{'loss': 0.8195, 'grad_norm': 3.3639402389526367, 'learning_rate': 7.2999999999999996e-06, 'epoch': 0.07}
対照学習損失: 0.7372949719429016
対照学習損失: 1.0298032760620117
{'loss': 0.8835, 'grad_norm': 3.352257251739502, 'learning_rate': 7.4e-06, 'epoch': 0.07}
対照学習損失: 0.6458889245986938
対照学習損失: 0.5844334363937378
{'loss': 0.6152, 'grad_norm': 2.994915723800659, 'learning_rate': 7.5e-06, 'epoch': 0.07}
対照学習損失: 0.8713581562042236
対照学習損失: 0.4769023060798645
{'loss': 0.6741, 'grad_norm': 3.0235989093780518, 'learning_rate': 7.6e-06, 'epoch': 0.07}
対照学習損失: 0.5698107481002808
対照学習損失: 0.7967056632041931
{'loss': 0.6833, 'grad_norm': 2.7257981300354004, 'learning_rate': 7.7e-06, 'epoch': 0.07}
対照学習損失: 0.961122989654541
対照学習損失: 0.799538254737854
{'loss': 0.8803, 'grad_norm': 5.1329026222229, 'learning_rate': 7.8e-06, 'epoch': 0.07}
対照学習損失: 0.6220422983169556
対照学習損失: 0.5776869654655457
{'loss': 0.5999, 'grad_norm': 3.218376874923706, 'learning_rate': 7.9e-06, 'epoch': 0.07}
対照学習損失: 0.578441321849823
対照学習損失: 0.7323544025421143
{'loss': 0.6554, 'grad_norm': 3.624344825744629, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.07}
対照学習損失: 0.7208304405212402
対照学習損失: 0.5513926148414612
{'loss': 0.6361, 'grad_norm': 2.780916690826416, 'learning_rate': 8.1e-06, 'epoch': 0.07}
対照学習損失: 0.666722297668457
対照学習損失: 0.6440724730491638
{'loss': 0.6554, 'grad_norm': 3.2680976390838623, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.08}
対照学習損失: 0.505640983581543
対照学習損失: 0.48220115900039673
{'loss': 0.4939, 'grad_norm': 2.7619762420654297, 'learning_rate': 8.3e-06, 'epoch': 0.08}
対照学習損失: 0.6747855544090271
対照学習損失: 0.6862609386444092
{'loss': 0.6805, 'grad_norm': 3.5451419353485107, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.08}
対照学習損失: 0.5075059533119202
対照学習損失: 0.6270244717597961
{'loss': 0.5673, 'grad_norm': 3.3572287559509277, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.08}
対照学習損失: 0.6048252582550049
対照学習損失: 0.8105379939079285
{'loss': 0.7077, 'grad_norm': 3.304821729660034, 'learning_rate': 8.599999999999999e-06, 'epoch': 0.08}
対照学習損失: 0.7402370572090149
対照学習損失: 0.5421686172485352
{'loss': 0.6412, 'grad_norm': 3.390749216079712, 'learning_rate': 8.7e-06, 'epoch': 0.08}
対照学習損失: 0.5207509994506836
対照学習損失: 0.38360297679901123
{'loss': 0.4522, 'grad_norm': 2.933596611022949, 'learning_rate': 8.8e-06, 'epoch': 0.08}
対照学習損失: 0.490237832069397
対照学習損失: 0.6409420967102051
{'loss': 0.5656, 'grad_norm': 3.120854139328003, 'learning_rate': 8.9e-06, 'epoch': 0.08}
対照学習損失: 0.4986686706542969
対照学習損失: 0.5210695266723633
{'loss': 0.5099, 'grad_norm': 3.0239250659942627, 'learning_rate': 9e-06, 'epoch': 0.08}
対照学習損失: 0.48667582869529724
対照学習損失: 0.4191379249095917
{'loss': 0.4529, 'grad_norm': nan, 'learning_rate': 9e-06, 'epoch': 0.08}
対照学習損失: 0.4641317129135132
対照学習損失: 0.5230444669723511
{'loss': 0.4936, 'grad_norm': 2.8730287551879883, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.08}
対照学習損失: 0.6197097301483154
対照学習損失: 0.34526336193084717
{'loss': 0.4825, 'grad_norm': 3.451556444168091, 'learning_rate': 9.2e-06, 'epoch': 0.09}
対照学習損失: 0.38165003061294556
対照学習損失: 0.4178057014942169
{'loss': 0.3997, 'grad_norm': 2.8879048824310303, 'learning_rate': 9.3e-06, 'epoch': 0.09}
対照学習損失: 0.3755459189414978
対照学習損失: 0.4471646547317505
{'loss': 0.4114, 'grad_norm': 2.466379404067993, 'learning_rate': 9.4e-06, 'epoch': 0.09}
対照学習損失: 0.2697962522506714
対照学習損失: 0.33608368039131165
{'loss': 0.3029, 'grad_norm': 2.668762445449829, 'learning_rate': 9.5e-06, 'epoch': 0.09}
対照学習損失: 0.25324082374572754
対照学習損失: 0.4460858702659607
{'loss': 0.3497, 'grad_norm': 2.5461251735687256, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.09}
対照学習損失: 0.17865708470344543
対照学習損失: 0.30814307928085327
{'loss': 0.2434, 'grad_norm': 1.935991644859314, 'learning_rate': 9.7e-06, 'epoch': 0.09}
対照学習損失: 0.26513952016830444
対照学習損失: 0.3189099133014679
{'loss': 0.292, 'grad_norm': 2.398454189300537, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.09}
対照学習損失: 0.2908267080783844
対照学習損失: 0.36119166016578674
{'loss': 0.326, 'grad_norm': 2.7482762336730957, 'learning_rate': 9.900000000000002e-06, 'epoch': 0.09}
対照学習損失: 0.2828744947910309
対照学習損失: 0.26132073998451233
{'loss': 0.2721, 'grad_norm': 2.520535707473755, 'learning_rate': 1e-05, 'epoch': 0.09}
対照学習損失: 0.29241618514060974
対照学習損失: 0.25537848472595215
{'loss': 0.2739, 'grad_norm': 2.661062479019165, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.09}
対照学習損失: 0.13472893834114075
対照学習損失: 0.31858572363853455
{'loss': 0.2267, 'grad_norm': 2.334759473800659, 'learning_rate': 1.02e-05, 'epoch': 0.09}
対照学習損失: 0.18289342522621155
対照学習損失: 0.17071707546710968
{'loss': 0.1768, 'grad_norm': 1.765905499458313, 'learning_rate': 1.03e-05, 'epoch': 0.1}
対照学習損失: 0.25916287302970886
対照学習損失: 0.1998409926891327
{'loss': 0.2295, 'grad_norm': 1.6822285652160645, 'learning_rate': 1.04e-05, 'epoch': 0.1}
対照学習損失: 0.21462181210517883
対照学習損失: 0.21767234802246094
{'loss': 0.2161, 'grad_norm': 1.5737208127975464, 'learning_rate': 1.05e-05, 'epoch': 0.1}
対照学習損失: 0.1565237045288086
対照学習損失: 0.17657123506069183
{'loss': 0.1665, 'grad_norm': 1.8657554388046265, 'learning_rate': 1.06e-05, 'epoch': 0.1}
対照学習損失: 0.19030699133872986
対照学習損失: 0.14995220303535461
{'loss': 0.1701, 'grad_norm': 1.706394910812378, 'learning_rate': 1.0700000000000001e-05, 'epoch': 0.1}
対照学習損失: 0.17640310525894165
対照学習損失: 0.22689945995807648
{'loss': 0.2017, 'grad_norm': 2.1304447650909424, 'learning_rate': 1.08e-05, 'epoch': 0.1}
対照学習損失: 0.16819000244140625
対照学習損失: 0.2733766734600067
{'loss': 0.2208, 'grad_norm': 2.4107632637023926, 'learning_rate': 1.09e-05, 'epoch': 0.1}
対照学習損失: 0.14422769844532013
対照学習損失: 0.17628201842308044
{'loss': 0.1603, 'grad_norm': 1.8539791107177734, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.1}
対照学習損失: 0.16178560256958008
対照学習損失: 0.11508867889642715
{'loss': 0.1384, 'grad_norm': 1.677670955657959, 'learning_rate': 1.11e-05, 'epoch': 0.1}
対照学習損失: 0.07434538751840591
対照学習損失: 0.2777574062347412
{'loss': 0.1761, 'grad_norm': 3.6350181102752686, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.1}
対照学習損失: 0.17488287389278412
対照学習損失: 0.13474085927009583
{'loss': 0.1548, 'grad_norm': 2.2008888721466064, 'learning_rate': 1.13e-05, 'epoch': 0.1}
対照学習損失: 0.14707818627357483
対照学習損失: 0.11869741231203079
{'loss': 0.1329, 'grad_norm': 1.7698352336883545, 'learning_rate': 1.1400000000000001e-05, 'epoch': 0.11}
対照学習損失: 0.14573073387145996
対照学習損失: 0.26910191774368286
{'loss': 0.2074, 'grad_norm': 2.2895431518554688, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.11}
対照学習損失: 0.12538309395313263
対照学習損失: 0.095841184258461
{'loss': 0.1106, 'grad_norm': 1.2539491653442383, 'learning_rate': 1.16e-05, 'epoch': 0.11}
対照学習損失: 0.07374560832977295
対照学習損失: 0.14522874355316162
{'loss': 0.1095, 'grad_norm': 1.8384947776794434, 'learning_rate': 1.1700000000000001e-05, 'epoch': 0.11}
対照学習損失: 0.09938094019889832
対照学習損失: 0.18246908485889435
{'loss': 0.1409, 'grad_norm': 2.3100695610046387, 'learning_rate': 1.18e-05, 'epoch': 0.11}
対照学習損失: 0.10289786756038666
対照学習損失: 0.16642791032791138
{'loss': 0.1347, 'grad_norm': 1.9512128829956055, 'learning_rate': 1.19e-05, 'epoch': 0.11}
対照学習損失: 0.12648430466651917
対照学習損失: 0.05075938627123833
{'loss': 0.0886, 'grad_norm': 1.15106999874115, 'learning_rate': 1.2e-05, 'epoch': 0.11}
対照学習損失: 0.15326662361621857
対照学習損失: 0.11539777368307114
{'loss': 0.1343, 'grad_norm': 1.8901259899139404, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.11}
対照学習損失: 0.28581205010414124
対照学習損失: 0.10053753852844238
{'loss': 0.1932, 'grad_norm': 2.421682119369507, 'learning_rate': 1.22e-05, 'epoch': 0.11}
対照学習損失: 0.16212064027786255
対照学習損失: 0.06385770440101624
{'loss': 0.113, 'grad_norm': 2.106553792953491, 'learning_rate': 1.23e-05, 'epoch': 0.11}
対照学習損失: 0.07794225215911865
対照学習損失: 0.057794272899627686
{'loss': 0.0679, 'grad_norm': 1.1317827701568604, 'learning_rate': 1.24e-05, 'epoch': 0.11}
対照学習損失: 0.07473897933959961
対照学習損失: 0.09609562158584595
{'loss': 0.0854, 'grad_norm': 1.9533582925796509, 'learning_rate': 1.25e-05, 'epoch': 0.12}
対照学習損失: 0.044989921152591705
対照学習損失: 0.033159591257572174
{'loss': 0.0391, 'grad_norm': 0.7131149768829346, 'learning_rate': 1.2600000000000001e-05, 'epoch': 0.12}
対照学習損失: 0.19684070348739624
対照学習損失: 0.08249659836292267
{'loss': 0.1397, 'grad_norm': 2.4993505477905273, 'learning_rate': 1.27e-05, 'epoch': 0.12}
対照学習損失: 0.1411437690258026
対照学習損失: 0.03846929967403412
{'loss': 0.0898, 'grad_norm': 1.2662409543991089, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.12}
対照学習損失: 0.060682546347379684
対照学習損失: 0.11721298098564148
{'loss': 0.0889, 'grad_norm': 1.3944849967956543, 'learning_rate': 1.29e-05, 'epoch': 0.12}
対照学習損失: 0.038353219628334045
対照学習損失: 0.054873690009117126
{'loss': 0.0466, 'grad_norm': 0.8473711609840393, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.12}
対照学習損失: 0.20785391330718994
対照学習損失: 0.016068818047642708
{'loss': 0.112, 'grad_norm': 2.7170462608337402, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.12}
対照学習損失: 0.0957990288734436
対照学習損失: 0.1529468148946762
{'loss': 0.1244, 'grad_norm': 2.0951833724975586, 'learning_rate': 1.32e-05, 'epoch': 0.12}
対照学習損失: 0.05725138634443283
対照学習損失: 0.11178921908140182
{'loss': 0.0845, 'grad_norm': 1.6184076070785522, 'learning_rate': 1.3300000000000001e-05, 'epoch': 0.12}
対照学習損失: 0.08925289660692215
対照学習損失: 0.023532861843705177
{'loss': 0.0564, 'grad_norm': 1.2034318447113037, 'learning_rate': 1.3400000000000002e-05, 'epoch': 0.12}
対照学習損失: 0.28612130880355835
対照学習損失: 0.08031903207302094
{'loss': 0.1832, 'grad_norm': 3.4327619075775146, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.12}
対照学習損失: 0.0558643564581871
対照学習損失: 0.07254175841808319
{'loss': 0.0642, 'grad_norm': 0.8785083889961243, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.13}
対照学習損失: 0.12032943218946457
対照学習損失: 0.06445035338401794
{'loss': 0.0924, 'grad_norm': 3.493173599243164, 'learning_rate': 1.3700000000000001e-05, 'epoch': 0.13}
対照学習損失: 0.2274230420589447
対照学習損失: 0.05698831379413605
{'loss': 0.1422, 'grad_norm': 2.1843254566192627, 'learning_rate': 1.3800000000000002e-05, 'epoch': 0.13}
対照学習損失: 0.016322869807481766
対照学習損失: 0.151297464966774
{'loss': 0.0838, 'grad_norm': 1.4652215242385864, 'learning_rate': 1.3900000000000002e-05, 'epoch': 0.13}
対照学習損失: 0.04198165610432625
対照学習損失: 0.024452056735754013
{'loss': 0.0332, 'grad_norm': 0.6754515767097473, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.13}
対照学習損失: 0.07924540340900421
対照学習損失: 0.012216221541166306
{'loss': 0.0457, 'grad_norm': 1.3899964094161987, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.13}
対照学習損失: 0.056942831724882126
対照学習損失: 0.02489612065255642
{'loss': 0.0409, 'grad_norm': 0.7887560725212097, 'learning_rate': 1.42e-05, 'epoch': 0.13}
対照学習損失: 0.02610914409160614
対照学習損失: 0.04857318848371506
{'loss': 0.0373, 'grad_norm': 0.4313015937805176, 'learning_rate': 1.43e-05, 'epoch': 0.13}
対照学習損失: 0.02555472031235695
対照学習損失: 0.07221996784210205
{'loss': 0.0489, 'grad_norm': 1.1514548063278198, 'learning_rate': 1.44e-05, 'epoch': 0.13}
対照学習損失: 0.02234075590968132
対照学習損失: 0.055472057312726974
{'loss': 0.0389, 'grad_norm': 0.6588969230651855, 'learning_rate': 1.45e-05, 'epoch': 0.13}
対照学習損失: 0.047799333930015564
対照学習損失: 0.03086298517882824
{'loss': 0.0393, 'grad_norm': 0.6054031848907471, 'learning_rate': 1.4599999999999999e-05, 'epoch': 0.13}
対照学習損失: 0.09330673515796661
対照学習損失: 0.040972042828798294
{'loss': 0.0671, 'grad_norm': 1.9908909797668457, 'learning_rate': 1.47e-05, 'epoch': 0.14}
対照学習損失: 0.21811310946941376
対照学習損失: 0.02155826985836029
{'loss': 0.1198, 'grad_norm': 2.0387163162231445, 'learning_rate': 1.48e-05, 'epoch': 0.14}
対照学習損失: 0.03521198779344559
対照学習損失: 0.018926147371530533
{'loss': 0.0271, 'grad_norm': 0.636117160320282, 'learning_rate': 1.49e-05, 'epoch': 0.14}
対照学習損失: 0.10770348459482193
対照学習損失: 0.10243004560470581
{'loss': 0.1051, 'grad_norm': 2.0937352180480957, 'learning_rate': 1.5e-05, 'epoch': 0.14}
対照学習損失: 0.021160922944545746
対照学習損失: 0.10530884563922882
{'loss': 0.0632, 'grad_norm': 1.1728068590164185, 'learning_rate': 1.51e-05, 'epoch': 0.14}
対照学習損失: 0.013228852301836014
対照学習損失: 0.020953210070729256
{'loss': 0.0171, 'grad_norm': 0.29708927869796753, 'learning_rate': 1.52e-05, 'epoch': 0.14}
対照学習損失: 0.038498975336551666
対照学習損失: 0.03536272794008255
{'loss': 0.0369, 'grad_norm': 0.6573880314826965, 'learning_rate': 1.53e-05, 'epoch': 0.14}
対照学習損失: 0.05058426409959793
対照学習損失: 0.015349853783845901
{'loss': 0.033, 'grad_norm': 0.6479547619819641, 'learning_rate': 1.54e-05, 'epoch': 0.14}
対照学習損失: 0.017766570672392845
対照学習損失: 0.013472290709614754
{'loss': 0.0156, 'grad_norm': 0.34483465552330017, 'learning_rate': 1.55e-05, 'epoch': 0.14}
対照学習損失: 0.039724767208099365
対照学習損失: 0.014638267457485199
{'loss': 0.0272, 'grad_norm': 0.6168773770332336, 'learning_rate': 1.56e-05, 'epoch': 0.14}
対照学習損失: 0.006528433412313461
対照学習損失: 0.010977670550346375
{'loss': 0.0088, 'grad_norm': 0.13514044880867004, 'learning_rate': 1.5700000000000002e-05, 'epoch': 0.14}
対照学習損失: 0.01613476313650608
対照学習損失: 0.03927641361951828
{'loss': 0.0277, 'grad_norm': 0.43429937958717346, 'learning_rate': 1.58e-05, 'epoch': 0.15}
対照学習損失: 0.07084061205387115
対照学習損失: 0.06213269755244255
{'loss': 0.0665, 'grad_norm': 1.147898554801941, 'learning_rate': 1.59e-05, 'epoch': 0.15}
対照学習損失: 0.0197325199842453
対照学習損失: 0.019368138164281845
{'loss': 0.0196, 'grad_norm': 0.4060176908969879, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.15}
対照学習損失: 0.03617435693740845
対照学習損失: 0.09252849221229553
{'loss': 0.0644, 'grad_norm': 1.0091100931167603, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.15}
対照学習損失: 0.05170430615544319
対照学習損失: 0.034954190254211426
{'loss': 0.0433, 'grad_norm': 0.894011914730072, 'learning_rate': 1.62e-05, 'epoch': 0.15}
対照学習損失: 0.02677571028470993
対照学習損失: 0.06669304519891739
{'loss': 0.0467, 'grad_norm': 0.6524558663368225, 'learning_rate': 1.63e-05, 'epoch': 0.15}
対照学習損失: 0.028613919392228127
対照学習損失: 0.03299230709671974
{'loss': 0.0308, 'grad_norm': 0.5064477324485779, 'learning_rate': 1.6400000000000002e-05, 'epoch': 0.15}
対照学習損失: 0.016396882012486458
対照学習損失: 0.020997080951929092
{'loss': 0.0187, 'grad_norm': 0.49297088384628296, 'learning_rate': 1.65e-05, 'epoch': 0.15}
対照学習損失: 0.05085715651512146
対照学習損失: 0.00869121216237545
{'loss': 0.0298, 'grad_norm': 0.6625755429267883, 'learning_rate': 1.66e-05, 'epoch': 0.15}
対照学習損失: 0.012956557795405388
対照学習損失: 0.016916830092668533
{'loss': 0.0149, 'grad_norm': 0.3382841944694519, 'learning_rate': 1.6700000000000003e-05, 'epoch': 0.15}
対照学習損失: 0.01855458691716194
対照学習損失: 0.02176845073699951
{'loss': 0.0202, 'grad_norm': 0.7140460014343262, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.15}
対照学習損失: 0.04471267759799957
対照学習損失: 0.0078824982047081
{'loss': 0.0263, 'grad_norm': 1.1999566555023193, 'learning_rate': 1.69e-05, 'epoch': 0.16}
対照学習損失: 0.01839091069996357
対照学習損失: 0.016974901780486107
{'loss': 0.0177, 'grad_norm': 0.36619672179222107, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.16}
対照学習損失: 0.1362992525100708
対照学習損失: 0.044552337378263474
{'loss': 0.0904, 'grad_norm': 3.3353772163391113, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.16}
対照学習損失: 0.011469215154647827
対照学習損失: 0.060247913002967834
{'loss': 0.0359, 'grad_norm': 1.1366910934448242, 'learning_rate': 1.7199999999999998e-05, 'epoch': 0.16}
対照学習損失: 0.014035506173968315
対照学習損失: 0.026381369680166245
{'loss': 0.0202, 'grad_norm': 0.5042950510978699, 'learning_rate': 1.73e-05, 'epoch': 0.16}
対照学習損失: 0.022145994007587433
対照学習損失: 0.009644903242588043
{'loss': 0.0159, 'grad_norm': 0.3564283847808838, 'learning_rate': 1.74e-05, 'epoch': 0.16}
対照学習損失: 0.011378527618944645
対照学習損失: 0.01548459380865097
{'loss': 0.0134, 'grad_norm': 0.35141104459762573, 'learning_rate': 1.75e-05, 'epoch': 0.16}
対照学習損失: 0.01781158149242401
対照学習損失: 0.009896173141896725
{'loss': 0.0139, 'grad_norm': 0.2666040062904358, 'learning_rate': 1.76e-05, 'epoch': 0.16}
対照学習損失: 0.005855603143572807
対照学習損失: 0.03292490169405937
{'loss': 0.0194, 'grad_norm': 0.5209980010986328, 'learning_rate': 1.77e-05, 'epoch': 0.16}
対照学習損失: 0.015704544261097908
対照学習損失: 0.01891433261334896
{'loss': 0.0173, 'grad_norm': 0.4631703495979309, 'learning_rate': 1.78e-05, 'epoch': 0.16}
対照学習損失: 0.005936220288276672
対照学習損失: 0.010011429898440838
{'loss': 0.008, 'grad_norm': 0.15726381540298462, 'learning_rate': 1.79e-05, 'epoch': 0.17}
対照学習損失: 0.013814018107950687
対照学習損失: 0.006861397065222263
{'loss': 0.0103, 'grad_norm': 0.253548264503479, 'learning_rate': 1.8e-05, 'epoch': 0.17}
対照学習損失: 0.007269018329679966
対照学習損失: 0.07560369372367859
{'loss': 0.0414, 'grad_norm': 1.0483213663101196, 'learning_rate': 1.81e-05, 'epoch': 0.17}
対照学習損失: 0.006150226108729839
対照学習損失: 0.010751264169812202
{'loss': 0.0085, 'grad_norm': 0.16397689282894135, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.17}
対照学習損失: 0.029272299259901047
対照学習損失: 0.02376164309680462
{'loss': 0.0265, 'grad_norm': 0.5206425786018372, 'learning_rate': 1.83e-05, 'epoch': 0.17}
対照学習損失: 0.014353876933455467
対照学習損失: 0.0057202912867069244
{'loss': 0.01, 'grad_norm': 0.1900678128004074, 'learning_rate': 1.84e-05, 'epoch': 0.17}
対照学習損失: 0.0038798917084932327
対照学習損失: 0.008419245481491089
{'loss': 0.0061, 'grad_norm': 0.11881812661886215, 'learning_rate': 1.85e-05, 'epoch': 0.17}
対照学習損失: 0.010966399684548378
対照学習損失: 0.0063905371353030205
{'loss': 0.0087, 'grad_norm': 0.2414306402206421, 'learning_rate': 1.86e-05, 'epoch': 0.17}
対照学習損失: 0.003617938607931137
対照学習損失: 0.0077798087149858475
{'loss': 0.0057, 'grad_norm': 0.16868889331817627, 'learning_rate': 1.87e-05, 'epoch': 0.17}
対照学習損失: 0.008987071923911572
対照学習損失: 0.023716900497674942
{'loss': 0.0164, 'grad_norm': 0.2955359220504761, 'learning_rate': 1.88e-05, 'epoch': 0.17}
対照学習損失: 0.06984905898571014
対照学習損失: 0.00468167569488287
{'loss': 0.0373, 'grad_norm': 1.3089498281478882, 'learning_rate': 1.8900000000000002e-05, 'epoch': 0.17}
対照学習損失: 0.059010669589042664
対照学習損失: 0.017511863261461258
{'loss': 0.0383, 'grad_norm': 0.5887475609779358, 'learning_rate': 1.9e-05, 'epoch': 0.18}
対照学習損失: 0.011501288041472435
対照学習損失: 0.006598569918423891
{'loss': 0.009, 'grad_norm': 0.20671284198760986, 'learning_rate': 1.91e-05, 'epoch': 0.18}
対照学習損失: 0.027850156649947166
対照学習損失: 0.004540388472378254
{'loss': 0.0162, 'grad_norm': 0.3889583349227905, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.18}
対照学習損失: 0.005836505442857742
対照学習損失: 0.021689143031835556
{'loss': 0.0138, 'grad_norm': 0.4393983483314514, 'learning_rate': 1.93e-05, 'epoch': 0.18}
対照学習損失: 0.021326452493667603
対照学習損失: 0.0010953574674203992
{'loss': 0.0112, 'grad_norm': 0.39809736609458923, 'learning_rate': 1.94e-05, 'epoch': 0.18}
対照学習損失: 0.01291749905794859
対照学習損失: 0.0026877126656472683
{'loss': 0.0078, 'grad_norm': 0.23784415423870087, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.18}
対照学習損失: 0.04022535681724548
対照学習損失: 0.02152383327484131
{'loss': 0.0309, 'grad_norm': 0.9017290472984314, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.18}
対照学習損失: 0.004909143783152103
対照学習損失: 0.008439261466264725
{'loss': 0.0067, 'grad_norm': 0.1458447277545929, 'learning_rate': 1.97e-05, 'epoch': 0.18}
対照学習損失: 0.0029740245081484318
対照学習損失: 0.01135056372731924
{'loss': 0.0072, 'grad_norm': 0.18868428468704224, 'learning_rate': 1.9800000000000004e-05, 'epoch': 0.18}
対照学習損失: 0.01561700738966465
対照学習損失: 0.03157953917980194
{'loss': 0.0236, 'grad_norm': 0.7329936623573303, 'learning_rate': 1.9900000000000003e-05, 'epoch': 0.18}
対照学習損失: 0.006162257399410009
対照学習損失: 0.0026542232371866703
{'loss': 0.0044, 'grad_norm': 0.11050589382648468, 'learning_rate': 2e-05, 'epoch': 0.18}
対照学習損失: 0.005868421867489815
対照学習損失: 0.007664369884878397
{'loss': 0.0068, 'grad_norm': 0.18473351001739502, 'learning_rate': 2.01e-05, 'epoch': 0.19}
対照学習損失: 0.004613349214196205
対照学習損失: 0.004486787132918835
{'loss': 0.0046, 'grad_norm': 0.13180352747440338, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.19}
対照学習損失: 0.0034455619752407074
対照学習損失: 0.0019254074431955814
{'loss': 0.0027, 'grad_norm': 0.08693094551563263, 'learning_rate': 2.0300000000000002e-05, 'epoch': 0.19}
対照学習損失: 0.0037188162095844746
対照学習損失: 0.003260859288275242
{'loss': 0.0035, 'grad_norm': 0.09003319591283798, 'learning_rate': 2.04e-05, 'epoch': 0.19}
対照学習損失: 0.0033387369476258755
対照学習損失: 0.003589872969314456
{'loss': 0.0035, 'grad_norm': 0.06951744109392166, 'learning_rate': 2.05e-05, 'epoch': 0.19}
対照学習損失: 0.0019072922877967358
対照学習損失: 0.004723484627902508
{'loss': 0.0033, 'grad_norm': 0.08261817693710327, 'learning_rate': 2.06e-05, 'epoch': 0.19}
対照学習損失: 0.0054411739110946655
対照学習損失: 0.008054525591433048
{'loss': 0.0067, 'grad_norm': 0.1387089490890503, 'learning_rate': 2.07e-05, 'epoch': 0.19}
対照学習損失: 0.006662127561867237
対照学習損失: 0.03133783116936684
{'loss': 0.019, 'grad_norm': 0.812857449054718, 'learning_rate': 2.08e-05, 'epoch': 0.19}
対照学習損失: 0.005578651558607817
対照学習損失: 0.009862575680017471
{'loss': 0.0077, 'grad_norm': 0.1339637190103531, 'learning_rate': 2.09e-05, 'epoch': 0.19}
対照学習損失: 0.0011481414549052715
対照学習損失: 0.01501037273555994
{'loss': 0.0081, 'grad_norm': 0.21399332582950592, 'learning_rate': 2.1e-05, 'epoch': 0.19}
対照学習損失: 0.008885262534022331
対照学習損失: 0.0463075153529644
{'loss': 0.0276, 'grad_norm': 0.9812496900558472, 'learning_rate': 2.11e-05, 'epoch': 0.19}
対照学習損失: 0.033193085342645645
対照学習損失: 0.0007354504195973277
{'loss': 0.017, 'grad_norm': 0.6263633370399475, 'learning_rate': 2.12e-05, 'epoch': 0.2}
対照学習損失: 0.0026442445814609528
対照学習損失: 0.12428239732980728
{'loss': 0.0635, 'grad_norm': 1.7035179138183594, 'learning_rate': 2.13e-05, 'epoch': 0.2}
対照学習損失: 0.0007846110966056585
対照学習損失: 0.025116009637713432
{'loss': 0.013, 'grad_norm': 0.5872471332550049, 'learning_rate': 2.1400000000000002e-05, 'epoch': 0.2}
対照学習損失: 0.10648979246616364
対照学習損失: 0.06898533552885056
{'loss': 0.0877, 'grad_norm': 2.420293092727661, 'learning_rate': 2.15e-05, 'epoch': 0.2}
対照学習損失: 0.02544512040913105
対照学習損失: 0.0028901719488203526
{'loss': 0.0142, 'grad_norm': 0.4892053008079529, 'learning_rate': 2.16e-05, 'epoch': 0.2}
対照学習損失: 0.008375899866223335
対照学習損失: 0.03680957853794098
{'loss': 0.0226, 'grad_norm': 0.6650288105010986, 'learning_rate': 2.1700000000000002e-05, 'epoch': 0.2}
対照学習損失: 0.002220563590526581
対照学習損失: 0.004869701340794563
{'loss': 0.0035, 'grad_norm': 0.08558642864227295, 'learning_rate': 2.18e-05, 'epoch': 0.2}
対照学習損失: 0.006857151165604591
対照学習損失: 0.03338242694735527
{'loss': 0.0201, 'grad_norm': 1.0461658239364624, 'learning_rate': 2.19e-05, 'epoch': 0.2}
対照学習損失: 0.0017373260343447328
対照学習損失: 0.0011164481984451413
{'loss': 0.0014, 'grad_norm': 0.03306782990694046, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.2}
対照学習損失: 0.0014169872738420963
対照学習損失: 0.00298732309602201
{'loss': 0.0022, 'grad_norm': 0.0666053369641304, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.2}
対照学習損失: 0.01677514798939228
対照学習損失: 0.006410193629562855
{'loss': 0.0116, 'grad_norm': 0.5567626357078552, 'learning_rate': 2.22e-05, 'epoch': 0.2}
対照学習損失: 0.11554056406021118
対照学習損失: 0.001309544313699007
{'loss': 0.0584, 'grad_norm': 1.2692734003067017, 'learning_rate': 2.23e-05, 'epoch': 0.21}
対照学習損失: 0.005633407738059759
対照学習損失: 0.0054539525881409645
{'loss': 0.0055, 'grad_norm': 0.15174153447151184, 'learning_rate': 2.2400000000000002e-05, 'epoch': 0.21}
対照学習損失: 0.0016112292651087046
対照学習損失: 0.004724702797830105
{'loss': 0.0032, 'grad_norm': 0.08291735500097275, 'learning_rate': 2.25e-05, 'epoch': 0.21}
対照学習損失: 0.0523538663983345
対照学習損失: 0.0413789264857769
{'loss': 0.0469, 'grad_norm': 1.4430608749389648, 'learning_rate': 2.26e-05, 'epoch': 0.21}
対照学習損失: 0.0016937964828684926
対照学習損失: 0.01571081578731537
{'loss': 0.0087, 'grad_norm': 0.2876831591129303, 'learning_rate': 2.2700000000000003e-05, 'epoch': 0.21}
対照学習損失: 0.002514720195904374
対照学習損失: 0.00905552227050066
{'loss': 0.0058, 'grad_norm': 0.19263018667697906, 'learning_rate': 2.2800000000000002e-05, 'epoch': 0.21}
対照学習損失: 0.000821119814645499
対照学習損失: 0.0006420332938432693
{'loss': 0.0007, 'grad_norm': 0.02433929219841957, 'learning_rate': 2.29e-05, 'epoch': 0.21}
対照学習損失: 0.0023071770556271076
対照学習損失: 0.001836756826378405
{'loss': 0.0021, 'grad_norm': 0.06310538947582245, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.21}
対照学習損失: 0.0028838436119258404
対照学習損失: 0.0026347204111516476
{'loss': 0.0028, 'grad_norm': 0.09592493623495102, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.21}
対照学習損失: 0.025715481489896774
対照学習損失: 0.0169744323939085
{'loss': 0.0213, 'grad_norm': 0.6780662536621094, 'learning_rate': 2.32e-05, 'epoch': 0.21}
対照学習損失: 0.0038704045582562685
対照学習損失: 0.0019735144451260567
{'loss': 0.0029, 'grad_norm': 0.12449680268764496, 'learning_rate': 2.3300000000000004e-05, 'epoch': 0.21}
対照学習損失: 0.003606782527640462
対照学習損失: 0.003923008684068918
{'loss': 0.0038, 'grad_norm': 0.11021289974451065, 'learning_rate': 2.3400000000000003e-05, 'epoch': 0.22}
対照学習損失: 0.015363510698080063
対照学習損失: 0.0022044451907277107
{'loss': 0.0088, 'grad_norm': 0.5456788539886475, 'learning_rate': 2.35e-05, 'epoch': 0.22}
対照学習損失: 0.012322044000029564
対照学習損失: 0.0069275470450520515
{'loss': 0.0096, 'grad_norm': 0.36486151814460754, 'learning_rate': 2.36e-05, 'epoch': 0.22}
対照学習損失: 0.005937840789556503
対照学習損失: 0.011937395669519901
{'loss': 0.0089, 'grad_norm': 0.3072716295719147, 'learning_rate': 2.37e-05, 'epoch': 0.22}
対照学習損失: 0.006825270596891642
対照学習損失: 0.0036141574382781982
{'loss': 0.0052, 'grad_norm': 0.49295875430107117, 'learning_rate': 2.38e-05, 'epoch': 0.22}
対照学習損失: 0.004394782707095146
対照学習損失: 0.0007475446909666061
{'loss': 0.0026, 'grad_norm': 0.06086638569831848, 'learning_rate': 2.39e-05, 'epoch': 0.22}
対照学習損失: 0.0008325580274686217
対照学習損失: 0.0031033200211822987
{'loss': 0.002, 'grad_norm': 0.10544539242982864, 'learning_rate': 2.4e-05, 'epoch': 0.22}
対照学習損失: 0.013212144374847412
対照学習損失: 0.003135716775432229
{'loss': 0.0082, 'grad_norm': 0.3605245053768158, 'learning_rate': 2.41e-05, 'epoch': 0.22}
対照学習損失: 0.012072578072547913
対照学習損失: 0.0005029622698202729
{'loss': 0.0063, 'grad_norm': 0.3229866921901703, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.22}
対照学習損失: 0.002834153361618519
対照学習損失: 0.0005839155055582523
{'loss': 0.0017, 'grad_norm': 0.05516153573989868, 'learning_rate': 2.43e-05, 'epoch': 0.22}
対照学習損失: 0.002956588286906481
対照学習損失: 0.0012625879608094692
{'loss': 0.0021, 'grad_norm': 0.16845834255218506, 'learning_rate': 2.44e-05, 'epoch': 0.22}
対照学習損失: 0.0004829902609344572
対照学習損失: 0.0018352058250457048
{'loss': 0.0012, 'grad_norm': 0.051112186163663864, 'learning_rate': 2.45e-05, 'epoch': 0.23}
対照学習損失: 0.09085299074649811
対照学習損失: 0.001971655059605837
{'loss': 0.0464, 'grad_norm': 1.9889676570892334, 'learning_rate': 2.46e-05, 'epoch': 0.23}
対照学習損失: 0.003138347063213587
対照学習損失: 0.002271385630592704
{'loss': 0.0027, 'grad_norm': 0.15157724916934967, 'learning_rate': 2.47e-05, 'epoch': 0.23}
対照学習損失: 0.00366798578761518
対照学習損失: 0.0032271903473883867
{'loss': 0.0034, 'grad_norm': 0.17977020144462585, 'learning_rate': 2.48e-05, 'epoch': 0.23}
対照学習損失: 0.0010511042783036828
対照学習損失: 0.0009999759495258331
{'loss': 0.001, 'grad_norm': 0.042168449610471725, 'learning_rate': 2.4900000000000002e-05, 'epoch': 0.23}
対照学習損失: 0.0018392872298136353
対照学習損失: 0.05214443430304527
{'loss': 0.027, 'grad_norm': 2.3019988536834717, 'learning_rate': 2.5e-05, 'epoch': 0.23}
対照学習損失: 0.12443141639232635
対照学習損失: 0.0003186339163221419
{'loss': 0.0624, 'grad_norm': 1.8341219425201416, 'learning_rate': 2.51e-05, 'epoch': 0.23}
対照学習損失: 0.0032206051982939243
対照学習損失: 0.005771060474216938
{'loss': 0.0045, 'grad_norm': 0.19838719069957733, 'learning_rate': 2.5200000000000003e-05, 'epoch': 0.23}
対照学習損失: 0.002199311973527074
対照学習損失: 0.004214462358504534
{'loss': 0.0032, 'grad_norm': 0.175077423453331, 'learning_rate': 2.5300000000000002e-05, 'epoch': 0.23}
対照学習損失: 0.0020766102243214846
対照学習損失: 0.00189499172847718
{'loss': 0.002, 'grad_norm': 0.08195573091506958, 'learning_rate': 2.54e-05, 'epoch': 0.23}
対照学習損失: 0.001760675571858883
対照学習損失: 0.003865445265546441
{'loss': 0.0028, 'grad_norm': 0.18484897911548615, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.23}
対照学習損失: 0.07666069269180298
対照学習損失: 0.009442823939025402
{'loss': 0.0431, 'grad_norm': 2.442479133605957, 'learning_rate': 2.5600000000000002e-05, 'epoch': 0.24}
対照学習損失: 0.0008935182704590261
対照学習損失: 0.0011287331581115723
{'loss': 0.001, 'grad_norm': 0.04469413310289383, 'learning_rate': 2.57e-05, 'epoch': 0.24}
対照学習損失: 0.00045220431638881564
対照学習損失: 0.0318426638841629
{'loss': 0.0161, 'grad_norm': 0.8830023407936096, 'learning_rate': 2.58e-05, 'epoch': 0.24}
対照学習損失: 0.09221047163009644
対照学習損失: 0.0010154515039175749
{'loss': 0.0466, 'grad_norm': 2.9185264110565186, 'learning_rate': 2.5900000000000003e-05, 'epoch': 0.24}
対照学習損失: 0.001241604913957417
対照学習損失: 0.0033529577776789665
{'loss': 0.0023, 'grad_norm': 0.08953632414340973, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.24}
対照学習損失: 0.0007441036286763847
対照学習損失: 0.0009288878063671291
{'loss': 0.0008, 'grad_norm': 0.01827755570411682, 'learning_rate': 2.61e-05, 'epoch': 0.24}
対照学習損失: 0.0005094136577099562
対照学習損失: 0.007638751529157162
{'loss': 0.0041, 'grad_norm': 0.13060930371284485, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.24}
対照学習損失: 0.0035467541310936213
対照学習損失: 0.00034981811768375337
{'loss': 0.0019, 'grad_norm': 0.2610898017883301, 'learning_rate': 2.6300000000000002e-05, 'epoch': 0.24}
対照学習損失: 0.0029133677016943693
対照学習損失: 0.00048245821380987763
{'loss': 0.0017, 'grad_norm': 0.10049669444561005, 'learning_rate': 2.64e-05, 'epoch': 0.24}
対照学習損失: 0.00023413005692418665
対照学習損失: 0.006673632189631462
{'loss': 0.0035, 'grad_norm': 0.22407574951648712, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.24}
対照学習損失: 0.0007605882710777223
対照学習損失: 0.000771906808950007
{'loss': 0.0008, 'grad_norm': 0.02384314499795437, 'learning_rate': 2.6600000000000003e-05, 'epoch': 0.24}
対照学習損失: 0.0012920672306790948
対照学習損失: 0.001207381603308022
{'loss': 0.0012, 'grad_norm': 0.030918383970856667, 'learning_rate': 2.6700000000000002e-05, 'epoch': 0.25}
対照学習損失: 0.010462828911840916
対照学習損失: 0.0029919277876615524
{'loss': 0.0067, 'grad_norm': 0.34370753169059753, 'learning_rate': 2.6800000000000004e-05, 'epoch': 0.25}
対照学習損失: 0.00048328869161196053
対照学習損失: 0.001401997171342373
{'loss': 0.0009, 'grad_norm': 0.0332992859184742, 'learning_rate': 2.6900000000000003e-05, 'epoch': 0.25}
対照学習損失: 0.00133976386860013
対照学習損失: 0.0003109207609668374
{'loss': 0.0008, 'grad_norm': 0.02803678996860981, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.25}
対照学習損失: 0.025940336287021637
対照学習損失: 0.0021048597991466522
{'loss': 0.014, 'grad_norm': 0.6137704849243164, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.25}
対照学習損失: 0.0022210332099348307
対照学習損失: 0.0011171052465215325
{'loss': 0.0017, 'grad_norm': 0.06580007821321487, 'learning_rate': 2.7200000000000004e-05, 'epoch': 0.25}
対照学習損失: 0.0007865101797506213
対照学習損失: 0.008304414339363575
{'loss': 0.0045, 'grad_norm': 0.14816121757030487, 'learning_rate': 2.7300000000000003e-05, 'epoch': 0.25}
対照学習損失: 0.001772978575900197
対照学習損失: 0.005292537156492472
{'loss': 0.0035, 'grad_norm': 0.2824094593524933, 'learning_rate': 2.7400000000000002e-05, 'epoch': 0.25}
対照学習損失: 0.0007337984861806035
対照学習損失: 0.00319425156340003
{'loss': 0.002, 'grad_norm': 0.08385768532752991, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.25}
対照学習損失: 0.00705360434949398
対照学習損失: 0.0028192084282636642
{'loss': 0.0049, 'grad_norm': 0.20001596212387085, 'learning_rate': 2.7600000000000003e-05, 'epoch': 0.25}
対照学習損失: 0.0006505891215056181
対照学習損失: 0.000772884814068675
{'loss': 0.0007, 'grad_norm': 0.018889155238866806, 'learning_rate': 2.7700000000000002e-05, 'epoch': 0.25}
対照学習損失: 0.0004150753084104508
対照学習損失: 0.0015860248822718859
{'loss': 0.001, 'grad_norm': 0.023035341873764992, 'learning_rate': 2.7800000000000005e-05, 'epoch': 0.26}
対照学習損失: 0.0009612314752303064
対照学習損失: 0.0035395543090999126
{'loss': 0.0023, 'grad_norm': 0.09204373508691788, 'learning_rate': 2.7900000000000004e-05, 'epoch': 0.26}
対照学習損失: 0.0002541868307162076
対照学習損失: 0.0021363485138863325
{'loss': 0.0012, 'grad_norm': 0.05724264308810234, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.26}
対照学習損失: 0.0021139855962246656
対照学習損失: 0.007308322470635176
{'loss': 0.0047, 'grad_norm': 0.15229107439517975, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.26}
対照学習損失: 0.0010867828968912363
対照学習損失: 0.0013639028184115887
{'loss': 0.0012, 'grad_norm': 0.035828541964292526, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.26}
対照学習損失: 0.0010308503406122327
対照学習損失: 0.004592103883624077
{'loss': 0.0028, 'grad_norm': 0.08357773721218109, 'learning_rate': 2.83e-05, 'epoch': 0.26}
対照学習損失: 0.0007352327229455113
対照学習損失: 0.0008965351735241711
{'loss': 0.0008, 'grad_norm': 0.02947663702070713, 'learning_rate': 2.84e-05, 'epoch': 0.26}
対照学習損失: 0.007838401943445206
対照学習損失: 0.003563312580808997
{'loss': 0.0057, 'grad_norm': 0.2626912593841553, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.26}
対照学習損失: 0.009429233148694038
対照学習損失: 0.0008546318276785314
{'loss': 0.0051, 'grad_norm': 0.20953762531280518, 'learning_rate': 2.86e-05, 'epoch': 0.26}
対照学習損失: 0.0006701187812723219
対照学習損失: 0.004110637586563826
{'loss': 0.0024, 'grad_norm': 0.08474129438400269, 'learning_rate': 2.87e-05, 'epoch': 0.26}
対照学習損失: 0.007338591385632753
対照学習損失: 0.0007640123949386179
{'loss': 0.0041, 'grad_norm': 0.23677051067352295, 'learning_rate': 2.88e-05, 'epoch': 0.27}
対照学習損失: 0.04678294062614441
対照学習損失: 0.0005459960084408522
{'loss': 0.0237, 'grad_norm': 1.2200984954833984, 'learning_rate': 2.8899999999999998e-05, 'epoch': 0.27}
対照学習損失: 0.0027604156639426947
対照学習損失: 0.0007474144222214818
{'loss': 0.0018, 'grad_norm': 0.0990414246916771, 'learning_rate': 2.9e-05, 'epoch': 0.27}
対照学習損失: 0.002385310363024473
対照学習損失: 0.0006897019920870662
{'loss': 0.0015, 'grad_norm': 0.08884184062480927, 'learning_rate': 2.91e-05, 'epoch': 0.27}
対照学習損失: 0.0014572864165529609
対照学習損失: 0.0010281119029968977
{'loss': 0.0012, 'grad_norm': 0.04417018964886665, 'learning_rate': 2.9199999999999998e-05, 'epoch': 0.27}
対照学習損失: 0.0019681949634104967
対照学習損失: 0.0019971616566181183
{'loss': 0.002, 'grad_norm': 0.14817771315574646, 'learning_rate': 2.93e-05, 'epoch': 0.27}
対照学習損失: 0.0018135234713554382
対照学習損失: 0.0077481516636908054
{'loss': 0.0048, 'grad_norm': 0.2905229330062866, 'learning_rate': 2.94e-05, 'epoch': 0.27}
対照学習損失: 0.0040016015991568565
対照学習損失: 0.001157772378064692
{'loss': 0.0026, 'grad_norm': 0.07384354621171951, 'learning_rate': 2.95e-05, 'epoch': 0.27}
対照学習損失: 0.0014991743955761194
対照学習損失: 0.0005630503874272108
{'loss': 0.001, 'grad_norm': 0.043627507984638214, 'learning_rate': 2.96e-05, 'epoch': 0.27}
対照学習損失: 0.00477308826521039
対照学習損失: 0.016799962148070335
{'loss': 0.0108, 'grad_norm': 0.8736889958381653, 'learning_rate': 2.97e-05, 'epoch': 0.27}
対照学習損失: 0.0050176712684333324
対照学習損失: 0.006885173264890909
{'loss': 0.006, 'grad_norm': 0.2614327371120453, 'learning_rate': 2.98e-05, 'epoch': 0.27}
対照学習損失: 0.0003064130141865462
対照学習損失: 0.0015627919929102063
{'loss': 0.0009, 'grad_norm': 0.05778369680047035, 'learning_rate': 2.9900000000000002e-05, 'epoch': 0.28}
対照学習損失: 0.002032435964792967
対照学習損失: 0.004746956750750542
{'loss': 0.0034, 'grad_norm': 0.33019959926605225, 'learning_rate': 3e-05, 'epoch': 0.28}
対照学習損失: 0.0007792467367835343
対照学習損失: 0.005861644167453051
{'loss': 0.0033, 'grad_norm': 0.22944262623786926, 'learning_rate': 3.01e-05, 'epoch': 0.28}
対照学習損失: 0.0028143743984401226
対照学習損失: 0.03695153817534447
{'loss': 0.0199, 'grad_norm': 1.7379469871520996, 'learning_rate': 3.02e-05, 'epoch': 0.28}
対照学習損失: 0.00335337920114398
対照学習損失: 0.003158452222123742
{'loss': 0.0033, 'grad_norm': 0.09203819185495377, 'learning_rate': 3.03e-05, 'epoch': 0.28}
対照学習損失: 0.006082961335778236
対照学習損失: 0.0061811842024326324
{'loss': 0.0061, 'grad_norm': 0.2632048428058624, 'learning_rate': 3.04e-05, 'epoch': 0.28}
対照学習損失: 0.09893765300512314
対照学習損失: 0.0008856233907863498
{'loss': 0.0499, 'grad_norm': 1.0562785863876343, 'learning_rate': 3.05e-05, 'epoch': 0.28}
対照学習損失: 0.06213117390871048
対照学習損失: 0.0011590151116251945
{'loss': 0.0316, 'grad_norm': 1.2021650075912476, 'learning_rate': 3.06e-05, 'epoch': 0.28}
対照学習損失: 0.00022057750902604312
対照学習損失: 0.0012714022304862738
{'loss': 0.0007, 'grad_norm': 0.03381535783410072, 'learning_rate': 3.07e-05, 'epoch': 0.28}
対照学習損失: 0.001676079467870295
対照学習損失: 0.0048455544747412205
{'loss': 0.0033, 'grad_norm': 0.22377178072929382, 'learning_rate': 3.08e-05, 'epoch': 0.28}
対照学習損失: 0.003001587698236108
対照学習損失: 0.01623937301337719
{'loss': 0.0096, 'grad_norm': 0.7247162461280823, 'learning_rate': 3.09e-05, 'epoch': 0.28}
対照学習損失: 0.006497167982161045
対照学習損失: 0.0033627983648329973
{'loss': 0.0049, 'grad_norm': 0.25451773405075073, 'learning_rate': 3.1e-05, 'epoch': 0.29}
対照学習損失: 0.041377659887075424
対照学習損失: 0.0011126170866191387
{'loss': 0.0212, 'grad_norm': 1.2548884153366089, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.29}
対照学習損失: 0.0008624302572570741
対照学習損失: 0.0003198446938768029
{'loss': 0.0006, 'grad_norm': 0.02365678921341896, 'learning_rate': 3.12e-05, 'epoch': 0.29}
対照学習損失: 0.00214086240157485
対照学習損失: 0.00022986525436863303
{'loss': 0.0012, 'grad_norm': 0.058373913168907166, 'learning_rate': 3.13e-05, 'epoch': 0.29}
対照学習損失: 0.0006279468070715666
対照学習損失: 0.0003239665529690683
{'loss': 0.0005, 'grad_norm': 0.01181697752326727, 'learning_rate': 3.1400000000000004e-05, 'epoch': 0.29}
対照学習損失: 0.0003184512897860259
対照学習損失: 0.0011093434877693653
{'loss': 0.0007, 'grad_norm': 0.037130072712898254, 'learning_rate': 3.15e-05, 'epoch': 0.29}
対照学習損失: 0.001541351666674018
対照学習損失: 0.00034205097472295165
{'loss': 0.0009, 'grad_norm': 0.04323110356926918, 'learning_rate': 3.16e-05, 'epoch': 0.29}
対照学習損失: 0.0005680372123606503
対照学習損失: 0.002251632045954466
{'loss': 0.0014, 'grad_norm': 0.06138613820075989, 'learning_rate': 3.1700000000000005e-05, 'epoch': 0.29}
対照学習損失: 0.0008280472247861326
対照学習損失: 0.09281638264656067
{'loss': 0.0468, 'grad_norm': 1.757974624633789, 'learning_rate': 3.18e-05, 'epoch': 0.29}
対照学習損失: 0.00024613243294879794
対照学習損失: 0.00030553387477993965
{'loss': 0.0003, 'grad_norm': 0.007493350189179182, 'learning_rate': 3.19e-05, 'epoch': 0.29}
対照学習損失: 0.0015023516025394201
対照学習損失: 0.0018605359364300966
{'loss': 0.0017, 'grad_norm': 0.08200749009847641, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.29}
対照学習損失: 0.0008404225809499621
対照学習損失: 0.0019515473395586014
{'loss': 0.0014, 'grad_norm': 0.1117599755525589, 'learning_rate': 3.21e-05, 'epoch': 0.3}
対照学習損失: 0.0027377300430089235
対照学習損失: 0.0015141390031203628
{'loss': 0.0021, 'grad_norm': 0.09083596616983414, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.3}
対照学習損失: 0.00019283614528831095
対照学習損失: 0.0045465994626283646
{'loss': 0.0024, 'grad_norm': 0.1121276393532753, 'learning_rate': 3.2300000000000006e-05, 'epoch': 0.3}
対照学習損失: 0.037336159497499466
対照学習損失: 0.008004771545529366
{'loss': 0.0227, 'grad_norm': 1.0030055046081543, 'learning_rate': 3.24e-05, 'epoch': 0.3}
対照学習損失: 0.00028336787363514304
対照学習損失: 0.0008512969361618161
{'loss': 0.0006, 'grad_norm': 0.017045797780156136, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.3}
対照学習損失: 0.008707352913916111
対照学習損失: 0.008327128365635872
{'loss': 0.0085, 'grad_norm': 0.36259374022483826, 'learning_rate': 3.26e-05, 'epoch': 0.3}
対照学習損失: 0.013438606634736061
対照学習損失: 0.0005879380041733384
{'loss': 0.007, 'grad_norm': 0.5515370965003967, 'learning_rate': 3.27e-05, 'epoch': 0.3}
対照学習損失: 0.00023952549963723868
対照学習損失: 0.0030017481185495853
{'loss': 0.0016, 'grad_norm': 0.09229008108377457, 'learning_rate': 3.2800000000000004e-05, 'epoch': 0.3}
対照学習損失: 0.0005073575885035098
対照学習損失: 0.0012824225705116987
{'loss': 0.0009, 'grad_norm': 0.03380734473466873, 'learning_rate': 3.29e-05, 'epoch': 0.3}
対照学習損失: 0.0033306083641946316
対照学習損失: 0.0006946364883333445
{'loss': 0.002, 'grad_norm': 0.0881146788597107, 'learning_rate': 3.3e-05, 'epoch': 0.3}
対照学習損失: 0.0035695102997124195
対照学習損失: 0.002782148076221347
{'loss': 0.0032, 'grad_norm': 0.09524022042751312, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.3}
対照学習損失: 0.0013619477394968271
対照学習損失: 0.0003570909611880779
{'loss': 0.0009, 'grad_norm': 0.04041414335370064, 'learning_rate': 3.32e-05, 'epoch': 0.31}
対照学習損失: 0.0020601842552423477
対照学習損失: 0.0008194431429728866
{'loss': 0.0014, 'grad_norm': 0.05482177063822746, 'learning_rate': 3.33e-05, 'epoch': 0.31}
対照学習損失: 0.0011570771457627416
対照学習損失: 0.0029996365774422884
{'loss': 0.0021, 'grad_norm': 0.07572302222251892, 'learning_rate': 3.3400000000000005e-05, 'epoch': 0.31}
対照学習損失: 0.001999954227358103
対照学習損失: 0.0010819011367857456
{'loss': 0.0015, 'grad_norm': 0.06740346550941467, 'learning_rate': 3.35e-05, 'epoch': 0.31}
対照学習損失: 0.000651082897093147
対照学習損失: 0.0028844571206718683
{'loss': 0.0018, 'grad_norm': 0.07846637815237045, 'learning_rate': 3.3600000000000004e-05, 'epoch': 0.31}
対照学習損失: 0.0008578942506574094
対照学習損失: 0.0009444853640161455
{'loss': 0.0009, 'grad_norm': 0.04705620929598808, 'learning_rate': 3.3700000000000006e-05, 'epoch': 0.31}
対照学習損失: 0.0003475903067737818
対照学習損失: 0.0006653144373558462
{'loss': 0.0005, 'grad_norm': 0.016458775848150253, 'learning_rate': 3.38e-05, 'epoch': 0.31}
対照学習損失: 0.0005169347277842462
対照学習損失: 0.0007135398336686194
{'loss': 0.0006, 'grad_norm': 0.024848947301506996, 'learning_rate': 3.3900000000000004e-05, 'epoch': 0.31}
対照学習損失: 0.00039002287667244673
対照学習損失: 0.00022228658781386912
{'loss': 0.0003, 'grad_norm': 0.008254957385361195, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.31}
対照学習損失: 0.0016041576163843274
対照学習損失: 0.00015161928604356945
{'loss': 0.0009, 'grad_norm': 0.028065096586942673, 'learning_rate': 3.41e-05, 'epoch': 0.31}
対照学習損失: 0.027263373136520386
対照学習損失: 0.0002815207408275455
{'loss': 0.0138, 'grad_norm': 0.6143913865089417, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.31}
対照学習損失: 0.010400468483567238
対照学習損失: 0.0009184738737531006
{'loss': 0.0057, 'grad_norm': 0.5186476111412048, 'learning_rate': 3.430000000000001e-05, 'epoch': 0.32}
対照学習損失: 0.002367784269154072
対照学習損失: 0.0009180116467177868
{'loss': 0.0016, 'grad_norm': 0.08921220898628235, 'learning_rate': 3.4399999999999996e-05, 'epoch': 0.32}
対照学習損失: 0.0006856578402221203
対照学習損失: 0.0032930769957602024
{'loss': 0.002, 'grad_norm': 0.0837288424372673, 'learning_rate': 3.45e-05, 'epoch': 0.32}
対照学習損失: 0.0001264703896595165
対照学習損失: 0.021396072581410408
{'loss': 0.0108, 'grad_norm': 0.7531810402870178, 'learning_rate': 3.46e-05, 'epoch': 0.32}
対照学習損失: 0.002524562180042267
対照学習損失: 0.003099908120930195
{'loss': 0.0028, 'grad_norm': 0.08760344237089157, 'learning_rate': 3.4699999999999996e-05, 'epoch': 0.32}
対照学習損失: 0.0006778972456231713
対照学習損失: 0.001348037039861083
{'loss': 0.001, 'grad_norm': 0.04281018301844597, 'learning_rate': 3.48e-05, 'epoch': 0.32}
対照学習損失: 0.017339523881673813
対照学習損失: 0.0002915208460763097
{'loss': 0.0088, 'grad_norm': 0.4961455762386322, 'learning_rate': 3.49e-05, 'epoch': 0.32}
対照学習損失: 0.006824007257819176
対照学習損失: 0.00033589263330213726
{'loss': 0.0036, 'grad_norm': 0.2522975504398346, 'learning_rate': 3.5e-05, 'epoch': 0.32}
対照学習損失: 0.0014730925904586911
対照学習損失: 0.0018830179469659925
{'loss': 0.0017, 'grad_norm': 0.07696213573217392, 'learning_rate': 3.51e-05, 'epoch': 0.32}
対照学習損失: 0.00044870120473206043
対照学習損失: 0.0004102075472474098
{'loss': 0.0004, 'grad_norm': 0.014635163359344006, 'learning_rate': 3.52e-05, 'epoch': 0.32}
対照学習損失: 0.0002906664740294218
対照学習損失: 0.0006181257776916027
{'loss': 0.0005, 'grad_norm': 0.015510148368775845, 'learning_rate': 3.53e-05, 'epoch': 0.32}
対照学習損失: 0.00047673546941950917
対照学習損失: 0.000478258851217106
{'loss': 0.0005, 'grad_norm': 0.02035076729953289, 'learning_rate': 3.54e-05, 'epoch': 0.33}
対照学習損失: 0.0002739058982115239
対照学習損失: 0.0017133643850684166
{'loss': 0.001, 'grad_norm': 0.06353456526994705, 'learning_rate': 3.55e-05, 'epoch': 0.33}
対照学習損失: 0.0002194336848333478
対照学習損失: 0.0019386629574000835
{'loss': 0.0011, 'grad_norm': 0.11766735464334488, 'learning_rate': 3.56e-05, 'epoch': 0.33}
対照学習損失: 0.0007591415196657181
対照学習損失: 0.004345136694610119
{'loss': 0.0026, 'grad_norm': 0.21068453788757324, 'learning_rate': 3.57e-05, 'epoch': 0.33}
対照学習損失: 6.48682180326432e-05
対照学習損失: 0.0003576996678020805
{'loss': 0.0002, 'grad_norm': 0.008787271566689014, 'learning_rate': 3.58e-05, 'epoch': 0.33}
対照学習損失: 0.000506704964209348
対照学習損失: 0.0008031544275581837
{'loss': 0.0007, 'grad_norm': 0.03195992484688759, 'learning_rate': 3.59e-05, 'epoch': 0.33}
対照学習損失: 0.00043339404510334134
対照学習損失: 0.00027773514739237726
{'loss': 0.0004, 'grad_norm': 0.01177234761416912, 'learning_rate': 3.6e-05, 'epoch': 0.33}
対照学習損失: 0.0029837116599082947
対照学習損失: 0.0019428357481956482
{'loss': 0.0025, 'grad_norm': 0.2385282963514328, 'learning_rate': 3.61e-05, 'epoch': 0.33}
対照学習損失: 0.0015060617588460445
対照学習損失: 0.07496120780706406
{'loss': 0.0382, 'grad_norm': 1.9114598035812378, 'learning_rate': 3.62e-05, 'epoch': 0.33}
対照学習損失: 0.0003381948627065867
対照学習損失: 0.001157071441411972
{'loss': 0.0007, 'grad_norm': 0.03105129487812519, 'learning_rate': 3.63e-05, 'epoch': 0.33}
対照学習損失: 0.027004612609744072
対照学習損失: 0.00013698646216653287
{'loss': 0.0136, 'grad_norm': 1.93070650100708, 'learning_rate': 3.6400000000000004e-05, 'epoch': 0.33}
対照学習損失: 0.00011202586756553501
対照学習損失: 0.0003241004596929997
{'loss': 0.0002, 'grad_norm': 0.008681248873472214, 'learning_rate': 3.65e-05, 'epoch': 0.34}
対照学習損失: 6.581849447684363e-05
対照学習損失: 0.00015920383157208562
{'loss': 0.0001, 'grad_norm': 0.003236933145672083, 'learning_rate': 3.66e-05, 'epoch': 0.34}
対照学習損失: 0.0005664099007844925
対照学習損失: 0.0002759327180683613
{'loss': 0.0004, 'grad_norm': 0.022341951727867126, 'learning_rate': 3.6700000000000004e-05, 'epoch': 0.34}
対照学習損失: 0.00013693106302525848
対照学習損失: 0.02042517624795437
{'loss': 0.0103, 'grad_norm': 0.8847098350524902, 'learning_rate': 3.68e-05, 'epoch': 0.34}
対照学習損失: 0.0035849546547979116
対照学習損失: 0.0015063622267916799
{'loss': 0.0025, 'grad_norm': 0.07353261858224869, 'learning_rate': 3.69e-05, 'epoch': 0.34}
対照学習損失: 0.00015949230873957276
対照学習損失: 0.0009968283120542765
{'loss': 0.0006, 'grad_norm': 0.026419561356306076, 'learning_rate': 3.7e-05, 'epoch': 0.34}
対照学習損失: 0.01676703803241253
対照学習損失: 7.968234422150999e-05
{'loss': 0.0084, 'grad_norm': 0.4099434018135071, 'learning_rate': 3.71e-05, 'epoch': 0.34}
対照学習損失: 0.0008136599790304899
対照学習損失: 0.00019537488697096705
{'loss': 0.0005, 'grad_norm': 0.0186898373067379, 'learning_rate': 3.72e-05, 'epoch': 0.34}
対照学習損失: 0.0012420366983860731
対照学習損失: 0.0032199486158788204
{'loss': 0.0022, 'grad_norm': 0.12031116336584091, 'learning_rate': 3.73e-05, 'epoch': 0.34}
対照学習損失: 0.0004494133172556758
対照学習損失: 0.0005609103245660663
{'loss': 0.0005, 'grad_norm': 0.0171178188174963, 'learning_rate': 3.74e-05, 'epoch': 0.34}
対照学習損失: 7.679415284655988e-05
対照学習損失: 0.0011366726830601692
{'loss': 0.0006, 'grad_norm': 0.05019703879952431, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.34}
対照学習損失: 0.05743005499243736
対照学習損失: 0.0012653634184971452
{'loss': 0.0293, 'grad_norm': 1.046292781829834, 'learning_rate': 3.76e-05, 'epoch': 0.35}
対照学習損失: 0.00015909317880868912
対照学習損失: 0.00555458152666688
{'loss': 0.0029, 'grad_norm': 0.2206946164369583, 'learning_rate': 3.77e-05, 'epoch': 0.35}
対照学習損失: 0.00010214932262897491
対照学習損失: 0.0003431311924941838
{'loss': 0.0002, 'grad_norm': 0.010273432359099388, 'learning_rate': 3.7800000000000004e-05, 'epoch': 0.35}
対照学習損失: 0.0003365935408510268
対照学習損失: 0.0006418618722818792
{'loss': 0.0005, 'grad_norm': 0.04026943817734718, 'learning_rate': 3.79e-05, 'epoch': 0.35}
対照学習損失: 0.0009088271763175726
対照学習損失: 0.000562685017939657
{'loss': 0.0007, 'grad_norm': 0.06187748908996582, 'learning_rate': 3.8e-05, 'epoch': 0.35}
対照学習損失: 0.0005209673545323312
対照学習損失: 0.0012686238624155521
{'loss': 0.0009, 'grad_norm': 0.04140307754278183, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.35}
対照学習損失: 0.00010029232362285256
対照学習損失: 0.002860093954950571
{'loss': 0.0015, 'grad_norm': 0.0825749859213829, 'learning_rate': 3.82e-05, 'epoch': 0.35}
対照学習損失: 0.001084841787815094
対照学習損失: 0.001450569019652903
{'loss': 0.0013, 'grad_norm': 0.11049351841211319, 'learning_rate': 3.83e-05, 'epoch': 0.35}
対照学習損失: 0.0002241154870716855
対照学習損失: 0.0004228570032864809
{'loss': 0.0003, 'grad_norm': 0.00996531080454588, 'learning_rate': 3.8400000000000005e-05, 'epoch': 0.35}
対照学習損失: 8.019689994398504e-05
対照学習損失: 0.0007805338827893138
{'loss': 0.0004, 'grad_norm': 0.01920541375875473, 'learning_rate': 3.85e-05, 'epoch': 0.35}
対照学習損失: 0.00015815606457181275
対照学習損失: 0.0005590779474005103
{'loss': 0.0004, 'grad_norm': 0.019029630348086357, 'learning_rate': 3.86e-05, 'epoch': 0.35}
対照学習損失: 0.0002348099515074864
対照学習損失: 0.002927954774349928
{'loss': 0.0016, 'grad_norm': 0.07706980407238007, 'learning_rate': 3.8700000000000006e-05, 'epoch': 0.36}
対照学習損失: 0.029261354357004166
対照学習損失: 0.00014740158803761005
{'loss': 0.0147, 'grad_norm': 0.8756533861160278, 'learning_rate': 3.88e-05, 'epoch': 0.36}
対照学習損失: 0.00016170035814866424
対照学習損失: 0.015275626443326473
{'loss': 0.0077, 'grad_norm': 0.344379723072052, 'learning_rate': 3.8900000000000004e-05, 'epoch': 0.36}
対照学習損失: 0.0007589877932332456
対照学習損失: 0.0006770671461708844
{'loss': 0.0007, 'grad_norm': 0.028583932667970657, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.36}
対照学習損失: 0.0014820521464571357
対照学習損失: 0.007391566876322031
{'loss': 0.0044, 'grad_norm': 0.15943019092082977, 'learning_rate': 3.91e-05, 'epoch': 0.36}
対照学習損失: 0.0008688902016729116
対照学習損失: 0.0008348294068127871
{'loss': 0.0009, 'grad_norm': 0.04724488407373428, 'learning_rate': 3.9200000000000004e-05, 'epoch': 0.36}
対照学習損失: 7.326970808207989e-05
対照学習損失: 0.0016702867578715086
{'loss': 0.0009, 'grad_norm': 0.04720780998468399, 'learning_rate': 3.9300000000000007e-05, 'epoch': 0.36}
対照学習損失: 0.0004625884466804564
対照学習損失: 0.0002519385889172554
{'loss': 0.0004, 'grad_norm': 0.012883278541266918, 'learning_rate': 3.94e-05, 'epoch': 0.36}
対照学習損失: 0.01498324703425169
対照学習損失: 0.0008392362506128848
{'loss': 0.0079, 'grad_norm': 0.5456557273864746, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.36}
対照学習損失: 0.00024979221052490175
対照学習損失: 0.003192658070474863
{'loss': 0.0017, 'grad_norm': 0.16097557544708252, 'learning_rate': 3.960000000000001e-05, 'epoch': 0.36}
対照学習損失: 8.683613123139367e-05
対照学習損失: 0.0004161881224717945
{'loss': 0.0003, 'grad_norm': 0.02486089989542961, 'learning_rate': 3.97e-05, 'epoch': 0.36}
対照学習損失: 0.002569602569565177
対照学習損失: 0.0005287439562380314
{'loss': 0.0015, 'grad_norm': 0.08234057575464249, 'learning_rate': 3.9800000000000005e-05, 'epoch': 0.37}
対照学習損失: 8.698958845343441e-05
対照学習損失: 0.0014119490515440702
{'loss': 0.0007, 'grad_norm': 0.03861311450600624, 'learning_rate': 3.99e-05, 'epoch': 0.37}
対照学習損失: 0.0003796173259615898
対照学習損失: 0.00048719372716732323
{'loss': 0.0004, 'grad_norm': 0.024678027257323265, 'learning_rate': 4e-05, 'epoch': 0.37}
対照学習損失: 0.00028539879713207483
対照学習損失: 0.00030265029636211693
{'loss': 0.0003, 'grad_norm': 0.012243492528796196, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.37}
対照学習損失: 5.350637366063893e-05
対照学習損失: 0.00011495203943923116
{'loss': 0.0001, 'grad_norm': 0.003316610585898161, 'learning_rate': 4.02e-05, 'epoch': 0.37}
対照学習損失: 0.00024294570903293788
対照学習損失: 0.00024350931926164776
{'loss': 0.0002, 'grad_norm': 0.007592322770506144, 'learning_rate': 4.0300000000000004e-05, 'epoch': 0.37}
対照学習損失: 0.0007854070863686502
対照学習損失: 0.01941847801208496
{'loss': 0.0101, 'grad_norm': 0.6615802645683289, 'learning_rate': 4.0400000000000006e-05, 'epoch': 0.37}
対照学習損失: 0.00038338458398357034
対照学習損失: 0.00019838049774989486
{'loss': 0.0003, 'grad_norm': 0.015202590264379978, 'learning_rate': 4.05e-05, 'epoch': 0.37}
対照学習損失: 0.0025485854130238295
対照学習損失: 5.514018630492501e-05
{'loss': 0.0013, 'grad_norm': 0.1756584644317627, 'learning_rate': 4.0600000000000004e-05, 'epoch': 0.37}
対照学習損失: 0.0003542586346156895
対照学習損失: 0.02061748504638672
{'loss': 0.0105, 'grad_norm': 0.4114754796028137, 'learning_rate': 4.07e-05, 'epoch': 0.37}
対照学習損失: 0.0005576503463089466
対照学習損失: 0.00037998909829184413
{'loss': 0.0005, 'grad_norm': 0.03253590315580368, 'learning_rate': 4.08e-05, 'epoch': 0.38}
対照学習損失: 0.00039711364661343396
対照学習損失: 0.0003774393117055297
{'loss': 0.0004, 'grad_norm': 0.02843474969267845, 'learning_rate': 4.09e-05, 'epoch': 0.38}
対照学習損失: 0.0004757949791382998
対照学習損失: 0.00025450342218391597
{'loss': 0.0004, 'grad_norm': 0.021626297384500504, 'learning_rate': 4.1e-05, 'epoch': 0.38}
対照学習損失: 9.496341226622462e-05
対照学習損失: 0.004531839396804571
{'loss': 0.0023, 'grad_norm': 0.17295235395431519, 'learning_rate': 4.11e-05, 'epoch': 0.38}
対照学習損失: 0.00037741364212706685
対照学習損失: 0.0004570254823192954
{'loss': 0.0004, 'grad_norm': 0.016412852331995964, 'learning_rate': 4.12e-05, 'epoch': 0.38}
対照学習損失: 0.0001450544805265963
対照学習損失: 0.0034458988811820745
{'loss': 0.0018, 'grad_norm': 0.1891203671693802, 'learning_rate': 4.13e-05, 'epoch': 0.38}
対照学習損失: 0.00014078343519940972
対照学習損失: 0.004206827376037836
{'loss': 0.0022, 'grad_norm': 0.22858817875385284, 'learning_rate': 4.14e-05, 'epoch': 0.38}
対照学習損失: 0.00011013612675014883
対照学習損失: 0.00017073417257051915
{'loss': 0.0001, 'grad_norm': 0.005778973456472158, 'learning_rate': 4.15e-05, 'epoch': 0.38}
対照学習損失: 0.0010109849972650409
対照学習損失: 3.372003266122192e-05
{'loss': 0.0005, 'grad_norm': 0.06919781118631363, 'learning_rate': 4.16e-05, 'epoch': 0.38}
対照学習損失: 0.0003833579830825329
対照学習損失: 0.0007703620940446854
{'loss': 0.0006, 'grad_norm': 0.022191274911165237, 'learning_rate': 4.17e-05, 'epoch': 0.38}
対照学習損失: 9.571596456225961e-05
対照学習損失: 0.0002489955513738096
{'loss': 0.0002, 'grad_norm': 0.008985823951661587, 'learning_rate': 4.18e-05, 'epoch': 0.38}
対照学習損失: 0.0013369724620133638
対照学習損失: 0.00012314475316088647
{'loss': 0.0007, 'grad_norm': 0.07821108400821686, 'learning_rate': 4.19e-05, 'epoch': 0.39}
対照学習損失: 0.00037072336999699473
対照学習損失: 0.0010592375183477998
{'loss': 0.0007, 'grad_norm': 0.0433187372982502, 'learning_rate': 4.2e-05, 'epoch': 0.39}
対照学習損失: 0.00013573933392763138
対照学習損失: 0.0008819671929813921
{'loss': 0.0005, 'grad_norm': 0.02220582216978073, 'learning_rate': 4.21e-05, 'epoch': 0.39}
対照学習損失: 0.0010338861029595137
対照学習損失: 0.00024029947235248983
{'loss': 0.0006, 'grad_norm': 0.02182534709572792, 'learning_rate': 4.22e-05, 'epoch': 0.39}
対照学習損失: 0.0008894954225979745
対照学習損失: 0.0025305356830358505
{'loss': 0.0017, 'grad_norm': 0.07133830338716507, 'learning_rate': 4.23e-05, 'epoch': 0.39}
対照学習損失: 0.0016667548334226012
対照学習損失: 0.00023892326862551272
{'loss': 0.001, 'grad_norm': 0.14845921099185944, 'learning_rate': 4.24e-05, 'epoch': 0.39}
対照学習損失: 0.00011694233398884535
対照学習損失: 0.010068127885460854
{'loss': 0.0051, 'grad_norm': 0.3009197413921356, 'learning_rate': 4.25e-05, 'epoch': 0.39}
対照学習損失: 0.005870354827493429
対照学習損失: 0.00014623516472056508
{'loss': 0.003, 'grad_norm': 0.1590539962053299, 'learning_rate': 4.26e-05, 'epoch': 0.39}
対照学習損失: 0.0013102266239002347
対照学習損失: 0.00028063071658834815
{'loss': 0.0008, 'grad_norm': 0.05931740254163742, 'learning_rate': 4.27e-05, 'epoch': 0.39}
対照学習損失: 0.0006038772407919168
対照学習損失: 0.0001652996288612485
{'loss': 0.0004, 'grad_norm': 0.01466564554721117, 'learning_rate': 4.2800000000000004e-05, 'epoch': 0.39}
対照学習損失: 0.004951693117618561
対照学習損失: 0.0012007653713226318
{'loss': 0.0031, 'grad_norm': 0.23780208826065063, 'learning_rate': 4.29e-05, 'epoch': 0.39}
対照学習損失: 0.00019313831580802798
対照学習損失: 2.9317023290786892e-05
{'loss': 0.0001, 'grad_norm': 0.00423581525683403, 'learning_rate': 4.3e-05, 'epoch': 0.4}
対照学習損失: 0.000934242270886898
対照学習損失: 0.018496176227927208
{'loss': 0.0097, 'grad_norm': 0.5563111305236816, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.4}
対照学習損失: 3.825718886218965e-05
対照学習損失: 0.0005412664031609893
{'loss': 0.0003, 'grad_norm': 0.015619037672877312, 'learning_rate': 4.32e-05, 'epoch': 0.4}
対照学習損失: 0.0007760570151731372
対照学習損失: 0.00039189448580145836
{'loss': 0.0006, 'grad_norm': 0.029549576342105865, 'learning_rate': 4.33e-05, 'epoch': 0.4}
対照学習損失: 0.0001716251572361216
対照学習損失: 0.013951050117611885
{'loss': 0.0071, 'grad_norm': 0.4779033064842224, 'learning_rate': 4.3400000000000005e-05, 'epoch': 0.4}
対照学習損失: 0.0008359324419870973
対照学習損失: 1.794063427951187e-05
{'loss': 0.0004, 'grad_norm': 0.01710030436515808, 'learning_rate': 4.35e-05, 'epoch': 0.4}
対照学習損失: 6.800268602091819e-05
対照学習損失: 0.000198187175556086
{'loss': 0.0001, 'grad_norm': 0.006038511171936989, 'learning_rate': 4.36e-05, 'epoch': 0.4}
対照学習損失: 0.00016060103371273726
対照学習損失: 0.01333005540072918
{'loss': 0.0067, 'grad_norm': 0.29309725761413574, 'learning_rate': 4.3700000000000005e-05, 'epoch': 0.4}
対照学習損失: 0.00048428322770632803
対照学習損失: 0.010450742207467556
{'loss': 0.0055, 'grad_norm': 0.4058097004890442, 'learning_rate': 4.38e-05, 'epoch': 0.4}
対照学習損失: 0.00012049387441948056
対照学習損失: 0.0030536099802702665
{'loss': 0.0016, 'grad_norm': 0.10059970617294312, 'learning_rate': 4.39e-05, 'epoch': 0.4}
対照学習損失: 7.74847503635101e-05
対照学習損失: 0.00025491471751593053
{'loss': 0.0002, 'grad_norm': 0.006815689150243998, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.4}
対照学習損失: 6.950159149710089e-05
対照学習損失: 0.00014178248238749802
{'loss': 0.0001, 'grad_norm': 0.007616670802235603, 'learning_rate': 4.41e-05, 'epoch': 0.41}
対照学習損失: 0.00021084246691316366
対照学習損失: 0.00012561128824017942
{'loss': 0.0002, 'grad_norm': 0.005003057885915041, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.41}
対照学習損失: 0.00024909741478040814
対照学習損失: 0.0007205299334600568
{'loss': 0.0005, 'grad_norm': 0.059531476348638535, 'learning_rate': 4.43e-05, 'epoch': 0.41}
対照学習損失: 0.0003370865888427943
対照学習損失: 0.0020696124993264675
{'loss': 0.0012, 'grad_norm': 0.06210263818502426, 'learning_rate': 4.44e-05, 'epoch': 0.41}
対照学習損失: 0.04366764798760414
対照学習損失: 0.0007361457101069391
{'loss': 0.0222, 'grad_norm': 1.035277009010315, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.41}
対照学習損失: 0.0003771559859160334
対照学習損失: 6.0248370573390275e-05
{'loss': 0.0002, 'grad_norm': 0.009571028873324394, 'learning_rate': 4.46e-05, 'epoch': 0.41}
対照学習損失: 0.000762669718824327
対照学習損失: 0.0013789942022413015
{'loss': 0.0011, 'grad_norm': 0.048161666840314865, 'learning_rate': 4.47e-05, 'epoch': 0.41}
対照学習損失: 0.001666889525949955
対照学習損失: 0.00011384410026948899
{'loss': 0.0009, 'grad_norm': 0.09825421124696732, 'learning_rate': 4.4800000000000005e-05, 'epoch': 0.41}
対照学習損失: 0.00016023052739910781
対照学習損失: 3.412257501622662e-05
{'loss': 0.0001, 'grad_norm': 0.0033352661412209272, 'learning_rate': 4.49e-05, 'epoch': 0.41}
対照学習損失: 0.0007042164215818048
対照学習損失: 9.496296115685254e-05
{'loss': 0.0004, 'grad_norm': 0.027883851900696754, 'learning_rate': 4.5e-05, 'epoch': 0.41}
対照学習損失: 0.00010482436482561752
対照学習損失: 8.554988744435832e-05
{'loss': 0.0001, 'grad_norm': 0.0036827127914875746, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.41}
対照学習損失: 0.0010131009621545672
対照学習損失: 0.00018327354337088764
{'loss': 0.0006, 'grad_norm': 0.05174264311790466, 'learning_rate': 4.52e-05, 'epoch': 0.42}
対照学習損失: 0.00018090629600919783
対照学習損失: 0.0002790066064335406
{'loss': 0.0002, 'grad_norm': 0.00877449568361044, 'learning_rate': 4.53e-05, 'epoch': 0.42}
対照学習損失: 0.001521609490737319
対照学習損失: 0.004822915885597467
{'loss': 0.0032, 'grad_norm': 0.15053309500217438, 'learning_rate': 4.5400000000000006e-05, 'epoch': 0.42}
対照学習損失: 8.234959386754781e-05
対照学習損失: 0.006499234586954117
{'loss': 0.0033, 'grad_norm': 0.2114575058221817, 'learning_rate': 4.55e-05, 'epoch': 0.42}
対照学習損失: 0.002211999846622348
対照学習損失: 0.00014105369336903095
{'loss': 0.0012, 'grad_norm': 0.08808831125497818, 'learning_rate': 4.5600000000000004e-05, 'epoch': 0.42}
対照学習損失: 7.633675704710186e-05
対照学習損失: 0.0001289465872105211
{'loss': 0.0001, 'grad_norm': 0.003496333956718445, 'learning_rate': 4.5700000000000006e-05, 'epoch': 0.42}
対照学習損失: 0.0009668749407865107
対照学習損失: 0.0002581738808657974
{'loss': 0.0006, 'grad_norm': 0.023480482399463654, 'learning_rate': 4.58e-05, 'epoch': 0.42}
対照学習損失: 0.0007759841391816735
対照学習損失: 0.005510276183485985
{'loss': 0.0031, 'grad_norm': 0.1542677879333496, 'learning_rate': 4.5900000000000004e-05, 'epoch': 0.42}
対照学習損失: 2.215751555922907e-05
対照学習損失: 0.001069442369043827
{'loss': 0.0005, 'grad_norm': 0.050603315234184265, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.42}
対照学習損失: 0.00010343080793973058
対照学習損失: 0.0010541585506871343
{'loss': 0.0006, 'grad_norm': 0.04343456029891968, 'learning_rate': 4.61e-05, 'epoch': 0.42}
対照学習損失: 0.0001681889989413321
対照学習損失: 8.731117122806609e-05
{'loss': 0.0001, 'grad_norm': 0.00853718165308237, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.42}
対照学習損失: 0.0010612638434395194
対照学習損失: 0.00016103731468319893
{'loss': 0.0006, 'grad_norm': 0.037613645195961, 'learning_rate': 4.630000000000001e-05, 'epoch': 0.43}
対照学習損失: 0.0006296582287177444
対照学習損失: 4.363479092717171e-05
{'loss': 0.0003, 'grad_norm': 0.054005108773708344, 'learning_rate': 4.64e-05, 'epoch': 0.43}
対照学習損失: 0.00011724684009095654
対照学習損失: 7.36424553906545e-05
{'loss': 0.0001, 'grad_norm': 0.0029855624306946993, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.43}
対照学習損失: 7.800529419910163e-05
対照学習損失: 0.00014361772628035396
{'loss': 0.0001, 'grad_norm': 0.005938522983342409, 'learning_rate': 4.660000000000001e-05, 'epoch': 0.43}
対照学習損失: 0.00172708579339087
対照学習損失: 0.0004463578516151756
{'loss': 0.0011, 'grad_norm': 0.03156515955924988, 'learning_rate': 4.6700000000000003e-05, 'epoch': 0.43}
対照学習損失: 0.0005347534315660596
対照学習損失: 3.487466528895311e-05
{'loss': 0.0003, 'grad_norm': 0.009651071391999722, 'learning_rate': 4.6800000000000006e-05, 'epoch': 0.43}
対照学習損失: 7.554506737506017e-05
対照学習損失: 0.0002419603697489947
{'loss': 0.0002, 'grad_norm': 0.006275283172726631, 'learning_rate': 4.69e-05, 'epoch': 0.43}
対照学習損失: 0.0006960176397114992
対照学習損失: 0.0009798145620152354
{'loss': 0.0008, 'grad_norm': 0.05793210119009018, 'learning_rate': 4.7e-05, 'epoch': 0.43}
対照学習損失: 0.00020508207671809942
対照学習損失: 0.009179494343698025
{'loss': 0.0047, 'grad_norm': 0.40168890357017517, 'learning_rate': 4.71e-05, 'epoch': 0.43}
対照学習損失: 0.0007344858022406697
対照学習損失: 0.00022869647364132106
{'loss': 0.0005, 'grad_norm': 0.047285087406635284, 'learning_rate': 4.72e-05, 'epoch': 0.43}
対照学習損失: 0.0009996953886002302
対照学習損失: 0.00010964237299049273
{'loss': 0.0006, 'grad_norm': 0.03191547468304634, 'learning_rate': 4.73e-05, 'epoch': 0.43}
対照学習損失: 0.00016106526891235262
対照学習損失: 0.00025582878151908517
{'loss': 0.0002, 'grad_norm': 0.01317460834980011, 'learning_rate': 4.74e-05, 'epoch': 0.44}
対照学習損失: 0.0002477727539371699
対照学習損失: 0.00299853109754622
{'loss': 0.0016, 'grad_norm': 0.25639569759368896, 'learning_rate': 4.75e-05, 'epoch': 0.44}
対照学習損失: 0.0007268015760928392
対照学習損失: 0.0002878251252695918
{'loss': 0.0005, 'grad_norm': 0.021757561713457108, 'learning_rate': 4.76e-05, 'epoch': 0.44}
対照学習損失: 0.00047152716433629394
対照学習損失: 0.0001741596934152767
{'loss': 0.0003, 'grad_norm': 0.015084664337337017, 'learning_rate': 4.77e-05, 'epoch': 0.44}
対照学習損失: 0.001260943477973342
対照学習損失: 0.0005133265512995422
{'loss': 0.0009, 'grad_norm': 0.03222934529185295, 'learning_rate': 4.78e-05, 'epoch': 0.44}
対照学習損失: 0.0037085905205458403
対照学習損失: 0.00047681230353191495
{'loss': 0.0021, 'grad_norm': 0.11902422457933426, 'learning_rate': 4.79e-05, 'epoch': 0.44}
対照学習損失: 0.0005284586804918945
対照学習損失: 0.0004918324411846697
{'loss': 0.0005, 'grad_norm': 0.033987101167440414, 'learning_rate': 4.8e-05, 'epoch': 0.44}
対照学習損失: 0.00015701810480095446
対照学習損失: 0.0009752926998771727
{'loss': 0.0006, 'grad_norm': 0.02635064721107483, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.44}
対照学習損失: 0.01614048145711422
対照学習損失: 0.00010972050949931145
{'loss': 0.0081, 'grad_norm': 0.47317594289779663, 'learning_rate': 4.82e-05, 'epoch': 0.44}
対照学習損失: 0.0009070959058590233
対照学習損失: 0.00045574858086183667
{'loss': 0.0007, 'grad_norm': 0.050618015229701996, 'learning_rate': 4.83e-05, 'epoch': 0.44}
対照学習損失: 6.346654845401645e-05
対照学習損失: 8.458946831524372e-05
{'loss': 0.0001, 'grad_norm': 0.0017983400030061603, 'learning_rate': 4.8400000000000004e-05, 'epoch': 0.44}
対照学習損失: 0.0007178768282756209
対照学習損失: 0.00011448634904809296
{'loss': 0.0004, 'grad_norm': 0.019682858139276505, 'learning_rate': 4.85e-05, 'epoch': 0.45}
対照学習損失: 0.031992439180612564
対照学習損失: 0.01661580801010132
{'loss': 0.0243, 'grad_norm': 2.013615846633911, 'learning_rate': 4.86e-05, 'epoch': 0.45}
対照学習損失: 4.716615512734279e-05
対照学習損失: 0.0011464140843600035
{'loss': 0.0006, 'grad_norm': 0.09902972728013992, 'learning_rate': 4.87e-05, 'epoch': 0.45}
対照学習損失: 6.566139927599579e-05
対照学習損失: 0.0002985744795296341
{'loss': 0.0002, 'grad_norm': 0.01011232752352953, 'learning_rate': 4.88e-05, 'epoch': 0.45}
対照学習損失: 0.017654767259955406
対照学習損失: 0.047766659408807755
{'loss': 0.0327, 'grad_norm': 1.4771106243133545, 'learning_rate': 4.89e-05, 'epoch': 0.45}
対照学習損失: 3.161919448757544e-05
対照学習損失: 0.00012152681301813573
{'loss': 0.0001, 'grad_norm': 0.0022705525625497103, 'learning_rate': 4.9e-05, 'epoch': 0.45}
対照学習損失: 0.00014739666949026287
対照学習損失: 0.0012172552524134517
{'loss': 0.0007, 'grad_norm': 0.03822402283549309, 'learning_rate': 4.91e-05, 'epoch': 0.45}
対照学習損失: 0.0004731951921712607
対照学習損失: 7.605784776387736e-05
{'loss': 0.0003, 'grad_norm': 0.012107237242162228, 'learning_rate': 4.92e-05, 'epoch': 0.45}
対照学習損失: 0.0001584124838700518
対照学習損失: 0.0007287729531526566
{'loss': 0.0004, 'grad_norm': 0.04818624258041382, 'learning_rate': 4.93e-05, 'epoch': 0.45}
対照学習損失: 9.916460112435743e-05
対照学習損失: 0.0011552262585610151
{'loss': 0.0006, 'grad_norm': 0.023492567241191864, 'learning_rate': 4.94e-05, 'epoch': 0.45}
対照学習損失: 0.0004452564171515405
対照学習損失: 0.00018276563787367195
{'loss': 0.0003, 'grad_norm': 0.016544541344046593, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.45}
対照学習損失: 0.0003392492653802037
対照学習損失: 0.0005092975916340947
{'loss': 0.0004, 'grad_norm': 0.014698690734803677, 'learning_rate': 4.96e-05, 'epoch': 0.46}
対照学習損失: 9.921018499881029e-05
対照学習損失: 0.00041145551949739456
{'loss': 0.0003, 'grad_norm': 0.008727742359042168, 'learning_rate': 4.97e-05, 'epoch': 0.46}
対照学習損失: 6.409308116417378e-05
対照学習損失: 0.0005612563109025359
{'loss': 0.0003, 'grad_norm': 0.031880080699920654, 'learning_rate': 4.9800000000000004e-05, 'epoch': 0.46}
対照学習損失: 0.0004618615785147995
対照学習損失: 0.00018347031436860561
{'loss': 0.0003, 'grad_norm': 0.01152838859707117, 'learning_rate': 4.99e-05, 'epoch': 0.46}
対照学習損失: 1.8767459550872445e-05
対照学習損失: 7.351206295425072e-05
{'loss': 0.0, 'grad_norm': 0.0022220839746296406, 'learning_rate': 5e-05, 'epoch': 0.46}
対照学習損失: 0.0003799705009441823
対照学習損失: 7.24473429727368e-05
{'loss': 0.0002, 'grad_norm': 0.013704529032111168, 'learning_rate': 4.998194945848376e-05, 'epoch': 0.46}
対照学習損失: 0.0026104922872036695
対照学習損失: 0.00046893046237528324
{'loss': 0.0015, 'grad_norm': 0.267439067363739, 'learning_rate': 4.996389891696751e-05, 'epoch': 0.46}
対照学習損失: 0.00011135716340504587
対照学習損失: 6.62107631796971e-05
{'loss': 0.0001, 'grad_norm': 0.003221627324819565, 'learning_rate': 4.994584837545127e-05, 'epoch': 0.46}
対照学習損失: 7.261885184561834e-05
対照学習損失: 0.00018936816195491701
{'loss': 0.0001, 'grad_norm': 0.007695197593420744, 'learning_rate': 4.992779783393502e-05, 'epoch': 0.46}
対照学習損失: 0.0006668656715191901
対照学習損失: 0.006685191299766302
{'loss': 0.0037, 'grad_norm': 0.4721790552139282, 'learning_rate': 4.9909747292418775e-05, 'epoch': 0.46}
対照学習損失: 2.4310400476679206e-05
対照学習損失: 0.00019964599050581455
{'loss': 0.0001, 'grad_norm': 0.009861492551863194, 'learning_rate': 4.9891696750902525e-05, 'epoch': 0.46}
対照学習損失: 7.180026295827702e-05
対照学習損失: 0.00014411819574888796
{'loss': 0.0001, 'grad_norm': 0.0067539154551923275, 'learning_rate': 4.987364620938628e-05, 'epoch': 0.47}
対照学習損失: 5.668649100698531e-05
対照学習損失: 0.011770409531891346
{'loss': 0.0059, 'grad_norm': 0.21102683246135712, 'learning_rate': 4.985559566787004e-05, 'epoch': 0.47}
対照学習損失: 0.00013073330046609044
対照学習損失: 3.984166687587276e-05
{'loss': 0.0001, 'grad_norm': 0.003837580792605877, 'learning_rate': 4.983754512635379e-05, 'epoch': 0.47}
対照学習損失: 7.901698700152338e-05
対照学習損失: 2.7990852686343715e-05
{'loss': 0.0001, 'grad_norm': 0.001687119947746396, 'learning_rate': 4.981949458483755e-05, 'epoch': 0.47}
対照学習損失: 0.002650987822562456
対照学習損失: 7.891866698628291e-05
{'loss': 0.0014, 'grad_norm': 0.10056700557470322, 'learning_rate': 4.9801444043321305e-05, 'epoch': 0.47}
対照学習損失: 0.0003062171454075724
対照学習損失: 0.0001426340313628316
{'loss': 0.0002, 'grad_norm': 0.019488753750920296, 'learning_rate': 4.978339350180506e-05, 'epoch': 0.47}
対照学習損失: 9.701304225018248e-05
対照学習損失: 0.0007778261788189411
{'loss': 0.0004, 'grad_norm': 0.030706238001585007, 'learning_rate': 4.976534296028881e-05, 'epoch': 0.47}
対照学習損失: 5.5077209253795445e-05
対照学習損失: 5.2692128519993275e-05
{'loss': 0.0001, 'grad_norm': 0.0020977661479264498, 'learning_rate': 4.974729241877257e-05, 'epoch': 0.47}
対照学習損失: 0.00010488645057193935
対照学習損失: 0.00024303604732267559
{'loss': 0.0002, 'grad_norm': 0.008040213957428932, 'learning_rate': 4.972924187725632e-05, 'epoch': 0.47}
対照学習損失: 0.0005887070437893271
対照学習損失: 0.0012445321772247553
{'loss': 0.0009, 'grad_norm': 0.0327296145260334, 'learning_rate': 4.971119133574008e-05, 'epoch': 0.47}
対照学習損失: 0.03851563110947609
対照学習損失: 0.00014962520799599588
{'loss': 0.0193, 'grad_norm': 3.9848616123199463, 'learning_rate': 4.969314079422383e-05, 'epoch': 0.48}
対照学習損失: 0.00014174636453390121
対照学習損失: 0.00018967367941513658
{'loss': 0.0002, 'grad_norm': 0.0059142648242414, 'learning_rate': 4.9675090252707585e-05, 'epoch': 0.48}
対照学習損失: 0.0007077775662764907
対照学習損失: 5.496948870131746e-05
{'loss': 0.0004, 'grad_norm': 0.06606291234493256, 'learning_rate': 4.965703971119134e-05, 'epoch': 0.48}
対照学習損失: 0.00038427559775300324
対照学習損失: 3.383092553121969e-05
{'loss': 0.0002, 'grad_norm': 0.017061872407794, 'learning_rate': 4.963898916967509e-05, 'epoch': 0.48}
対照学習損失: 0.0007378087029792368
対照学習損失: 0.00020540444529615343
{'loss': 0.0005, 'grad_norm': 0.022706205025315285, 'learning_rate': 4.962093862815885e-05, 'epoch': 0.48}
対照学習損失: 5.047926970291883e-05
対照学習損失: 7.551396993221715e-05
{'loss': 0.0001, 'grad_norm': 0.002343457192182541, 'learning_rate': 4.96028880866426e-05, 'epoch': 0.48}
対照学習損失: 0.001906226621940732
対照学習損失: 0.00013096389011479914
{'loss': 0.001, 'grad_norm': 0.05672568082809448, 'learning_rate': 4.958483754512636e-05, 'epoch': 0.48}
対照学習損失: 0.00024445238523185253
対照学習損失: 0.017803264781832695
{'loss': 0.009, 'grad_norm': 0.6232329607009888, 'learning_rate': 4.956678700361011e-05, 'epoch': 0.48}
対照学習損失: 5.705831426894292e-05
対照学習損失: 4.597527004079893e-05
{'loss': 0.0001, 'grad_norm': 0.0013386330101639032, 'learning_rate': 4.9548736462093865e-05, 'epoch': 0.48}
対照学習損失: 0.0007228816393762827
対照学習損失: 0.0009897968266159296
{'loss': 0.0009, 'grad_norm': 0.04158011078834534, 'learning_rate': 4.953068592057762e-05, 'epoch': 0.48}
対照学習損失: 5.285117731546052e-05
対照学習損失: 0.00011042936239391565
{'loss': 0.0001, 'grad_norm': 0.005849658045917749, 'learning_rate': 4.951263537906137e-05, 'epoch': 0.48}
対照学習損失: 0.0001014494919218123
対照学習損失: 0.0005339927738532424
{'loss': 0.0003, 'grad_norm': 0.026776758953928947, 'learning_rate': 4.949458483754513e-05, 'epoch': 0.49}
対照学習損失: 0.010699127800762653
対照学習損失: 3.164093504892662e-05
{'loss': 0.0054, 'grad_norm': 0.8860551714897156, 'learning_rate': 4.947653429602888e-05, 'epoch': 0.49}
対照学習損失: 4.895441816188395e-05
対照学習損失: 6.305979331955314e-05
{'loss': 0.0001, 'grad_norm': 0.003184436820447445, 'learning_rate': 4.945848375451264e-05, 'epoch': 0.49}
対照学習損失: 8.988220361061394e-05
対照学習損失: 0.0002816513879224658
{'loss': 0.0002, 'grad_norm': 0.011691494844853878, 'learning_rate': 4.944043321299639e-05, 'epoch': 0.49}
対照学習損失: 9.388675971422344e-05
対照学習損失: 2.6060162781504914e-05
{'loss': 0.0001, 'grad_norm': 0.0058655403554439545, 'learning_rate': 4.9422382671480145e-05, 'epoch': 0.49}
対照学習損失: 8.117269317153841e-05
対照学習損失: 9.424216841580346e-05
{'loss': 0.0001, 'grad_norm': 0.003006362123414874, 'learning_rate': 4.94043321299639e-05, 'epoch': 0.49}
対照学習損失: 4.2880936234723777e-05
対照学習損失: 0.0002919121761806309
{'loss': 0.0002, 'grad_norm': 0.007501477375626564, 'learning_rate': 4.938628158844766e-05, 'epoch': 0.49}
対照学習損失: 0.00017282231419812888
対照学習損失: 6.690745067317039e-05
{'loss': 0.0001, 'grad_norm': 0.005593548063188791, 'learning_rate': 4.936823104693141e-05, 'epoch': 0.49}
対照学習損失: 3.8427369872806594e-05
対照学習損失: 5.3842868510400876e-05
{'loss': 0.0, 'grad_norm': 0.0014628360513597727, 'learning_rate': 4.935018050541517e-05, 'epoch': 0.49}
対照学習損失: 0.0002739662304520607
対照学習損失: 0.0008411328308284283
{'loss': 0.0006, 'grad_norm': 0.024808717891573906, 'learning_rate': 4.9332129963898924e-05, 'epoch': 0.49}
対照学習損失: 0.00021207061945460737
対照学習損失: 0.00010666026355465874
{'loss': 0.0002, 'grad_norm': 0.010032332502305508, 'learning_rate': 4.9314079422382675e-05, 'epoch': 0.49}
対照学習損失: 0.0020165187306702137
対照学習損失: 0.0017651073867455125
{'loss': 0.0019, 'grad_norm': 0.14099432528018951, 'learning_rate': 4.929602888086643e-05, 'epoch': 0.5}
対照学習損失: 0.0001465418899897486
対照学習損失: 5.6937773479148746e-05
{'loss': 0.0001, 'grad_norm': 0.006234840955585241, 'learning_rate': 4.927797833935018e-05, 'epoch': 0.5}
対照学習損失: 0.0003166783135384321
対照学習損失: 0.00017097762611228973
{'loss': 0.0002, 'grad_norm': 0.033233579248189926, 'learning_rate': 4.925992779783394e-05, 'epoch': 0.5}
対照学習損失: 0.0029711052775382996
対照学習損失: 0.0001402792549924925
{'loss': 0.0016, 'grad_norm': 0.22172383964061737, 'learning_rate': 4.924187725631769e-05, 'epoch': 0.5}
対照学習損失: 2.100275014527142e-05
対照学習損失: 0.0013157679932191968
{'loss': 0.0007, 'grad_norm': 0.03400446102023125, 'learning_rate': 4.922382671480145e-05, 'epoch': 0.5}
対照学習損失: 2.9741409889538772e-05
対照学習損失: 0.00012037484702887014
{'loss': 0.0001, 'grad_norm': 0.0037191701121628284, 'learning_rate': 4.92057761732852e-05, 'epoch': 0.5}
対照学習損失: 0.0003340981784276664
対照学習損失: 0.0005739229382015765
{'loss': 0.0005, 'grad_norm': 0.023420369252562523, 'learning_rate': 4.9187725631768955e-05, 'epoch': 0.5}
対照学習損失: 0.0002076325472444296
対照学習損失: 0.0012718409998342395
{'loss': 0.0007, 'grad_norm': 0.05659416317939758, 'learning_rate': 4.916967509025271e-05, 'epoch': 0.5}
対照学習損失: 0.00014267547521740198
対照学習損失: 4.390216417959891e-05
{'loss': 0.0001, 'grad_norm': 0.0025794189423322678, 'learning_rate': 4.915162454873646e-05, 'epoch': 0.5}
対照学習損失: 0.006758378352969885
対照学習損失: 0.001101303263567388
{'loss': 0.0039, 'grad_norm': 0.39524686336517334, 'learning_rate': 4.913357400722022e-05, 'epoch': 0.5}
対照学習損失: 4.699591954704374e-05
対照学習損失: 0.011603273451328278
{'loss': 0.0058, 'grad_norm': 0.8500744104385376, 'learning_rate': 4.911552346570397e-05, 'epoch': 0.5}
対照学習損失: 0.0005647435318678617
対照学習損失: 6.607124669244513e-05
{'loss': 0.0003, 'grad_norm': 0.020653750747442245, 'learning_rate': 4.909747292418773e-05, 'epoch': 0.51}
対照学習損失: 2.5739154807524756e-05
対照学習損失: 0.001535826246254146
{'loss': 0.0008, 'grad_norm': 0.03933890536427498, 'learning_rate': 4.907942238267148e-05, 'epoch': 0.51}
対照学習損失: 0.0021238168701529503
対照学習損失: 0.0010142186656594276
{'loss': 0.0016, 'grad_norm': 0.07153502106666565, 'learning_rate': 4.9061371841155235e-05, 'epoch': 0.51}
対照学習損失: 0.00042565292096696794
対照学習損失: 5.9340305597288534e-05
{'loss': 0.0002, 'grad_norm': 0.010031147859990597, 'learning_rate': 4.904332129963899e-05, 'epoch': 0.51}
対照学習損失: 0.004132947418838739
対照学習損失: 0.00030763205722905695
{'loss': 0.0022, 'grad_norm': 0.18475690484046936, 'learning_rate': 4.902527075812274e-05, 'epoch': 0.51}
対照学習損失: 4.114689363632351e-05
対照学習損失: 0.008006651885807514
{'loss': 0.004, 'grad_norm': 0.4580604135990143, 'learning_rate': 4.9007220216606506e-05, 'epoch': 0.51}
対照学習損失: 1.7947941159945913e-05
対照学習損失: 0.0001440049527445808
{'loss': 0.0001, 'grad_norm': 0.0037558800540864468, 'learning_rate': 4.898916967509026e-05, 'epoch': 0.51}
対照学習損失: 0.00012909101496916264
対照学習損失: 0.0005968473269604146
{'loss': 0.0004, 'grad_norm': 0.026181437075138092, 'learning_rate': 4.8971119133574014e-05, 'epoch': 0.51}
対照学習損失: 7.824275235179812e-05
対照学習損失: 3.330997787998058e-05
{'loss': 0.0001, 'grad_norm': 0.002461346099153161, 'learning_rate': 4.8953068592057764e-05, 'epoch': 0.51}
対照学習損失: 3.6289689887780696e-05
対照学習損失: 3.730936805368401e-05
{'loss': 0.0, 'grad_norm': 0.001118505490012467, 'learning_rate': 4.893501805054152e-05, 'epoch': 0.51}
対照学習損失: 0.023864977061748505
対照学習損失: 0.0002485179284121841
{'loss': 0.0121, 'grad_norm': 0.7581583261489868, 'learning_rate': 4.891696750902527e-05, 'epoch': 0.51}
対照学習損失: 0.06825114786624908
対照学習損失: 3.167793693137355e-05
{'loss': 0.0341, 'grad_norm': 2.992460250854492, 'learning_rate': 4.889891696750903e-05, 'epoch': 0.52}
対照学習損失: 0.004600848536938429
対照学習損失: 9.697449422674254e-05
{'loss': 0.0023, 'grad_norm': 0.28435859084129333, 'learning_rate': 4.888086642599278e-05, 'epoch': 0.52}
対照学習損失: 0.00024395080981776118
対照学習損失: 4.065412576892413e-05
{'loss': 0.0001, 'grad_norm': 0.005313661880791187, 'learning_rate': 4.886281588447654e-05, 'epoch': 0.52}
対照学習損失: 0.00011325209925416857
対照学習損失: 0.0001748427312122658
{'loss': 0.0001, 'grad_norm': 0.01676010526716709, 'learning_rate': 4.8844765342960294e-05, 'epoch': 0.52}
対照学習損失: 0.0075975763611495495
対照学習損失: 0.011439966037869453
{'loss': 0.0095, 'grad_norm': 0.7413060665130615, 'learning_rate': 4.8826714801444045e-05, 'epoch': 0.52}
対照学習損失: 5.5547348893014714e-05
対照学習損失: 3.0158629670040682e-05
{'loss': 0.0, 'grad_norm': 0.0014585463795810938, 'learning_rate': 4.88086642599278e-05, 'epoch': 0.52}
対照学習損失: 0.00012376494123600423
対照学習損失: 0.00018584671488497406
{'loss': 0.0002, 'grad_norm': 0.005684099160134792, 'learning_rate': 4.879061371841155e-05, 'epoch': 0.52}
対照学習損失: 2.4183978894143365e-05
対照学習損失: 5.004904596717097e-05
{'loss': 0.0, 'grad_norm': 0.0013538369676098228, 'learning_rate': 4.877256317689531e-05, 'epoch': 0.52}
対照学習損失: 0.0005327489925548434
対照学習損失: 0.03235992416739464
{'loss': 0.0164, 'grad_norm': 0.8396668434143066, 'learning_rate': 4.875451263537906e-05, 'epoch': 0.52}
対照学習損失: 0.0007100789807736874
対照学習損失: 4.209982944303192e-05
{'loss': 0.0004, 'grad_norm': 0.02407214045524597, 'learning_rate': 4.873646209386282e-05, 'epoch': 0.52}
対照学習損失: 0.00015599567268509418
対照学習損失: 4.9630336434347555e-05
{'loss': 0.0001, 'grad_norm': 0.009985445998609066, 'learning_rate': 4.8718411552346574e-05, 'epoch': 0.52}
対照学習損失: 5.991605939925648e-05
対照学習損失: 8.504086872562766e-05
{'loss': 0.0001, 'grad_norm': 0.0020913167390972376, 'learning_rate': 4.8700361010830325e-05, 'epoch': 0.53}
対照学習損失: 0.007547819055616856
対照学習損失: 0.0003006812767125666
{'loss': 0.0039, 'grad_norm': 0.24415422976016998, 'learning_rate': 4.868231046931408e-05, 'epoch': 0.53}
対照学習損失: 0.0020470486488193274
対照学習損失: 0.0005256166914477944
{'loss': 0.0013, 'grad_norm': 0.1252756267786026, 'learning_rate': 4.866425992779783e-05, 'epoch': 0.53}
対照学習損失: 0.007307800929993391
対照学習損失: 7.880640623625368e-05
{'loss': 0.0037, 'grad_norm': 0.10752590745687485, 'learning_rate': 4.864620938628159e-05, 'epoch': 0.53}
対照学習損失: 0.0004546024138107896
対照学習損失: 0.002447787206619978
{'loss': 0.0015, 'grad_norm': 0.11168034374713898, 'learning_rate': 4.862815884476535e-05, 'epoch': 0.53}
対照学習損失: 0.00012313088518567383
対照学習損失: 0.00017595769895706326
{'loss': 0.0001, 'grad_norm': 0.01647189073264599, 'learning_rate': 4.8610108303249104e-05, 'epoch': 0.53}
対照学習損失: 6.546157965203747e-05
対照学習損失: 0.0002057552628684789
{'loss': 0.0001, 'grad_norm': 0.0049133007414639, 'learning_rate': 4.8592057761732854e-05, 'epoch': 0.53}
対照学習損失: 0.0003166770620737225
対照学習損失: 0.0022626295685768127
{'loss': 0.0013, 'grad_norm': 0.07649821043014526, 'learning_rate': 4.857400722021661e-05, 'epoch': 0.53}
対照学習損失: 0.00025810900842770934
対照学習損失: 0.005600172095000744
{'loss': 0.0029, 'grad_norm': 0.21084342896938324, 'learning_rate': 4.855595667870036e-05, 'epoch': 0.53}
対照学習損失: 2.772311017906759e-05
対照学習損失: 0.0001964685507118702
{'loss': 0.0001, 'grad_norm': 0.009183214977383614, 'learning_rate': 4.853790613718412e-05, 'epoch': 0.53}
対照学習損失: 0.0003244693798478693
対照学習損失: 0.0003213257296010852
{'loss': 0.0003, 'grad_norm': 0.013147330842912197, 'learning_rate': 4.8519855595667876e-05, 'epoch': 0.53}
対照学習損失: 0.0005614252295345068
対照学習損失: 0.0038474651519209146
{'loss': 0.0022, 'grad_norm': 0.30747541785240173, 'learning_rate': 4.850180505415163e-05, 'epoch': 0.54}
対照学習損失: 3.635674875113182e-05
対照学習損失: 0.004559652879834175
{'loss': 0.0023, 'grad_norm': 0.16332769393920898, 'learning_rate': 4.8483754512635384e-05, 'epoch': 0.54}
対照学習損失: 4.228705074638128e-05
対照学習損失: 7.071318395901471e-05
{'loss': 0.0001, 'grad_norm': 0.0016197043005377054, 'learning_rate': 4.8465703971119134e-05, 'epoch': 0.54}
対照学習損失: 0.00026394613087177277
対照学習損失: 0.002740064635872841
{'loss': 0.0015, 'grad_norm': 0.10818526148796082, 'learning_rate': 4.844765342960289e-05, 'epoch': 0.54}
対照学習損失: 0.00030355583294294775
対照学習損失: 5.167773997527547e-05
{'loss': 0.0002, 'grad_norm': 0.008820666000247002, 'learning_rate': 4.842960288808664e-05, 'epoch': 0.54}
対照学習損失: 0.00020367397519294173
対照学習損失: 0.00036839215317741036
{'loss': 0.0003, 'grad_norm': 0.010677887126803398, 'learning_rate': 4.84115523465704e-05, 'epoch': 0.54}
対照学習損失: 0.00040606758557260036
対照学習損失: 0.00010294627281837165
{'loss': 0.0003, 'grad_norm': 0.012853370048105717, 'learning_rate': 4.8393501805054156e-05, 'epoch': 0.54}
対照学習損失: 0.0010423950152471662
対照学習損失: 0.00028813877725042403
{'loss': 0.0007, 'grad_norm': 0.06470312178134918, 'learning_rate': 4.837545126353791e-05, 'epoch': 0.54}
対照学習損失: 0.00045636421418748796
対照学習損失: 5.300309567246586e-05
{'loss': 0.0003, 'grad_norm': 0.01583884470164776, 'learning_rate': 4.8357400722021664e-05, 'epoch': 0.54}
対照学習損失: 4.369371526991017e-05
対照学習損失: 0.00017875185585580766
{'loss': 0.0001, 'grad_norm': 0.008299196138978004, 'learning_rate': 4.8339350180505415e-05, 'epoch': 0.54}
対照学習損失: 0.009752373211085796
対照学習損失: 0.00018957111751660705
{'loss': 0.005, 'grad_norm': 0.2700614333152771, 'learning_rate': 4.832129963898917e-05, 'epoch': 0.54}
対照学習損失: 1.8842054487322457e-05
対照学習損失: 0.0001369839155813679
{'loss': 0.0001, 'grad_norm': 0.0032896592747420073, 'learning_rate': 4.830324909747292e-05, 'epoch': 0.55}
対照学習損失: 0.0005224326741881669
対照学習損失: 0.00032393811852671206
{'loss': 0.0004, 'grad_norm': 0.038693979382514954, 'learning_rate': 4.828519855595668e-05, 'epoch': 0.55}
対照学習損失: 0.010447532869875431
対照学習損失: 3.3399908716091886e-05
{'loss': 0.0052, 'grad_norm': 0.28482192754745483, 'learning_rate': 4.826714801444043e-05, 'epoch': 0.55}
対照学習損失: 0.00016920684720389545
対照学習損失: 7.111567538231611e-05
{'loss': 0.0001, 'grad_norm': 0.006033186335116625, 'learning_rate': 4.824909747292419e-05, 'epoch': 0.55}
対照学習損失: 0.0001844988582888618
対照学習損失: 0.0005725477822124958
{'loss': 0.0004, 'grad_norm': 0.052475471049547195, 'learning_rate': 4.8231046931407944e-05, 'epoch': 0.55}
対照学習損失: 0.0005437037325464189
対照学習損失: 0.0756467804312706
{'loss': 0.0381, 'grad_norm': 4.85006856918335, 'learning_rate': 4.82129963898917e-05, 'epoch': 0.55}
対照学習損失: 0.0003055857087019831
対照学習損失: 9.169556869892403e-05
{'loss': 0.0002, 'grad_norm': 0.00935107097029686, 'learning_rate': 4.819494584837546e-05, 'epoch': 0.55}
対照学習損失: 0.0003463059547357261
対照学習損失: 0.00044711242662742734
{'loss': 0.0004, 'grad_norm': 0.02319137379527092, 'learning_rate': 4.817689530685921e-05, 'epoch': 0.55}
対照学習損失: 0.0001800298341549933
対照学習損失: 0.0005243771011009812
{'loss': 0.0004, 'grad_norm': 0.01667802222073078, 'learning_rate': 4.8158844765342966e-05, 'epoch': 0.55}
対照学習損失: 4.986119893146679e-05
対照学習損失: 0.0028442805632948875
{'loss': 0.0014, 'grad_norm': 0.1234358623623848, 'learning_rate': 4.814079422382672e-05, 'epoch': 0.55}
対照学習損失: 3.728881347342394e-05
対照学習損失: 5.081190465716645e-05
{'loss': 0.0, 'grad_norm': 0.0012895666295662522, 'learning_rate': 4.8122743682310474e-05, 'epoch': 0.55}
対照学習損失: 0.00010770752851385623
対照学習損失: 8.472300396533683e-05
{'loss': 0.0001, 'grad_norm': 0.005061222240328789, 'learning_rate': 4.8104693140794224e-05, 'epoch': 0.56}
対照学習損失: 0.0001259251293959096
対照学習損失: 3.2319469028152525e-05
{'loss': 0.0001, 'grad_norm': 0.004979870282113552, 'learning_rate': 4.808664259927798e-05, 'epoch': 0.56}
対照学習損失: 0.00040032403194345534
対照学習損失: 0.00027339288499206305
{'loss': 0.0003, 'grad_norm': 0.01977348141372204, 'learning_rate': 4.806859205776174e-05, 'epoch': 0.56}
対照学習損失: 3.503711195662618e-05
対照学習損失: 0.0004520612710621208
{'loss': 0.0002, 'grad_norm': 0.007474272511899471, 'learning_rate': 4.805054151624549e-05, 'epoch': 0.56}
対照学習損失: 0.0002000954991672188
対照学習損失: 0.00015238040941767395
{'loss': 0.0002, 'grad_norm': 0.009742888621985912, 'learning_rate': 4.8032490974729246e-05, 'epoch': 0.56}
対照学習損失: 0.0005396014894358814
対照学習損失: 0.0001797535951482132
{'loss': 0.0004, 'grad_norm': 0.027620188891887665, 'learning_rate': 4.8014440433213e-05, 'epoch': 0.56}
対照学習損失: 0.0008141061407513916
対照学習損失: 2.7253285225015134e-05
{'loss': 0.0004, 'grad_norm': 0.02557247132062912, 'learning_rate': 4.7996389891696754e-05, 'epoch': 0.56}
対照学習損失: 0.004460696130990982
対照学習損失: 0.003973822575062513
{'loss': 0.0042, 'grad_norm': 0.28906935453414917, 'learning_rate': 4.7978339350180504e-05, 'epoch': 0.56}
対照学習損失: 0.00013168250734452158
対照学習損失: 3.301745164208114e-05
{'loss': 0.0001, 'grad_norm': 0.003697378560900688, 'learning_rate': 4.796028880866426e-05, 'epoch': 0.56}
対照学習損失: 0.0022256080992519855
対照学習損失: 0.00012414441152941436
{'loss': 0.0012, 'grad_norm': 0.04579850286245346, 'learning_rate': 4.794223826714801e-05, 'epoch': 0.56}
対照学習損失: 0.00038580139516852796
対照学習損失: 0.00013615397620014846
{'loss': 0.0003, 'grad_norm': 0.015637140721082687, 'learning_rate': 4.792418772563177e-05, 'epoch': 0.56}
対照学習損失: 0.0027109014336019754
対照学習損失: 0.0001727224444039166
{'loss': 0.0014, 'grad_norm': 0.10794402658939362, 'learning_rate': 4.7906137184115526e-05, 'epoch': 0.57}
対照学習損失: 0.00018954162078443915
対照学習損失: 0.003485606051981449
{'loss': 0.0018, 'grad_norm': 0.19594122469425201, 'learning_rate': 4.788808664259928e-05, 'epoch': 0.57}
対照学習損失: 0.00017008712165988982
対照学習損失: 0.00031137699261307716
{'loss': 0.0002, 'grad_norm': 0.010387646034359932, 'learning_rate': 4.7870036101083034e-05, 'epoch': 0.57}
対照学習損失: 0.0001146829963545315
対照学習損失: 4.1795930883381516e-05
{'loss': 0.0001, 'grad_norm': 0.005549819674342871, 'learning_rate': 4.7851985559566785e-05, 'epoch': 0.57}
対照学習損失: 0.000145873156725429
対照学習損失: 0.00030825595604255795
{'loss': 0.0002, 'grad_norm': 0.00590452179312706, 'learning_rate': 4.783393501805055e-05, 'epoch': 0.57}
対照学習損失: 0.00010477150499355048
対照学習損失: 0.00023640479776076972
{'loss': 0.0002, 'grad_norm': 0.005301163997501135, 'learning_rate': 4.78158844765343e-05, 'epoch': 0.57}
対照学習損失: 6.990209658397362e-05
対照学習損失: 0.003105897456407547
{'loss': 0.0016, 'grad_norm': 0.16083413362503052, 'learning_rate': 4.7797833935018056e-05, 'epoch': 0.57}
対照学習損失: 0.00030676726601086557
対照学習損失: 0.00019398730364628136
{'loss': 0.0003, 'grad_norm': 0.00943813007324934, 'learning_rate': 4.7779783393501807e-05, 'epoch': 0.57}
対照学習損失: 2.458629387547262e-05
対照学習損失: 0.00029170128982514143
{'loss': 0.0002, 'grad_norm': 0.0074292258359491825, 'learning_rate': 4.7761732851985564e-05, 'epoch': 0.57}
対照学習損失: 0.0010152121540158987
対照学習損失: 0.00015844032168388367
{'loss': 0.0006, 'grad_norm': 0.0419149212539196, 'learning_rate': 4.7743682310469314e-05, 'epoch': 0.57}
対照学習損失: 0.0006002522422932088
対照学習損失: 0.00021481704607140273
{'loss': 0.0004, 'grad_norm': 0.0410463847219944, 'learning_rate': 4.772563176895307e-05, 'epoch': 0.57}
対照学習損失: 9.361209231428802e-05
対照学習損失: 0.0002853120968211442
{'loss': 0.0002, 'grad_norm': 0.006552885286509991, 'learning_rate': 4.770758122743683e-05, 'epoch': 0.58}
対照学習損失: 0.0003372851060703397
対照学習損失: 0.0013271627249196172
{'loss': 0.0008, 'grad_norm': 0.09678429365158081, 'learning_rate': 4.768953068592058e-05, 'epoch': 0.58}
対照学習損失: 0.00013481958012562245
対照学習損失: 0.00014884345000609756
{'loss': 0.0001, 'grad_norm': 0.014877784065902233, 'learning_rate': 4.7671480144404336e-05, 'epoch': 0.58}
対照学習損失: 0.0002912311756517738
対照学習損失: 0.02588093839585781
{'loss': 0.0131, 'grad_norm': 1.1711468696594238, 'learning_rate': 4.765342960288809e-05, 'epoch': 0.58}
対照学習損失: 4.245599848218262e-05
対照学習損失: 0.00013481322093866765
{'loss': 0.0001, 'grad_norm': 0.0032908327411860228, 'learning_rate': 4.7635379061371844e-05, 'epoch': 0.58}
対照学習損失: 0.00047077200724743307
対照学習損失: 8.005568088265136e-05
{'loss': 0.0003, 'grad_norm': 0.011044668965041637, 'learning_rate': 4.7617328519855594e-05, 'epoch': 0.58}
対照学習損失: 7.716163236182183e-05
対照学習損失: 0.00012884018360637128
{'loss': 0.0001, 'grad_norm': 0.00368529767729342, 'learning_rate': 4.759927797833935e-05, 'epoch': 0.58}
対照学習損失: 0.015502489171922207
対照学習損失: 0.0004070108407177031
{'loss': 0.008, 'grad_norm': 0.5322087407112122, 'learning_rate': 4.758122743682311e-05, 'epoch': 0.58}
対照学習損失: 0.00020115634833928198
対照学習損失: 0.0002628133515827358
{'loss': 0.0002, 'grad_norm': 0.010954328812658787, 'learning_rate': 4.756317689530686e-05, 'epoch': 0.58}
対照学習損失: 8.62095330376178e-05
対照学習損失: 0.0007023227517493069
{'loss': 0.0004, 'grad_norm': 0.03823195770382881, 'learning_rate': 4.7545126353790616e-05, 'epoch': 0.58}
対照学習損失: 0.000367001979611814
対照学習損失: 5.879579111933708e-05
{'loss': 0.0002, 'grad_norm': 0.01103128306567669, 'learning_rate': 4.752707581227437e-05, 'epoch': 0.59}
対照学習損失: 0.00021100608864799142
対照学習損失: 0.004611392039805651
{'loss': 0.0024, 'grad_norm': 0.30092620849609375, 'learning_rate': 4.7509025270758124e-05, 'epoch': 0.59}
対照学習損失: 0.001629316946491599
対照学習損失: 0.0018254965543746948
{'loss': 0.0017, 'grad_norm': 0.13265670835971832, 'learning_rate': 4.7490974729241874e-05, 'epoch': 0.59}
対照学習損失: 0.00036680157063528895
対照学習損失: 0.006911212578415871
{'loss': 0.0036, 'grad_norm': 0.48449572920799255, 'learning_rate': 4.747292418772563e-05, 'epoch': 0.59}
対照学習損失: 0.000133944078697823
対照学習損失: 8.084713044809178e-05
{'loss': 0.0001, 'grad_norm': 0.007333925925195217, 'learning_rate': 4.745487364620939e-05, 'epoch': 0.59}
対照学習損失: 0.001173968194052577
対照学習損失: 0.0016824158374220133
{'loss': 0.0014, 'grad_norm': 0.08264558762311935, 'learning_rate': 4.7436823104693146e-05, 'epoch': 0.59}
対照学習損失: 0.00013028396642766893
対照学習損失: 0.0006152525311335921
{'loss': 0.0004, 'grad_norm': 0.03279362618923187, 'learning_rate': 4.7418772563176896e-05, 'epoch': 0.59}
対照学習損失: 0.003996462095528841
対照学習損失: 0.00018626834207680076
{'loss': 0.0021, 'grad_norm': 0.2971355617046356, 'learning_rate': 4.7400722021660654e-05, 'epoch': 0.59}
対照学習損失: 0.000625338579993695
対照学習損失: 0.00014915771316736937
{'loss': 0.0004, 'grad_norm': 0.037671614438295364, 'learning_rate': 4.738267148014441e-05, 'epoch': 0.59}
対照学習損失: 0.00017577590188011527
対照学習損失: 0.0042771571315824986
{'loss': 0.0022, 'grad_norm': 0.21890145540237427, 'learning_rate': 4.736462093862816e-05, 'epoch': 0.59}
対照学習損失: 0.0004424401558935642
対照学習損失: 6.759068492101505e-05
{'loss': 0.0003, 'grad_norm': 0.023517314344644547, 'learning_rate': 4.734657039711192e-05, 'epoch': 0.59}
対照学習損失: 6.121645856183022e-05
対照学習損失: 4.675739910453558e-05
{'loss': 0.0001, 'grad_norm': 0.0018133908743038774, 'learning_rate': 4.732851985559567e-05, 'epoch': 0.6}
対照学習損失: 0.0016049551777541637
対照学習損失: 0.00026328800595365465
{'loss': 0.0009, 'grad_norm': 0.06750620156526566, 'learning_rate': 4.7310469314079426e-05, 'epoch': 0.6}
対照学習損失: 9.447355114389211e-05
対照学習損失: 0.00013585395936388522
{'loss': 0.0001, 'grad_norm': 0.005180120933800936, 'learning_rate': 4.7292418772563177e-05, 'epoch': 0.6}
対照学習損失: 0.010089907795190811
対照学習損失: 0.0013733296655118465
{'loss': 0.0057, 'grad_norm': 0.30020976066589355, 'learning_rate': 4.7274368231046934e-05, 'epoch': 0.6}
対照学習損失: 0.0001031028077704832
対照学習損失: 0.0011346435640007257
{'loss': 0.0006, 'grad_norm': 0.07592812925577164, 'learning_rate': 4.725631768953069e-05, 'epoch': 0.6}
対照学習損失: 0.000159608680405654
対照学習損失: 4.425362931215204e-05
{'loss': 0.0001, 'grad_norm': 0.003794953227043152, 'learning_rate': 4.723826714801444e-05, 'epoch': 0.6}
対照学習損失: 2.305893212906085e-05
対照学習損失: 0.00026441775844432414
{'loss': 0.0001, 'grad_norm': 0.0104597769677639, 'learning_rate': 4.72202166064982e-05, 'epoch': 0.6}
対照学習損失: 3.777140227612108e-05
対照学習損失: 0.004134749993681908
{'loss': 0.0021, 'grad_norm': 0.19536949694156647, 'learning_rate': 4.720216606498195e-05, 'epoch': 0.6}
対照学習損失: 0.0006044998299330473
対照学習損失: 8.853197505231947e-05
{'loss': 0.0003, 'grad_norm': 0.020908866077661514, 'learning_rate': 4.7184115523465706e-05, 'epoch': 0.6}
対照学習損失: 0.00034284594585187733
対照学習損失: 5.6056553148664534e-05
{'loss': 0.0002, 'grad_norm': 0.012581629678606987, 'learning_rate': 4.716606498194946e-05, 'epoch': 0.6}
対照学習損失: 0.001490856404416263
対照学習損失: 0.05147695541381836
{'loss': 0.0265, 'grad_norm': 1.3639825582504272, 'learning_rate': 4.7148014440433214e-05, 'epoch': 0.6}
対照学習損失: 0.0006517262663692236
対照学習損失: 0.0001505496329627931
{'loss': 0.0004, 'grad_norm': 0.02775498479604721, 'learning_rate': 4.712996389891697e-05, 'epoch': 0.61}
対照学習損失: 0.004552018363028765
対照学習損失: 0.00013896284508518875
{'loss': 0.0023, 'grad_norm': 0.24577368795871735, 'learning_rate': 4.711191335740072e-05, 'epoch': 0.61}
対照学習損失: 0.0016034940490499139
対照学習損失: 6.577090243808925e-05
{'loss': 0.0008, 'grad_norm': 0.11714999377727509, 'learning_rate': 4.709386281588448e-05, 'epoch': 0.61}
対照学習損失: 0.0002537660184316337
対照学習損失: 4.527372948359698e-05
{'loss': 0.0001, 'grad_norm': 0.004915656987577677, 'learning_rate': 4.707581227436823e-05, 'epoch': 0.61}
対照学習損失: 8.588015043642372e-05
対照学習損失: 0.00117196561768651
{'loss': 0.0006, 'grad_norm': 0.039625078439712524, 'learning_rate': 4.705776173285199e-05, 'epoch': 0.61}
対照学習損失: 0.0026862919330596924
対照学習損失: 4.9560774641577154e-05
{'loss': 0.0014, 'grad_norm': 0.1070525199174881, 'learning_rate': 4.7039711191335744e-05, 'epoch': 0.61}
対照学習損失: 0.00014392629964277148
対照学習損失: 0.00020214964752085507
{'loss': 0.0002, 'grad_norm': 0.007418625522404909, 'learning_rate': 4.70216606498195e-05, 'epoch': 0.61}
対照学習損失: 0.00011785904644057155
対照学習損失: 0.0010868506506085396
{'loss': 0.0006, 'grad_norm': 0.03398717939853668, 'learning_rate': 4.700361010830325e-05, 'epoch': 0.61}
対照学習損失: 6.699487857986242e-05
対照学習損失: 4.485703175305389e-05
{'loss': 0.0001, 'grad_norm': 0.0019685993902385235, 'learning_rate': 4.698555956678701e-05, 'epoch': 0.61}
対照学習損失: 0.00012129535025451332
対照学習損失: 8.65246620378457e-05
{'loss': 0.0001, 'grad_norm': 0.0047548022121191025, 'learning_rate': 4.696750902527076e-05, 'epoch': 0.61}
対照学習損失: 0.00033072434598580003
対照学習損失: 0.00025776925031095743
{'loss': 0.0003, 'grad_norm': 0.014814225025475025, 'learning_rate': 4.6949458483754516e-05, 'epoch': 0.61}
対照学習損失: 0.00013304311141837388
対照学習損失: 6.505515921162441e-05
{'loss': 0.0001, 'grad_norm': 0.0031949568074196577, 'learning_rate': 4.693140794223827e-05, 'epoch': 0.62}
対照学習損失: 0.0003496979479677975
対照学習損失: 0.0014026830904185772
{'loss': 0.0009, 'grad_norm': 0.1902036815881729, 'learning_rate': 4.6913357400722024e-05, 'epoch': 0.62}
対照学習損失: 6.503962504211813e-05
対照学習損失: 0.0007242906140163541
{'loss': 0.0004, 'grad_norm': 0.02135801687836647, 'learning_rate': 4.689530685920578e-05, 'epoch': 0.62}
対照学習損失: 0.0009593066642992198
対照学習損失: 5.9634377976180986e-05
{'loss': 0.0005, 'grad_norm': 0.06685108691453934, 'learning_rate': 4.687725631768953e-05, 'epoch': 0.62}
対照学習損失: 9.804916044231504e-05
対照学習損失: 4.924485256196931e-05
{'loss': 0.0001, 'grad_norm': 0.005452726036310196, 'learning_rate': 4.685920577617329e-05, 'epoch': 0.62}
対照学習損失: 0.00020541928824968636
対照学習損失: 0.00018408344476483762
{'loss': 0.0002, 'grad_norm': 0.009133074432611465, 'learning_rate': 4.684115523465704e-05, 'epoch': 0.62}
対照学習損失: 0.002383650979027152
対照学習損失: 8.171613444574177e-05
{'loss': 0.0012, 'grad_norm': 0.165546253323555, 'learning_rate': 4.6823104693140796e-05, 'epoch': 0.62}
対照学習損失: 0.00015734686166979373
対照学習損失: 0.000228971301112324
{'loss': 0.0002, 'grad_norm': 0.008339619264006615, 'learning_rate': 4.680505415162455e-05, 'epoch': 0.62}
対照学習損失: 0.00021027441835030913
対照学習損失: 0.0008796083275228739
{'loss': 0.0005, 'grad_norm': 0.037061188369989395, 'learning_rate': 4.6787003610108304e-05, 'epoch': 0.62}
対照学習損失: 0.00041393248829990625
対照学習損失: 0.00015006512694526464
{'loss': 0.0003, 'grad_norm': 0.012604729272425175, 'learning_rate': 4.676895306859206e-05, 'epoch': 0.62}
対照学習損失: 0.00010213519999524578
対照学習損失: 0.00024270567519124597
{'loss': 0.0002, 'grad_norm': 0.010824745520949364, 'learning_rate': 4.675090252707581e-05, 'epoch': 0.62}
対照学習損失: 5.894946662010625e-05
対照学習損失: 0.0005449215532280505
{'loss': 0.0003, 'grad_norm': 0.01406023371964693, 'learning_rate': 4.673285198555957e-05, 'epoch': 0.63}
対照学習損失: 0.001295866328291595
対照学習損失: 0.004965681117027998
{'loss': 0.0031, 'grad_norm': 0.1726568043231964, 'learning_rate': 4.671480144404332e-05, 'epoch': 0.63}
対照学習損失: 6.60201403661631e-05
対照学習損失: 0.0002687973319552839
{'loss': 0.0002, 'grad_norm': 0.010247391648590565, 'learning_rate': 4.6696750902527076e-05, 'epoch': 0.63}
対照学習損失: 9.660237992648035e-05
対照学習損失: 0.0026305504143238068
{'loss': 0.0014, 'grad_norm': 0.07808767259120941, 'learning_rate': 4.6678700361010833e-05, 'epoch': 0.63}
対照学習損失: 0.0002856049104593694
対照学習損失: 0.00012950471136718988
{'loss': 0.0002, 'grad_norm': 0.00766959460452199, 'learning_rate': 4.666064981949459e-05, 'epoch': 0.63}
対照学習損失: 0.00047768588410690427
対照学習損失: 8.554807573091239e-05
{'loss': 0.0003, 'grad_norm': 0.011234212666749954, 'learning_rate': 4.664259927797834e-05, 'epoch': 0.63}
対照学習損失: 0.00011877482029376552
対照学習損失: 0.00011213847028557211
{'loss': 0.0001, 'grad_norm': 0.004865104798227549, 'learning_rate': 4.66245487364621e-05, 'epoch': 0.63}
対照学習損失: 3.2207426556851715e-05
対照学習損失: 3.444906178629026e-05
{'loss': 0.0, 'grad_norm': 0.0012288010912016034, 'learning_rate': 4.6606498194945856e-05, 'epoch': 0.63}
対照学習損失: 0.0005161644658073783
対照学習損失: 0.0009512343676760793
{'loss': 0.0007, 'grad_norm': 0.030014805495738983, 'learning_rate': 4.6588447653429606e-05, 'epoch': 0.63}
対照学習損失: 0.00013857975136488676
対照学習損失: 6.863555609015748e-05
{'loss': 0.0001, 'grad_norm': 0.0030410070903599262, 'learning_rate': 4.657039711191336e-05, 'epoch': 0.63}
対照学習損失: 0.0001372839615214616
対照学習損失: 0.0001459734921809286
{'loss': 0.0001, 'grad_norm': 0.00676796305924654, 'learning_rate': 4.6552346570397114e-05, 'epoch': 0.63}
対照学習損失: 0.00024208269314840436
対照学習損失: 0.00025831107632257044
{'loss': 0.0003, 'grad_norm': 0.009600512683391571, 'learning_rate': 4.653429602888087e-05, 'epoch': 0.64}
対照学習損失: 0.0004662005230784416
対照学習損失: 0.00128533027600497
{'loss': 0.0009, 'grad_norm': 0.04269402101635933, 'learning_rate': 4.651624548736462e-05, 'epoch': 0.64}
対照学習損失: 0.00019518588669598103
対照学習損失: 0.0005054980865679681
{'loss': 0.0004, 'grad_norm': 0.026437455788254738, 'learning_rate': 4.649819494584838e-05, 'epoch': 0.64}
対照学習損失: 0.00546463904902339
対照学習損失: 0.0001107311254600063
{'loss': 0.0028, 'grad_norm': 0.11668127775192261, 'learning_rate': 4.648014440433213e-05, 'epoch': 0.64}
対照学習損失: 0.0015927592758089304
対照学習損失: 0.00017423740064259619
{'loss': 0.0009, 'grad_norm': 0.08068295568227768, 'learning_rate': 4.6462093862815886e-05, 'epoch': 0.64}
対照学習損失: 0.0055795819498598576
対照学習損失: 0.012957058846950531
{'loss': 0.0093, 'grad_norm': 0.6384212374687195, 'learning_rate': 4.644404332129964e-05, 'epoch': 0.64}
対照学習損失: 3.713096884894185e-05
対照学習損失: 0.00044935193727724254
{'loss': 0.0002, 'grad_norm': 0.010010633617639542, 'learning_rate': 4.6425992779783394e-05, 'epoch': 0.64}
対照学習損失: 0.0005810181610286236
対照学習損失: 3.311667751404457e-05
{'loss': 0.0003, 'grad_norm': 0.013782106339931488, 'learning_rate': 4.640794223826715e-05, 'epoch': 0.64}
対照学習損失: 8.306890958920121e-05
対照学習損失: 0.001478336751461029
{'loss': 0.0008, 'grad_norm': 0.054652269929647446, 'learning_rate': 4.63898916967509e-05, 'epoch': 0.64}
対照学習損失: 5.201334715820849e-05
対照学習損失: 2.449654675729107e-05
{'loss': 0.0, 'grad_norm': 0.0011580395512282848, 'learning_rate': 4.637184115523466e-05, 'epoch': 0.64}
対照学習損失: 0.0357975959777832
対照学習損失: 2.3021641027298756e-05
{'loss': 0.0179, 'grad_norm': 0.8705249428749084, 'learning_rate': 4.635379061371841e-05, 'epoch': 0.64}
対照学習損失: 6.117039447417483e-05
対照学習損失: 7.18332885298878e-05
{'loss': 0.0001, 'grad_norm': 0.0039387047290802, 'learning_rate': 4.6335740072202166e-05, 'epoch': 0.65}
対照学習損失: 7.225346780614927e-05
対照学習損失: 0.00030646321829408407
{'loss': 0.0002, 'grad_norm': 0.006494630593806505, 'learning_rate': 4.631768953068592e-05, 'epoch': 0.65}
対照学習損失: 3.8011145079508424e-05
対照学習損失: 4.3649866711348295e-05
{'loss': 0.0, 'grad_norm': 0.001138897961936891, 'learning_rate': 4.6299638989169674e-05, 'epoch': 0.65}
対照学習損失: 0.00015761824033688754
対照学習損失: 7.694984378758818e-05
{'loss': 0.0001, 'grad_norm': 0.00497142318636179, 'learning_rate': 4.628158844765344e-05, 'epoch': 0.65}
対照学習損失: 2.41537818510551e-05
対照学習損失: 7.561304664704949e-05
{'loss': 0.0, 'grad_norm': 0.001306334976106882, 'learning_rate': 4.626353790613719e-05, 'epoch': 0.65}
対照学習損失: 0.00044714551768265665
対照学習損失: 6.974768621148542e-05
{'loss': 0.0003, 'grad_norm': 0.0131481708958745, 'learning_rate': 4.6245487364620945e-05, 'epoch': 0.65}
対照学習損失: 6.535795546369627e-05
対照学習損失: 0.00021845723676960915
{'loss': 0.0001, 'grad_norm': 0.009556244127452374, 'learning_rate': 4.6227436823104696e-05, 'epoch': 0.65}
対照学習損失: 0.00026091450126841664
対照学習損失: 0.001828489825129509
{'loss': 0.001, 'grad_norm': 0.04330574721097946, 'learning_rate': 4.620938628158845e-05, 'epoch': 0.65}
対照学習損失: 0.00024105168995447457
対照学習損失: 8.158976561389863e-05
{'loss': 0.0002, 'grad_norm': 0.008024601265788078, 'learning_rate': 4.6191335740072203e-05, 'epoch': 0.65}
対照学習損失: 0.0004348434740677476
対照学習損失: 0.0005872503970749676
{'loss': 0.0005, 'grad_norm': 0.03428391367197037, 'learning_rate': 4.617328519855596e-05, 'epoch': 0.65}
対照学習損失: 7.564292172901332e-05
対照学習損失: 0.00024883486912585795
{'loss': 0.0002, 'grad_norm': 0.006200073286890984, 'learning_rate': 4.615523465703971e-05, 'epoch': 0.65}
対照学習損失: 0.00016071568825282156
対照学習損失: 0.000916265940759331
{'loss': 0.0005, 'grad_norm': 0.02606157958507538, 'learning_rate': 4.613718411552347e-05, 'epoch': 0.66}
対照学習損失: 0.0007152918842621148
対照学習損失: 5.034594505559653e-05
{'loss': 0.0004, 'grad_norm': 0.08452148735523224, 'learning_rate': 4.6119133574007226e-05, 'epoch': 0.66}
対照学習損失: 0.00029804330551996827
対照学習損失: 0.00011688156519085169
{'loss': 0.0002, 'grad_norm': 0.02203018218278885, 'learning_rate': 4.6101083032490976e-05, 'epoch': 0.66}
対照学習損失: 0.0009284469415433705
対照学習損失: 0.002173991408199072
{'loss': 0.0016, 'grad_norm': 0.08878806233406067, 'learning_rate': 4.608303249097473e-05, 'epoch': 0.66}
対照学習損失: 4.095416079508141e-05
対照学習損失: 0.06315924972295761
{'loss': 0.0316, 'grad_norm': 1.6098519563674927, 'learning_rate': 4.6064981949458484e-05, 'epoch': 0.66}
対照学習損失: 0.00023010959557723254
対照学習損失: 4.2801857489394024e-05
{'loss': 0.0001, 'grad_norm': 0.016601882874965668, 'learning_rate': 4.604693140794224e-05, 'epoch': 0.66}
対照学習損失: 0.0007008190150372684
対照学習損失: 0.000219159948755987
{'loss': 0.0005, 'grad_norm': 0.017276939004659653, 'learning_rate': 4.602888086642599e-05, 'epoch': 0.66}
対照学習損失: 0.0001565504353493452
対照学習損失: 0.0012441520811989903
{'loss': 0.0007, 'grad_norm': 0.05004194378852844, 'learning_rate': 4.601083032490975e-05, 'epoch': 0.66}
対照学習損失: 0.013748690485954285
対照学習損失: 0.0001061300135916099
{'loss': 0.0069, 'grad_norm': 0.4761856198310852, 'learning_rate': 4.5992779783393506e-05, 'epoch': 0.66}
対照学習損失: 0.00019729562336578965
対照学習損失: 0.030070926994085312
{'loss': 0.0151, 'grad_norm': 0.6637765169143677, 'learning_rate': 4.5974729241877256e-05, 'epoch': 0.66}
対照学習損失: 0.0028438838198781013
対照学習損失: 3.6140059819445014e-05
{'loss': 0.0014, 'grad_norm': 0.09294742345809937, 'learning_rate': 4.595667870036101e-05, 'epoch': 0.66}
対照学習損失: 0.00013119762297719717
対照学習損失: 4.565159906633198e-05
{'loss': 0.0001, 'grad_norm': 0.005019185598939657, 'learning_rate': 4.5938628158844764e-05, 'epoch': 0.67}
対照学習損失: 0.0003512085822876543
対照学習損失: 3.450263466220349e-05
{'loss': 0.0002, 'grad_norm': 0.011933756060898304, 'learning_rate': 4.592057761732852e-05, 'epoch': 0.67}
対照学習損失: 0.00030933061498217285
対照学習損失: 2.0116147425142117e-05
{'loss': 0.0002, 'grad_norm': 0.0298899095505476, 'learning_rate': 4.590252707581227e-05, 'epoch': 0.67}
対照学習損失: 0.00042442031553946435
対照学習損失: 0.0012362379347905517
{'loss': 0.0008, 'grad_norm': 0.044273048639297485, 'learning_rate': 4.5884476534296035e-05, 'epoch': 0.67}
対照学習損失: 0.00013323655002750456
対照学習損失: 0.00015475775580853224
{'loss': 0.0001, 'grad_norm': 0.008594760671257973, 'learning_rate': 4.5866425992779786e-05, 'epoch': 0.67}
対照学習損失: 0.0003354676009621471
対照学習損失: 0.00011166240437887609
{'loss': 0.0002, 'grad_norm': 0.024758029729127884, 'learning_rate': 4.584837545126354e-05, 'epoch': 0.67}
対照学習損失: 8.51338409120217e-05
対照学習損失: 4.614373392541893e-05
{'loss': 0.0001, 'grad_norm': 0.0021564015187323093, 'learning_rate': 4.583032490974729e-05, 'epoch': 0.67}
対照学習損失: 0.000325267348671332
対照学習損失: 0.00024799982202239335
{'loss': 0.0003, 'grad_norm': 0.022956762462854385, 'learning_rate': 4.581227436823105e-05, 'epoch': 0.67}
対照学習損失: 0.0038764802739024162
対照学習損失: 0.0004716473340522498
{'loss': 0.0022, 'grad_norm': 0.2809329926967621, 'learning_rate': 4.579422382671481e-05, 'epoch': 0.67}
対照学習損失: 0.002336375880986452
対照学習損失: 0.0012673325836658478
{'loss': 0.0018, 'grad_norm': 0.23326532542705536, 'learning_rate': 4.577617328519856e-05, 'epoch': 0.67}
対照学習損失: 5.135292303748429e-05
対照学習損失: 0.0005082311108708382
{'loss': 0.0003, 'grad_norm': 0.02267988584935665, 'learning_rate': 4.5758122743682315e-05, 'epoch': 0.67}
対照学習損失: 7.4245166615583e-05
対照学習損失: 0.00015896197874099016
{'loss': 0.0001, 'grad_norm': 0.007972500286996365, 'learning_rate': 4.5740072202166066e-05, 'epoch': 0.68}
対照学習損失: 0.00015512318350374699
対照学習損失: 0.0002852132311090827
{'loss': 0.0002, 'grad_norm': 0.01662113517522812, 'learning_rate': 4.572202166064982e-05, 'epoch': 0.68}
対照学習損失: 5.845205669174902e-05
対照学習損失: 0.00014839787036180496
{'loss': 0.0001, 'grad_norm': 0.003908259328454733, 'learning_rate': 4.5703971119133573e-05, 'epoch': 0.68}
対照学習損失: 0.0002035442303167656
対照学習損失: 0.001984385307878256
{'loss': 0.0011, 'grad_norm': 0.05145548656582832, 'learning_rate': 4.568592057761733e-05, 'epoch': 0.68}
対照学習損失: 6.217662303242832e-05
対照学習損失: 5.892974877497181e-05
{'loss': 0.0001, 'grad_norm': 0.001969363773241639, 'learning_rate': 4.566787003610109e-05, 'epoch': 0.68}
対照学習損失: 0.015012843534350395
対照学習損失: 3.932137042284012e-05
{'loss': 0.0075, 'grad_norm': 1.4486193656921387, 'learning_rate': 4.564981949458484e-05, 'epoch': 0.68}
対照学習損失: 0.0003868131898343563
対照学習損失: 0.00015628190885763615
{'loss': 0.0003, 'grad_norm': 0.010829153470695019, 'learning_rate': 4.5631768953068596e-05, 'epoch': 0.68}
対照学習損失: 1.835783405113034e-05
対照学習損失: 0.0005163575406186283
{'loss': 0.0003, 'grad_norm': 0.01865767501294613, 'learning_rate': 4.5613718411552346e-05, 'epoch': 0.68}
対照学習損失: 0.00045243470231071115
対照学習損失: 0.00012051653175149113
{'loss': 0.0003, 'grad_norm': 0.01743459887802601, 'learning_rate': 4.55956678700361e-05, 'epoch': 0.68}
対照学習損失: 0.00013837197911925614
対照学習損失: 0.00015919990255497396
{'loss': 0.0001, 'grad_norm': 0.006287793163210154, 'learning_rate': 4.5577617328519854e-05, 'epoch': 0.68}
対照学習損失: 0.0007350969826802611
対照学習損失: 0.00012225197860971093
{'loss': 0.0004, 'grad_norm': 0.05951764062047005, 'learning_rate': 4.555956678700361e-05, 'epoch': 0.69}
対照学習損失: 0.0003273012989666313
対照学習損失: 0.0006096626166254282
{'loss': 0.0005, 'grad_norm': 0.0387488454580307, 'learning_rate': 4.554151624548736e-05, 'epoch': 0.69}
対照学習損失: 0.00016081043577287346
対照学習損失: 7.729010394541547e-05
{'loss': 0.0001, 'grad_norm': 0.006609813310205936, 'learning_rate': 4.552346570397112e-05, 'epoch': 0.69}
対照学習損失: 0.00014869487495161593
対照学習損失: 0.0005159587599337101
{'loss': 0.0003, 'grad_norm': 0.021636201068758965, 'learning_rate': 4.5505415162454876e-05, 'epoch': 0.69}
対照学習損失: 0.0030169235542416573
対照学習損失: 0.0010010360274463892
{'loss': 0.002, 'grad_norm': 0.17814628779888153, 'learning_rate': 4.548736462093863e-05, 'epoch': 0.69}
対照学習損失: 0.00025083066429942846
対照学習損失: 9.506730566499755e-05
{'loss': 0.0002, 'grad_norm': 0.02354348823428154, 'learning_rate': 4.546931407942239e-05, 'epoch': 0.69}
対照学習損失: 0.00010230463522020727
対照学習損失: 0.0001309901854256168
{'loss': 0.0001, 'grad_norm': 0.006349296774715185, 'learning_rate': 4.545126353790614e-05, 'epoch': 0.69}
対照学習損失: 0.0001504861720604822
対照学習損失: 0.00024428474716842175
{'loss': 0.0002, 'grad_norm': 0.014707258902490139, 'learning_rate': 4.54332129963899e-05, 'epoch': 0.69}
対照学習損失: 0.000536122708581388
対照学習損失: 8.196554699679837e-05
{'loss': 0.0003, 'grad_norm': 0.03795359656214714, 'learning_rate': 4.541516245487365e-05, 'epoch': 0.69}
対照学習損失: 0.00036307680420577526
対照学習損失: 0.00023982430866453797
{'loss': 0.0003, 'grad_norm': 0.028700338676571846, 'learning_rate': 4.5397111913357405e-05, 'epoch': 0.69}
対照学習損失: 0.0004027503600809723
対照学習損失: 0.00024307460989803076
{'loss': 0.0003, 'grad_norm': 0.0339256152510643, 'learning_rate': 4.5379061371841156e-05, 'epoch': 0.69}
対照学習損失: 0.0066125113517045975
対照学習損失: 3.882302553392947e-05
{'loss': 0.0033, 'grad_norm': 0.673328161239624, 'learning_rate': 4.536101083032491e-05, 'epoch': 0.7}
対照学習損失: 0.0004198158858343959
対照学習損失: 0.00018763000844046474
{'loss': 0.0003, 'grad_norm': 0.016135655343532562, 'learning_rate': 4.534296028880867e-05, 'epoch': 0.7}
対照学習損失: 0.0010756548726931214
対照学習損失: 0.014639753848314285
{'loss': 0.0079, 'grad_norm': 1.2694127559661865, 'learning_rate': 4.532490974729242e-05, 'epoch': 0.7}
対照学習損失: 0.0020188242197036743
対照学習損失: 0.0001250053901458159
{'loss': 0.0011, 'grad_norm': 0.07190913707017899, 'learning_rate': 4.530685920577618e-05, 'epoch': 0.7}
対照学習損失: 6.490237137768418e-05
対照学習損失: 3.891218875651248e-05
{'loss': 0.0001, 'grad_norm': 0.0017066828440874815, 'learning_rate': 4.528880866425993e-05, 'epoch': 0.7}
対照学習損失: 0.00015031418297439814
対照学習損失: 3.2675827242201194e-05
{'loss': 0.0001, 'grad_norm': 0.005506059154868126, 'learning_rate': 4.5270758122743685e-05, 'epoch': 0.7}
対照学習損失: 4.658205580199137e-05
対照学習損失: 0.00023203912132885307
{'loss': 0.0001, 'grad_norm': 0.006809210404753685, 'learning_rate': 4.5252707581227436e-05, 'epoch': 0.7}
対照学習損失: 0.0002968987973872572
対照学習損失: 0.0018124842317774892
{'loss': 0.0011, 'grad_norm': 0.09694128483533859, 'learning_rate': 4.523465703971119e-05, 'epoch': 0.7}
対照学習損失: 0.0017157836118713021
対照学習損失: 0.0003854790120385587
{'loss': 0.0011, 'grad_norm': 0.0966106727719307, 'learning_rate': 4.5216606498194943e-05, 'epoch': 0.7}
対照学習損失: 0.00018292195454705507
対照学習損失: 0.0023128523025661707
{'loss': 0.0012, 'grad_norm': 0.12184256315231323, 'learning_rate': 4.51985559566787e-05, 'epoch': 0.7}
対照学習損失: 0.00040257914224639535
対照学習損失: 0.0026449249126017094
{'loss': 0.0015, 'grad_norm': 0.17831669747829437, 'learning_rate': 4.518050541516246e-05, 'epoch': 0.7}
対照学習損失: 0.0022414217237383127
対照学習損失: 0.0007954344619065523
{'loss': 0.0015, 'grad_norm': 0.11275667697191238, 'learning_rate': 4.516245487364621e-05, 'epoch': 0.71}
対照学習損失: 0.0004330119991209358
対照学習損失: 0.0015120238531380892
{'loss': 0.001, 'grad_norm': 0.06256416440010071, 'learning_rate': 4.5144404332129966e-05, 'epoch': 0.71}
対照学習損失: 0.0005359571077860892
対照学習損失: 0.0008086109301075339
{'loss': 0.0007, 'grad_norm': 0.047408003360033035, 'learning_rate': 4.5126353790613716e-05, 'epoch': 0.71}
対照学習損失: 0.04075600206851959
対照学習損失: 0.001053060870617628
{'loss': 0.0209, 'grad_norm': 1.1654797792434692, 'learning_rate': 4.510830324909748e-05, 'epoch': 0.71}
対照学習損失: 0.001686416449956596
対照学習損失: 0.0001493012678110972
{'loss': 0.0009, 'grad_norm': 0.09281443059444427, 'learning_rate': 4.509025270758123e-05, 'epoch': 0.71}
対照学習損失: 0.0013486855896189809
対照学習損失: 0.0003001222212333232
{'loss': 0.0008, 'grad_norm': 0.07915513217449188, 'learning_rate': 4.507220216606499e-05, 'epoch': 0.71}
対照学習損失: 0.0004705045139417052
対照学習損失: 5.7260680478066206e-05
{'loss': 0.0003, 'grad_norm': 0.02028292417526245, 'learning_rate': 4.505415162454874e-05, 'epoch': 0.71}
対照学習損失: 0.0034172332379966974
対照学習損失: 0.0011461495887488127
{'loss': 0.0023, 'grad_norm': 0.17688192427158356, 'learning_rate': 4.5036101083032495e-05, 'epoch': 0.71}
対照学習損失: 4.83887197333388e-05
対照学習損失: 0.0020738779567182064
{'loss': 0.0011, 'grad_norm': 0.09049557149410248, 'learning_rate': 4.5018050541516246e-05, 'epoch': 0.71}
対照学習損失: 0.0012849702034145594
対照学習損失: 0.00015202596841845661
{'loss': 0.0007, 'grad_norm': 0.07078798115253448, 'learning_rate': 4.5e-05, 'epoch': 0.71}
対照学習損失: 0.00037157151382416487
対照学習損失: 0.019191274419426918
{'loss': 0.0098, 'grad_norm': 1.1427016258239746, 'learning_rate': 4.498194945848376e-05, 'epoch': 0.71}
対照学習損失: 0.00013849101378582418
対照学習損失: 9.096398571273312e-05
{'loss': 0.0001, 'grad_norm': 0.006469834130257368, 'learning_rate': 4.496389891696751e-05, 'epoch': 0.72}
対照学習損失: 0.0005234655691310763
対照学習損失: 9.959814633475617e-05
{'loss': 0.0003, 'grad_norm': 0.01691359095275402, 'learning_rate': 4.494584837545127e-05, 'epoch': 0.72}
対照学習損失: 0.00011009783338522539
対照学習損失: 0.00028919579926878214
{'loss': 0.0002, 'grad_norm': 0.012044806033372879, 'learning_rate': 4.492779783393502e-05, 'epoch': 0.72}
対照学習損失: 1.9929582776967436e-05
対照学習損失: 0.00035306462086737156
{'loss': 0.0002, 'grad_norm': 0.016756964847445488, 'learning_rate': 4.4909747292418775e-05, 'epoch': 0.72}
対照学習損失: 3.2878182537388057e-05
対照学習損失: 0.0029550546314567327
{'loss': 0.0015, 'grad_norm': 0.0945897176861763, 'learning_rate': 4.4891696750902526e-05, 'epoch': 0.72}
対照学習損失: 4.873926081927493e-05
対照学習損失: 0.00019617780344560742
{'loss': 0.0001, 'grad_norm': 0.004815238993614912, 'learning_rate': 4.487364620938628e-05, 'epoch': 0.72}
対照学習損失: 5.654318374581635e-05
対照学習損失: 0.0008502660784870386
{'loss': 0.0005, 'grad_norm': 0.037859637290239334, 'learning_rate': 4.485559566787004e-05, 'epoch': 0.72}
対照学習損失: 2.7678041078615934e-05
対照学習損失: 3.327969534439035e-05
{'loss': 0.0, 'grad_norm': 0.0008329962147399783, 'learning_rate': 4.483754512635379e-05, 'epoch': 0.72}
対照学習損失: 0.00013809882511850446
対照学習損失: 8.063521090662107e-05
{'loss': 0.0001, 'grad_norm': 0.004987533204257488, 'learning_rate': 4.481949458483755e-05, 'epoch': 0.72}
対照学習損失: 4.2295152525184676e-05
対照学習損失: 2.5279114197473973e-05
{'loss': 0.0, 'grad_norm': 0.0008719885372556746, 'learning_rate': 4.48014440433213e-05, 'epoch': 0.72}
対照学習損失: 8.174210961442441e-05
対照学習損失: 0.0017405481776222587
{'loss': 0.0009, 'grad_norm': 0.0481707863509655, 'learning_rate': 4.4783393501805055e-05, 'epoch': 0.72}
対照学習損失: 0.0038634114898741245
対照学習損失: 0.00014760543126612902
{'loss': 0.002, 'grad_norm': 0.19379733502864838, 'learning_rate': 4.4765342960288806e-05, 'epoch': 0.73}
対照学習損失: 0.00028462568297982216
対照学習損失: 3.157389801344834e-05
{'loss': 0.0002, 'grad_norm': 0.0074313245713710785, 'learning_rate': 4.474729241877256e-05, 'epoch': 0.73}
対照学習損失: 0.0023752448614686728
対照学習損失: 0.19775785505771637
{'loss': 0.1001, 'grad_norm': 2.347104549407959, 'learning_rate': 4.472924187725632e-05, 'epoch': 0.73}
対照学習損失: 0.004398969933390617
対照学習損失: 0.002118172124028206
{'loss': 0.0033, 'grad_norm': 0.10266145318746567, 'learning_rate': 4.471119133574008e-05, 'epoch': 0.73}
対照学習損失: 0.0022397497668862343
対照学習損失: 0.001139169093221426
{'loss': 0.0017, 'grad_norm': 0.09390803426504135, 'learning_rate': 4.469314079422383e-05, 'epoch': 0.73}
対照学習損失: 0.0002518457768019289
対照学習損失: 0.0001629614707781002
{'loss': 0.0002, 'grad_norm': 0.005164925940334797, 'learning_rate': 4.4675090252707585e-05, 'epoch': 0.73}
対照学習損失: 5.659049202222377e-05
対照学習損失: 7.208398164948449e-05
{'loss': 0.0001, 'grad_norm': 0.0017721140757203102, 'learning_rate': 4.465703971119134e-05, 'epoch': 0.73}
対照学習損失: 8.606546907685697e-05
対照学習損失: 5.145441900822334e-05
{'loss': 0.0001, 'grad_norm': 0.0024134500417858362, 'learning_rate': 4.463898916967509e-05, 'epoch': 0.73}
対照学習損失: 9.09813170437701e-05
対照学習損失: 4.0558352338848636e-05
{'loss': 0.0001, 'grad_norm': 0.0023935590870678425, 'learning_rate': 4.462093862815885e-05, 'epoch': 0.73}
対照学習損失: 0.0002724163350649178
対照学習損失: 0.010197246447205544
{'loss': 0.0052, 'grad_norm': 0.5141613483428955, 'learning_rate': 4.46028880866426e-05, 'epoch': 0.73}
対照学習損失: 0.00011925157741643488
対照学習損失: 0.00010556759661994874
{'loss': 0.0001, 'grad_norm': 0.004184863530099392, 'learning_rate': 4.458483754512636e-05, 'epoch': 0.73}
対照学習損失: 0.00019461200281511992
対照学習損失: 0.00010699746781028807
{'loss': 0.0002, 'grad_norm': 0.005554375238716602, 'learning_rate': 4.456678700361011e-05, 'epoch': 0.74}
対照学習損失: 0.00036358763463795185
対照学習損失: 0.0010648190509527922
{'loss': 0.0007, 'grad_norm': 0.058112580329179764, 'learning_rate': 4.4548736462093865e-05, 'epoch': 0.74}
対照学習損失: 0.01027658861130476
対照学習損失: 0.00030138029251247644
{'loss': 0.0053, 'grad_norm': 0.5333478450775146, 'learning_rate': 4.453068592057762e-05, 'epoch': 0.74}
対照学習損失: 0.0002944740990642458
対照学習損失: 0.00019180908566340804
{'loss': 0.0002, 'grad_norm': 0.011754153296351433, 'learning_rate': 4.451263537906137e-05, 'epoch': 0.74}
対照学習損失: 0.0001797126606106758
対照学習損失: 0.00011222958710277453
{'loss': 0.0001, 'grad_norm': 0.008341420441865921, 'learning_rate': 4.449458483754513e-05, 'epoch': 0.74}
対照学習損失: 0.010894007049500942
対照学習損失: 0.00015330698806792498
{'loss': 0.0055, 'grad_norm': 0.6301061511039734, 'learning_rate': 4.447653429602888e-05, 'epoch': 0.74}
対照学習損失: 6.527371442643926e-05
対照学習損失: 0.0002077093959087506
{'loss': 0.0001, 'grad_norm': 0.00663633132353425, 'learning_rate': 4.445848375451264e-05, 'epoch': 0.74}
対照学習損失: 0.0001827840314945206
対照学習損失: 7.105364056769758e-05
{'loss': 0.0001, 'grad_norm': 0.013587864115834236, 'learning_rate': 4.444043321299639e-05, 'epoch': 0.74}
対照学習損失: 0.00048609444638714194
対照学習損失: 0.00022483739303424954
{'loss': 0.0004, 'grad_norm': 0.02051585167646408, 'learning_rate': 4.4422382671480145e-05, 'epoch': 0.74}
対照学習損失: 7.755021215416491e-05
対照学習損失: 0.00031482611666433513
{'loss': 0.0002, 'grad_norm': 0.012544427998363972, 'learning_rate': 4.44043321299639e-05, 'epoch': 0.74}
対照学習損失: 8.628189243609086e-05
対照学習損失: 0.00014314665168058127
{'loss': 0.0001, 'grad_norm': 0.005697219632565975, 'learning_rate': 4.438628158844765e-05, 'epoch': 0.74}
対照学習損失: 0.0002855837228707969
対照学習損失: 7.810835086274892e-05
{'loss': 0.0002, 'grad_norm': 0.00727538438513875, 'learning_rate': 4.436823104693141e-05, 'epoch': 0.75}
対照学習損失: 4.167575389146805e-05
対照学習損失: 0.00038020015927031636
{'loss': 0.0002, 'grad_norm': 0.010936345905065536, 'learning_rate': 4.435018050541516e-05, 'epoch': 0.75}
対照学習損失: 0.00020871750894002616
対照学習損失: 0.0019888279493898153
{'loss': 0.0011, 'grad_norm': 0.08660470694303513, 'learning_rate': 4.4332129963898925e-05, 'epoch': 0.75}
対照学習損失: 7.419065514113754e-05
対照学習損失: 0.000129163934616372
{'loss': 0.0001, 'grad_norm': 0.00394061952829361, 'learning_rate': 4.4314079422382675e-05, 'epoch': 0.75}
対照学習損失: 3.0321956728585064e-05
対照学習損失: 0.0002798557106871158
{'loss': 0.0002, 'grad_norm': 0.009202420711517334, 'learning_rate': 4.429602888086643e-05, 'epoch': 0.75}
対照学習損失: 0.00021098583238199353
対照学習損失: 0.0003405849274713546
{'loss': 0.0003, 'grad_norm': 0.019597453996539116, 'learning_rate': 4.427797833935018e-05, 'epoch': 0.75}
対照学習損失: 0.00013215417857281864
対照学習損失: 0.00014033533807378262
{'loss': 0.0001, 'grad_norm': 0.006098622921854258, 'learning_rate': 4.425992779783394e-05, 'epoch': 0.75}
対照学習損失: 3.872630622936413e-05
対照学習損失: 0.00018185499357059598
{'loss': 0.0001, 'grad_norm': 0.004737063776701689, 'learning_rate': 4.424187725631769e-05, 'epoch': 0.75}
対照学習損失: 8.039994281716645e-05
対照学習損失: 0.00019006345246452838
{'loss': 0.0001, 'grad_norm': 0.012743676081299782, 'learning_rate': 4.422382671480145e-05, 'epoch': 0.75}
対照学習損失: 0.0002629878290463239
対照学習損失: 0.00030757900094613433
{'loss': 0.0003, 'grad_norm': 0.011160844005644321, 'learning_rate': 4.4205776173285205e-05, 'epoch': 0.75}
対照学習損失: 0.00011996774264844134
対照学習損失: 0.00010457442840561271
{'loss': 0.0001, 'grad_norm': 0.004507292062044144, 'learning_rate': 4.4187725631768955e-05, 'epoch': 0.75}
対照学習損失: 0.0005060672992840409
対照学習損失: 0.00017366577230859548
{'loss': 0.0003, 'grad_norm': 0.014779685996472836, 'learning_rate': 4.416967509025271e-05, 'epoch': 0.76}
対照学習損失: 4.639173130271956e-05
対照学習損失: 0.005276991054415703
{'loss': 0.0027, 'grad_norm': 0.17703095078468323, 'learning_rate': 4.415162454873646e-05, 'epoch': 0.76}
対照学習損失: 0.029204443097114563
対照学習損失: 8.970810449682176e-05
{'loss': 0.0146, 'grad_norm': 1.253688097000122, 'learning_rate': 4.413357400722022e-05, 'epoch': 0.76}
対照学習損失: 0.000201051909243688
対照学習損失: 0.00014186406042426825
{'loss': 0.0002, 'grad_norm': 0.008956163190305233, 'learning_rate': 4.411552346570397e-05, 'epoch': 0.76}
対照学習損失: 8.074111246969551e-05
対照学習損失: 0.00010631792974891141
{'loss': 0.0001, 'grad_norm': 0.004456030670553446, 'learning_rate': 4.409747292418773e-05, 'epoch': 0.76}
対照学習損失: 5.4154021199792624e-05
対照学習損失: 0.0036442256532609463
{'loss': 0.0018, 'grad_norm': 0.07937562465667725, 'learning_rate': 4.4079422382671485e-05, 'epoch': 0.76}
対照学習損失: 4.685097519541159e-05
対照学習損失: 0.00023334924480877817
{'loss': 0.0001, 'grad_norm': 0.009438368491828442, 'learning_rate': 4.4061371841155235e-05, 'epoch': 0.76}
対照学習損失: 0.0002858536026906222
対照学習損失: 0.0001373468548990786
{'loss': 0.0002, 'grad_norm': 0.0090324142947793, 'learning_rate': 4.404332129963899e-05, 'epoch': 0.76}
対照学習損失: 8.020877430681139e-05
対照学習損失: 0.00047317068674601614
{'loss': 0.0003, 'grad_norm': 0.013350690715014935, 'learning_rate': 4.402527075812274e-05, 'epoch': 0.76}
対照学習損失: 0.00036482527502812445
対照学習損失: 0.0007592582260258496
{'loss': 0.0006, 'grad_norm': 0.04631802439689636, 'learning_rate': 4.40072202166065e-05, 'epoch': 0.76}
対照学習損失: 0.00017776148160919547
対照学習損失: 0.00011630554217845201
{'loss': 0.0001, 'grad_norm': 0.004824274219572544, 'learning_rate': 4.398916967509025e-05, 'epoch': 0.76}
対照学習損失: 0.000292811484541744
対照学習損失: 0.0015027644112706184
{'loss': 0.0009, 'grad_norm': 0.05494590103626251, 'learning_rate': 4.397111913357401e-05, 'epoch': 0.77}
対照学習損失: 4.665300730266608e-05
対照学習損失: 0.00034034380223602057
{'loss': 0.0002, 'grad_norm': 0.010927275754511356, 'learning_rate': 4.395306859205776e-05, 'epoch': 0.77}
対照学習損失: 0.0002833757025655359
対照学習損失: 0.0006487448699772358
{'loss': 0.0005, 'grad_norm': 0.04097916558384895, 'learning_rate': 4.393501805054152e-05, 'epoch': 0.77}
対照学習損失: 0.0008034541388042271
対照学習損失: 0.00024117546854540706
{'loss': 0.0005, 'grad_norm': 0.04522295668721199, 'learning_rate': 4.391696750902527e-05, 'epoch': 0.77}
対照学習損失: 1.7806531104724854e-05
対照学習損失: 0.004328027367591858
{'loss': 0.0022, 'grad_norm': 0.2636266052722931, 'learning_rate': 4.389891696750903e-05, 'epoch': 0.77}
対照学習損失: 0.00023088876332622021
対照学習損失: 6.941487663425505e-05
{'loss': 0.0002, 'grad_norm': 0.013484462164342403, 'learning_rate': 4.388086642599279e-05, 'epoch': 0.77}
対照学習損失: 0.0001098398061003536
対照学習損失: 7.782262400723994e-05
{'loss': 0.0001, 'grad_norm': 0.006371617782860994, 'learning_rate': 4.386281588447654e-05, 'epoch': 0.77}
対照学習損失: 0.00021653983276337385
対照学習損失: 0.005258455406874418
{'loss': 0.0027, 'grad_norm': 0.33889901638031006, 'learning_rate': 4.3844765342960295e-05, 'epoch': 0.77}
対照学習損失: 0.0002639721496962011
対照学習損失: 2.2351065126713365e-05
{'loss': 0.0001, 'grad_norm': 0.006398029625415802, 'learning_rate': 4.3826714801444045e-05, 'epoch': 0.77}
対照学習損失: 0.0002454782952554524
対照学習損失: 0.0003035783884115517
{'loss': 0.0003, 'grad_norm': 0.01431711670011282, 'learning_rate': 4.38086642599278e-05, 'epoch': 0.77}
対照学習損失: 0.00010399226448498666
対照学習損失: 5.0421833293512464e-05
{'loss': 0.0001, 'grad_norm': 0.006321318913251162, 'learning_rate': 4.379061371841155e-05, 'epoch': 0.77}
対照学習損失: 0.00010047847899841145
対照学習損失: 0.0004065456159878522
{'loss': 0.0003, 'grad_norm': 0.02142503671348095, 'learning_rate': 4.377256317689531e-05, 'epoch': 0.78}
対照学習損失: 0.000424717873102054
対照学習損失: 0.00027026538737118244
{'loss': 0.0003, 'grad_norm': 0.017726480960845947, 'learning_rate': 4.375451263537906e-05, 'epoch': 0.78}
対照学習損失: 4.7535817429888994e-05
対照学習損失: 0.0003329832397866994
{'loss': 0.0002, 'grad_norm': 0.022300662472844124, 'learning_rate': 4.373646209386282e-05, 'epoch': 0.78}
対照学習損失: 0.0005124376039020717
対照学習損失: 0.03954918682575226
{'loss': 0.02, 'grad_norm': 0.8945276737213135, 'learning_rate': 4.3718411552346575e-05, 'epoch': 0.78}
対照学習損失: 0.0002666577638592571
対照学習損失: 5.122599395690486e-05
{'loss': 0.0002, 'grad_norm': 0.008537666872143745, 'learning_rate': 4.3700361010830325e-05, 'epoch': 0.78}
対照学習損失: 0.0001962442765943706
対照学習損失: 0.0004037468461319804
{'loss': 0.0003, 'grad_norm': 0.012862810865044594, 'learning_rate': 4.368231046931408e-05, 'epoch': 0.78}
対照学習損失: 1.7381657016812824e-05
対照学習損失: 0.00011626601190073416
{'loss': 0.0001, 'grad_norm': 0.0024887348990887403, 'learning_rate': 4.366425992779783e-05, 'epoch': 0.78}
対照学習損失: 0.0003002877929247916
対照学習損失: 4.535651532933116e-05
{'loss': 0.0002, 'grad_norm': 0.018393293023109436, 'learning_rate': 4.364620938628159e-05, 'epoch': 0.78}
対照学習損失: 8.904888818506151e-05
対照学習損失: 8.40078282635659e-05
{'loss': 0.0001, 'grad_norm': 0.004184380639344454, 'learning_rate': 4.362815884476534e-05, 'epoch': 0.78}
対照学習損失: 0.00031044668867252767
対照学習損失: 0.0002686381048988551
{'loss': 0.0003, 'grad_norm': 0.010907414369285107, 'learning_rate': 4.36101083032491e-05, 'epoch': 0.78}
対照学習損失: 0.00014651668607257307
対照学習損失: 0.00017136459064204246
{'loss': 0.0002, 'grad_norm': 0.004888642113655806, 'learning_rate': 4.3592057761732855e-05, 'epoch': 0.78}
対照学習損失: 0.00042930495692417026
対照学習損失: 0.0008753007859922945
{'loss': 0.0007, 'grad_norm': 0.029979432001709938, 'learning_rate': 4.3574007220216605e-05, 'epoch': 0.79}
対照学習損失: 0.000433891371358186
対照学習損失: 0.010755459778010845
{'loss': 0.0056, 'grad_norm': 0.44218456745147705, 'learning_rate': 4.355595667870036e-05, 'epoch': 0.79}
対照学習損失: 0.00011085470032412559
対照学習損失: 0.0006110261310823262
{'loss': 0.0004, 'grad_norm': 0.016199734061956406, 'learning_rate': 4.353790613718412e-05, 'epoch': 0.79}
対照学習損失: 6.328702147584409e-05
対照学習損失: 0.0009696746128611267
{'loss': 0.0005, 'grad_norm': 0.04465101659297943, 'learning_rate': 4.351985559566788e-05, 'epoch': 0.79}
対照学習損失: 3.894164910889231e-05
対照学習損失: 0.0006548150558955967
{'loss': 0.0003, 'grad_norm': 0.019740328192710876, 'learning_rate': 4.350180505415163e-05, 'epoch': 0.79}
対照学習損失: 0.0010857583256438375
対照学習損失: 0.002352505223825574
{'loss': 0.0017, 'grad_norm': 0.12744581699371338, 'learning_rate': 4.3483754512635384e-05, 'epoch': 0.79}
対照学習損失: 0.0002651563554536551
対照学習損失: 0.04170409217476845
{'loss': 0.021, 'grad_norm': 0.6264439821243286, 'learning_rate': 4.3465703971119135e-05, 'epoch': 0.79}
対照学習損失: 0.0002428596344543621
対照学習損失: 3.0858856916893274e-05
{'loss': 0.0001, 'grad_norm': 0.007054717745631933, 'learning_rate': 4.344765342960289e-05, 'epoch': 0.79}
対照学習損失: 0.0002903347194660455
対照学習損失: 9.788367606233805e-05
{'loss': 0.0002, 'grad_norm': 0.009496760554611683, 'learning_rate': 4.342960288808664e-05, 'epoch': 0.79}
対照学習損失: 7.854406430851668e-05
対照学習損失: 8.612677629571408e-05
{'loss': 0.0001, 'grad_norm': 0.0032519185915589333, 'learning_rate': 4.34115523465704e-05, 'epoch': 0.79}
対照学習損失: 4.9431044317316264e-05
対照学習損失: 3.059007940464653e-05
{'loss': 0.0, 'grad_norm': 0.0010995453922078013, 'learning_rate': 4.339350180505416e-05, 'epoch': 0.8}
対照学習損失: 0.00011707236262736842
対照学習損失: 0.0006892303354106843
{'loss': 0.0004, 'grad_norm': 0.029344862326979637, 'learning_rate': 4.337545126353791e-05, 'epoch': 0.8}
対照学習損失: 7.754289254080504e-05
対照学習損失: 0.0002266976225655526
{'loss': 0.0002, 'grad_norm': 0.013634891249239445, 'learning_rate': 4.3357400722021665e-05, 'epoch': 0.8}
対照学習損失: 0.0002710657427087426
対照学習損失: 0.0001419870968675241
{'loss': 0.0002, 'grad_norm': 0.010027061216533184, 'learning_rate': 4.3339350180505415e-05, 'epoch': 0.8}
対照学習損失: 3.501502578728832e-05
対照学習損失: 7.282445585587993e-05
{'loss': 0.0001, 'grad_norm': 0.002151265274733305, 'learning_rate': 4.332129963898917e-05, 'epoch': 0.8}
対照学習損失: 8.856267959345132e-05
対照学習損失: 0.003634332912042737
{'loss': 0.0019, 'grad_norm': 0.08216032385826111, 'learning_rate': 4.330324909747292e-05, 'epoch': 0.8}
対照学習損失: 4.031313437735662e-05
対照学習損失: 0.0025460838805884123
{'loss': 0.0013, 'grad_norm': 0.2101701945066452, 'learning_rate': 4.328519855595668e-05, 'epoch': 0.8}
対照学習損失: 8.640727901365608e-05
対照学習損失: 0.00074782595038414
{'loss': 0.0004, 'grad_norm': 0.05231500789523125, 'learning_rate': 4.326714801444044e-05, 'epoch': 0.8}
対照学習損失: 5.196456186240539e-05
対照学習損失: 3.385300442459993e-05
{'loss': 0.0, 'grad_norm': 0.0009511442622169852, 'learning_rate': 4.324909747292419e-05, 'epoch': 0.8}
対照学習損失: 0.00016439362661913037
対照学習損失: 0.0008179007563740015
{'loss': 0.0005, 'grad_norm': 0.02770543284714222, 'learning_rate': 4.3231046931407945e-05, 'epoch': 0.8}
対照学習損失: 0.0002244857168989256
対照学習損失: 0.00011190520308446139
{'loss': 0.0002, 'grad_norm': 0.005837109871208668, 'learning_rate': 4.3212996389891695e-05, 'epoch': 0.8}
対照学習損失: 0.00017839294741861522
対照学習損失: 0.0004093557654414326
{'loss': 0.0003, 'grad_norm': 0.011561873368918896, 'learning_rate': 4.319494584837545e-05, 'epoch': 0.81}
対照学習損失: 0.00037953516584821045
対照学習損失: 0.00028331734938547015
{'loss': 0.0003, 'grad_norm': 0.022402847185730934, 'learning_rate': 4.31768953068592e-05, 'epoch': 0.81}
対照学習損失: 0.0002033580094575882
対照学習損失: 0.09908708184957504
{'loss': 0.0496, 'grad_norm': 1.6500974893569946, 'learning_rate': 4.315884476534297e-05, 'epoch': 0.81}
対照学習損失: 5.5457232519984245e-05
対照学習損失: 0.00023102221894077957
{'loss': 0.0001, 'grad_norm': 0.005200070794671774, 'learning_rate': 4.314079422382672e-05, 'epoch': 0.81}
対照学習損失: 0.00014817444025538862
対照学習損失: 0.00012094376143068075
{'loss': 0.0001, 'grad_norm': 0.007206237409263849, 'learning_rate': 4.3122743682310474e-05, 'epoch': 0.81}
対照学習損失: 0.0004406988446135074
対照学習損失: 0.00015057293057907373
{'loss': 0.0003, 'grad_norm': 0.014804694801568985, 'learning_rate': 4.3104693140794225e-05, 'epoch': 0.81}
対照学習損失: 0.0007142059039324522
対照学習損失: 0.02316301316022873
{'loss': 0.0119, 'grad_norm': 1.3353890180587769, 'learning_rate': 4.308664259927798e-05, 'epoch': 0.81}
対照学習損失: 4.768103826791048e-05
対照学習損失: 3.846416075248271e-05
{'loss': 0.0, 'grad_norm': 0.0009406143799424171, 'learning_rate': 4.306859205776174e-05, 'epoch': 0.81}
対照学習損失: 0.00012451136717572808
対照学習損失: 2.9100328902131878e-05
{'loss': 0.0001, 'grad_norm': 0.0029471672605723143, 'learning_rate': 4.305054151624549e-05, 'epoch': 0.81}
対照学習損失: 2.6255311240674928e-05
対照学習損失: 0.0005288601969368756
{'loss': 0.0003, 'grad_norm': 0.04121403023600578, 'learning_rate': 4.303249097472925e-05, 'epoch': 0.81}
対照学習損失: 0.001496999873779714
対照学習損失: 6.951505929464474e-05
{'loss': 0.0008, 'grad_norm': 0.034574661403894424, 'learning_rate': 4.3014440433213e-05, 'epoch': 0.81}
対照学習損失: 7.911103602964431e-05
対照学習損失: 1.9497763787512667e-05
{'loss': 0.0, 'grad_norm': 0.0018487708875909448, 'learning_rate': 4.2996389891696754e-05, 'epoch': 0.82}
対照学習損失: 0.00021164535428397357
対照学習損失: 3.648194615379907e-05
{'loss': 0.0001, 'grad_norm': 0.004068739712238312, 'learning_rate': 4.2978339350180505e-05, 'epoch': 0.82}
対照学習損失: 9.260986189474352e-06
対照学習損失: 4.8409794544568285e-05
{'loss': 0.0, 'grad_norm': 0.0008846934651955962, 'learning_rate': 4.296028880866426e-05, 'epoch': 0.82}
対照学習損失: 0.0001373464911011979
対照学習損失: 0.0002943144063465297
{'loss': 0.0002, 'grad_norm': 0.01675987057387829, 'learning_rate': 4.294223826714802e-05, 'epoch': 0.82}
対照学習損失: 0.00023510868777520955
対照学習損失: 0.00028074337751604617
{'loss': 0.0003, 'grad_norm': 0.015252131037414074, 'learning_rate': 4.292418772563177e-05, 'epoch': 0.82}
対照学習損失: 7.039841875666752e-05
対照学習損失: 3.241395461373031e-05
{'loss': 0.0001, 'grad_norm': 0.001797127304598689, 'learning_rate': 4.290613718411553e-05, 'epoch': 0.82}
対照学習損失: 0.0001638974790694192
対照学習損失: 4.7412264393642545e-05
{'loss': 0.0001, 'grad_norm': 0.004017922095954418, 'learning_rate': 4.288808664259928e-05, 'epoch': 0.82}
対照学習損失: 0.0003833605151157826
対照学習損失: 3.249056317145005e-05
{'loss': 0.0002, 'grad_norm': 0.0094077093526721, 'learning_rate': 4.2870036101083035e-05, 'epoch': 0.82}
対照学習損失: 0.003975550644099712
対照学習損失: 0.0006966742221266031
{'loss': 0.0023, 'grad_norm': 0.10460258275270462, 'learning_rate': 4.2851985559566785e-05, 'epoch': 0.82}
対照学習損失: 4.7463152441196144e-05
対照学習損失: 0.00012233316374477
{'loss': 0.0001, 'grad_norm': 0.0024224561639130116, 'learning_rate': 4.283393501805054e-05, 'epoch': 0.82}
対照学習損失: 1.6726067769923247e-05
対照学習損失: 9.880952711682767e-05
{'loss': 0.0001, 'grad_norm': 0.0022815431002527475, 'learning_rate': 4.281588447653429e-05, 'epoch': 0.82}
対照学習損失: 0.0001097565473173745
対照学習損失: 6.597492756554857e-05
{'loss': 0.0001, 'grad_norm': 0.0021518778521567583, 'learning_rate': 4.279783393501805e-05, 'epoch': 0.83}
対照学習損失: 7.007282692939043e-05
対照学習損失: 2.247777592856437e-05
{'loss': 0.0, 'grad_norm': 0.0022603280376642942, 'learning_rate': 4.277978339350181e-05, 'epoch': 0.83}
対照学習損失: 2.902586493291892e-05
対照学習損失: 0.0010980198858305812
{'loss': 0.0006, 'grad_norm': 0.032696548849344254, 'learning_rate': 4.2761732851985564e-05, 'epoch': 0.83}
対照学習損失: 5.636475543724373e-05
対照学習損失: 9.438963752472773e-05
{'loss': 0.0001, 'grad_norm': 0.002757274080067873, 'learning_rate': 4.274368231046932e-05, 'epoch': 0.83}
対照学習損失: 0.0001205187727464363
対照学習損失: 0.00031796173425391316
{'loss': 0.0002, 'grad_norm': 0.008530305698513985, 'learning_rate': 4.272563176895307e-05, 'epoch': 0.83}
対照学習損失: 2.5018460291903466e-05
対照学習損失: 0.0007512450683861971
{'loss': 0.0004, 'grad_norm': 0.019877804443240166, 'learning_rate': 4.270758122743683e-05, 'epoch': 0.83}
対照学習損失: 6.240567017812282e-05
対照学習損失: 0.00044926017289981246
{'loss': 0.0003, 'grad_norm': 0.0139247952029109, 'learning_rate': 4.268953068592058e-05, 'epoch': 0.83}
対照学習損失: 4.010896373074502e-05
対照学習損失: 0.0001452213036827743
{'loss': 0.0001, 'grad_norm': 0.0034091442357748747, 'learning_rate': 4.267148014440434e-05, 'epoch': 0.83}
対照学習損失: 3.6736313631990924e-05
対照学習損失: 0.00011244158667977899
{'loss': 0.0001, 'grad_norm': 0.0025288425385951996, 'learning_rate': 4.265342960288809e-05, 'epoch': 0.83}
対照学習損失: 0.0002633002295624465
対照学習損失: 0.0004931932780891657
{'loss': 0.0004, 'grad_norm': 0.026247506961226463, 'learning_rate': 4.2635379061371844e-05, 'epoch': 0.83}
対照学習損失: 3.691588790388778e-05
対照学習損失: 0.0005181637825444341
{'loss': 0.0003, 'grad_norm': 0.02670799195766449, 'learning_rate': 4.26173285198556e-05, 'epoch': 0.83}
対照学習損失: 7.979912334121764e-05
対照学習損失: 1.841746416175738e-05
{'loss': 0.0, 'grad_norm': 0.0013449351536110044, 'learning_rate': 4.259927797833935e-05, 'epoch': 0.84}
対照学習損失: 0.000256878265645355
対照学習損失: 0.03228724002838135
{'loss': 0.0163, 'grad_norm': 0.46764713525772095, 'learning_rate': 4.258122743682311e-05, 'epoch': 0.84}
対照学習損失: 5.725011578761041e-05
対照学習損失: 4.8229867388727143e-05
{'loss': 0.0001, 'grad_norm': 0.001923175761476159, 'learning_rate': 4.256317689530686e-05, 'epoch': 0.84}
対照学習損失: 0.00011684943456202745
対照学習損失: 0.00010209869651589543
{'loss': 0.0001, 'grad_norm': 0.004053622018545866, 'learning_rate': 4.254512635379062e-05, 'epoch': 0.84}
対照学習損失: 3.091054895776324e-05
対照学習損失: 3.582058707252145e-05
{'loss': 0.0, 'grad_norm': 0.0009314134367741644, 'learning_rate': 4.252707581227437e-05, 'epoch': 0.84}
対照学習損失: 2.3431068257195875e-05
対照学習損失: 0.00022231497860047966
{'loss': 0.0001, 'grad_norm': 0.006398976780474186, 'learning_rate': 4.2509025270758124e-05, 'epoch': 0.84}
対照学習損失: 9.280089579988271e-05
対照学習損失: 1.8424700101604685e-05
{'loss': 0.0001, 'grad_norm': 0.0018225398380309343, 'learning_rate': 4.2490974729241875e-05, 'epoch': 0.84}
対照学習損失: 0.0005993578815832734
対照学習損失: 0.00012032187078148127
{'loss': 0.0004, 'grad_norm': 0.02461141161620617, 'learning_rate': 4.247292418772563e-05, 'epoch': 0.84}
対照学習損失: 0.00010211086191702634
対照学習損失: 0.00014337198808789253
{'loss': 0.0001, 'grad_norm': 0.0045965369790792465, 'learning_rate': 4.245487364620939e-05, 'epoch': 0.84}
対照学習損失: 7.593128975713626e-05
対照学習損失: 0.0001510901638539508
{'loss': 0.0001, 'grad_norm': 0.005635159555822611, 'learning_rate': 4.243682310469314e-05, 'epoch': 0.84}
対照学習損失: 3.9804559492040426e-05
対照学習損失: 2.0272356778150424e-05
{'loss': 0.0, 'grad_norm': 0.0007605580613017082, 'learning_rate': 4.24187725631769e-05, 'epoch': 0.84}
対照学習損失: 2.9912898753536865e-05
対照学習損失: 0.00015685548714827746
{'loss': 0.0001, 'grad_norm': 0.0062522878870368, 'learning_rate': 4.240072202166065e-05, 'epoch': 0.85}
対照学習損失: 0.0001744934415910393
対照学習損失: 0.0005090825143270195
{'loss': 0.0003, 'grad_norm': 0.015818195417523384, 'learning_rate': 4.238267148014441e-05, 'epoch': 0.85}
対照学習損失: 4.127392094233073e-05
対照学習損失: 0.00011617479322012514
{'loss': 0.0001, 'grad_norm': 0.002692230511456728, 'learning_rate': 4.236462093862816e-05, 'epoch': 0.85}
対照学習損失: 0.00017340108752250671
対照学習損失: 0.00035673758247867227
{'loss': 0.0003, 'grad_norm': 0.023789918050169945, 'learning_rate': 4.234657039711192e-05, 'epoch': 0.85}
対照学習損失: 7.352745160460472e-05
対照学習損失: 0.0027488218620419502
{'loss': 0.0014, 'grad_norm': 0.09158282727003098, 'learning_rate': 4.232851985559567e-05, 'epoch': 0.85}
対照学習損失: 8.946980233304203e-05
対照学習損失: 0.00066449522273615
{'loss': 0.0004, 'grad_norm': 0.01388523168861866, 'learning_rate': 4.2310469314079427e-05, 'epoch': 0.85}
対照学習損失: 0.00015514277038164437
対照学習損失: 0.0002897710364777595
{'loss': 0.0002, 'grad_norm': 0.010899126529693604, 'learning_rate': 4.229241877256318e-05, 'epoch': 0.85}
対照学習損失: 9.653074812376872e-05
対照学習損失: 0.003081659087911248
{'loss': 0.0016, 'grad_norm': 0.25609949231147766, 'learning_rate': 4.2274368231046934e-05, 'epoch': 0.85}
対照学習損失: 0.002869079587981105
対照学習損失: 0.00013242350541986525
{'loss': 0.0015, 'grad_norm': 0.06323304027318954, 'learning_rate': 4.225631768953069e-05, 'epoch': 0.85}
対照学習損失: 0.0017862870590761304
対照学習損失: 7.58002424845472e-05
{'loss': 0.0009, 'grad_norm': 0.03310605511069298, 'learning_rate': 4.223826714801444e-05, 'epoch': 0.85}
対照学習損失: 8.485458238283172e-05
対照学習損失: 0.001008029212243855
{'loss': 0.0005, 'grad_norm': 0.049270883202552795, 'learning_rate': 4.22202166064982e-05, 'epoch': 0.85}
対照学習損失: 0.0003606979735195637
対照学習損失: 0.0011671765241771936
{'loss': 0.0008, 'grad_norm': 0.03529474511742592, 'learning_rate': 4.220216606498195e-05, 'epoch': 0.86}
対照学習損失: 0.00049537792801857
対照学習損失: 0.0002903604181483388
{'loss': 0.0004, 'grad_norm': 0.04321605712175369, 'learning_rate': 4.218411552346571e-05, 'epoch': 0.86}
対照学習損失: 3.1067604140844196e-05
対照学習損失: 0.0019361674785614014
{'loss': 0.001, 'grad_norm': 0.07164973765611649, 'learning_rate': 4.216606498194946e-05, 'epoch': 0.86}
対照学習損失: 2.427994149911683e-05
対照学習損失: 0.00027284218231216073
{'loss': 0.0001, 'grad_norm': 0.010012008249759674, 'learning_rate': 4.2148014440433214e-05, 'epoch': 0.86}
対照学習損失: 7.833701965864748e-05
対照学習損失: 6.203709199326113e-05
{'loss': 0.0001, 'grad_norm': 0.0022335515823215246, 'learning_rate': 4.212996389891697e-05, 'epoch': 0.86}
対照学習損失: 0.0011712933192029595
対照学習損失: 3.3122934837592766e-05
{'loss': 0.0006, 'grad_norm': 0.031839240342378616, 'learning_rate': 4.211191335740072e-05, 'epoch': 0.86}
対照学習損失: 0.00015365389117505401
対照学習損失: 0.0005627494538202882
{'loss': 0.0004, 'grad_norm': 0.02046687714755535, 'learning_rate': 4.209386281588448e-05, 'epoch': 0.86}
対照学習損失: 9.249228605767712e-05
対照学習損失: 6.905203190399334e-05
{'loss': 0.0001, 'grad_norm': 0.002379033016040921, 'learning_rate': 4.207581227436823e-05, 'epoch': 0.86}
対照学習損失: 3.0225037335185334e-05
対照学習損失: 8.460418757749721e-05
{'loss': 0.0001, 'grad_norm': 0.006620829924941063, 'learning_rate': 4.205776173285199e-05, 'epoch': 0.86}
対照学習損失: 8.143421837303322e-06
対照学習損失: 0.0004459221672732383
{'loss': 0.0002, 'grad_norm': 0.027495434507727623, 'learning_rate': 4.203971119133574e-05, 'epoch': 0.86}
対照学習損失: 0.00010705723980208859
対照学習損失: 8.097090176306665e-05
{'loss': 0.0001, 'grad_norm': 0.003968862816691399, 'learning_rate': 4.2021660649819494e-05, 'epoch': 0.86}
対照学習損失: 3.9475460653193295e-05
対照学習損失: 4.344283661339432e-05
{'loss': 0.0, 'grad_norm': 0.0018524866318330169, 'learning_rate': 4.200361010830325e-05, 'epoch': 0.87}
対照学習損失: 0.00015978454030118883
対照学習損失: 0.00261448766104877
{'loss': 0.0014, 'grad_norm': 0.15087980031967163, 'learning_rate': 4.198555956678701e-05, 'epoch': 0.87}
対照学習損失: 0.000957624870352447
対照学習損失: 0.0001637384993955493
{'loss': 0.0006, 'grad_norm': 0.06019041687250137, 'learning_rate': 4.196750902527076e-05, 'epoch': 0.87}
対照学習損失: 0.00024225354718510062
対照学習損失: 0.013340801931917667
{'loss': 0.0068, 'grad_norm': 0.8198258280754089, 'learning_rate': 4.1949458483754516e-05, 'epoch': 0.87}
対照学習損失: 2.599433355499059e-05
対照学習損失: 2.6544939828454517e-05
{'loss': 0.0, 'grad_norm': 0.0008682786719873548, 'learning_rate': 4.1931407942238274e-05, 'epoch': 0.87}
対照学習損失: 1.8022516087512486e-05
対照学習損失: 0.0007374801207333803
{'loss': 0.0004, 'grad_norm': 0.04194161295890808, 'learning_rate': 4.1913357400722024e-05, 'epoch': 0.87}
対照学習損失: 2.590427902759984e-05
対照学習損失: 8.73721728567034e-05
{'loss': 0.0001, 'grad_norm': 0.0038352918345481157, 'learning_rate': 4.189530685920578e-05, 'epoch': 0.87}
対照学習損失: 0.0009784745052456856
対照学習損失: 0.0009557484299875796
{'loss': 0.001, 'grad_norm': 0.07112530618906021, 'learning_rate': 4.187725631768953e-05, 'epoch': 0.87}
対照学習損失: 0.00042836141074076295
対照学習損失: 3.4375887480564415e-05
{'loss': 0.0002, 'grad_norm': 0.008051874116063118, 'learning_rate': 4.185920577617329e-05, 'epoch': 0.87}
対照学習損失: 6.04945162194781e-05
対照学習損失: 0.00015286609414033592
{'loss': 0.0001, 'grad_norm': 0.003754722885787487, 'learning_rate': 4.184115523465704e-05, 'epoch': 0.87}
対照学習損失: 0.0002015632198890671
対照学習損失: 3.9067665056791157e-05
{'loss': 0.0001, 'grad_norm': 0.0092668691650033, 'learning_rate': 4.1823104693140797e-05, 'epoch': 0.87}
対照学習損失: 2.5353579985676333e-05
対照学習損失: 3.343591379234567e-05
{'loss': 0.0, 'grad_norm': 0.0007749475771561265, 'learning_rate': 4.1805054151624554e-05, 'epoch': 0.88}
対照学習損失: 0.013720916584134102
対照学習損失: 0.0002769116254057735
{'loss': 0.007, 'grad_norm': 0.30120620131492615, 'learning_rate': 4.1787003610108304e-05, 'epoch': 0.88}
対照学習損失: 0.0028556420002132654
対照学習損失: 0.0001719605061225593
{'loss': 0.0015, 'grad_norm': 0.15686430037021637, 'learning_rate': 4.176895306859206e-05, 'epoch': 0.88}
対照学習損失: 0.00014526532322634012
対照学習損失: 4.223394353175536e-05
{'loss': 0.0001, 'grad_norm': 0.0044061713851988316, 'learning_rate': 4.175090252707581e-05, 'epoch': 0.88}
対照学習損失: 7.615624781465158e-05
対照学習損失: 0.0006389355985447764
{'loss': 0.0004, 'grad_norm': 0.029928265139460564, 'learning_rate': 4.173285198555957e-05, 'epoch': 0.88}
対照学習損失: 4.45497571490705e-05
対照学習損失: 8.645770867587999e-05
{'loss': 0.0001, 'grad_norm': 0.005321569740772247, 'learning_rate': 4.171480144404332e-05, 'epoch': 0.88}
対照学習損失: 0.000424011901486665
対照学習損失: 0.00014141025894787163
{'loss': 0.0003, 'grad_norm': 0.024410430341959, 'learning_rate': 4.169675090252708e-05, 'epoch': 0.88}
対照学習損失: 0.0029367441311478615
対照学習損失: 0.0008237904403358698
{'loss': 0.0019, 'grad_norm': 0.16158059239387512, 'learning_rate': 4.1678700361010834e-05, 'epoch': 0.88}
対照学習損失: 0.0007247477769851685
対照学習損失: 6.445717735914513e-05
{'loss': 0.0004, 'grad_norm': 0.011104168370366096, 'learning_rate': 4.1660649819494584e-05, 'epoch': 0.88}
対照学習損失: 0.007136154919862747
対照学習損失: 0.0002346490800846368
{'loss': 0.0037, 'grad_norm': 0.345357745885849, 'learning_rate': 4.164259927797834e-05, 'epoch': 0.88}
対照学習損失: 0.000180058108526282
対照学習損失: 4.3218118662480265e-05
{'loss': 0.0001, 'grad_norm': 0.005486650392413139, 'learning_rate': 4.162454873646209e-05, 'epoch': 0.88}
対照学習損失: 0.0003631505824159831
対照学習損失: 4.538585926638916e-05
{'loss': 0.0002, 'grad_norm': 0.007645247038453817, 'learning_rate': 4.160649819494585e-05, 'epoch': 0.89}
対照学習損失: 3.696593557833694e-05
対照学習損失: 3.1655992643209174e-05
{'loss': 0.0, 'grad_norm': 0.0009337011142633855, 'learning_rate': 4.1588447653429606e-05, 'epoch': 0.89}
対照学習損失: 1.533298745926004e-05
対照学習損失: 1.0989451766363345e-05
{'loss': 0.0, 'grad_norm': 0.0003487848152872175, 'learning_rate': 4.1570397111913364e-05, 'epoch': 0.89}
対照学習損失: 8.870380406733602e-05
対照学習損失: 3.7696103390771896e-05
{'loss': 0.0001, 'grad_norm': 0.0020013893954455853, 'learning_rate': 4.1552346570397114e-05, 'epoch': 0.89}
対照学習損失: 0.0005817626952193677
対照学習損失: 0.04030539467930794
{'loss': 0.0204, 'grad_norm': 0.9962982535362244, 'learning_rate': 4.153429602888087e-05, 'epoch': 0.89}
対照学習損失: 3.149221811327152e-05
対照学習損失: 8.122548751998693e-05
{'loss': 0.0001, 'grad_norm': 0.002286978531628847, 'learning_rate': 4.151624548736462e-05, 'epoch': 0.89}
対照学習損失: 5.443314148578793e-05
対照学習損失: 6.680349179077893e-05
{'loss': 0.0001, 'grad_norm': 0.002171984640881419, 'learning_rate': 4.149819494584838e-05, 'epoch': 0.89}
対照学習損失: 7.260896381922066e-05
対照学習損失: 2.173273060179781e-05
{'loss': 0.0, 'grad_norm': 0.002099424134939909, 'learning_rate': 4.1480144404332136e-05, 'epoch': 0.89}
対照学習損失: 4.606806760421023e-05
対照学習損失: 8.93312790140044e-06
{'loss': 0.0, 'grad_norm': 0.0015318236546590924, 'learning_rate': 4.1462093862815886e-05, 'epoch': 0.89}
対照学習損失: 0.0007942204247228801
対照学習損失: 7.84867734182626e-05
{'loss': 0.0004, 'grad_norm': 0.04594961553812027, 'learning_rate': 4.1444043321299644e-05, 'epoch': 0.89}
対照学習損失: 0.000493139261379838
対照学習損失: 8.777088078204542e-05
{'loss': 0.0003, 'grad_norm': 0.010731247253715992, 'learning_rate': 4.1425992779783394e-05, 'epoch': 0.9}
対照学習損失: 4.402792910696007e-05
対照学習損失: 0.00016818047151900828
{'loss': 0.0001, 'grad_norm': 0.0037630770821124315, 'learning_rate': 4.140794223826715e-05, 'epoch': 0.9}
対照学習損失: 0.00013860402395948768
対照学習損失: 0.0001478975755162537
{'loss': 0.0001, 'grad_norm': 0.00525833060964942, 'learning_rate': 4.13898916967509e-05, 'epoch': 0.9}
対照学習損失: 2.137485716957599e-05
対照学習損失: 0.00046870551886968315
{'loss': 0.0002, 'grad_norm': 0.014824346639215946, 'learning_rate': 4.137184115523466e-05, 'epoch': 0.9}
対照学習損失: 3.864460813929327e-05
対照学習損失: 0.00012571120169013739
{'loss': 0.0001, 'grad_norm': 0.006156079936772585, 'learning_rate': 4.1353790613718416e-05, 'epoch': 0.9}
対照学習損失: 9.450521611142904e-05
対照学習損失: 8.198200521292165e-05
{'loss': 0.0001, 'grad_norm': 0.002949612680822611, 'learning_rate': 4.1335740072202167e-05, 'epoch': 0.9}
対照学習損失: 0.0010184133425354958
対照学習損失: 0.00028564027161337435
{'loss': 0.0007, 'grad_norm': 0.034061066806316376, 'learning_rate': 4.1317689530685924e-05, 'epoch': 0.9}
対照学習損失: 0.0001649576734052971
対照学習損失: 0.0009512797696515918
{'loss': 0.0006, 'grad_norm': 0.040644872933626175, 'learning_rate': 4.1299638989169674e-05, 'epoch': 0.9}
対照学習損失: 0.013542785309255123
対照学習損失: 6.060451778466813e-05
{'loss': 0.0068, 'grad_norm': 0.3258739113807678, 'learning_rate': 4.128158844765343e-05, 'epoch': 0.9}
対照学習損失: 3.043437573069241e-05
対照学習損失: 0.00012364352005533874
{'loss': 0.0001, 'grad_norm': 0.003173894714564085, 'learning_rate': 4.126353790613718e-05, 'epoch': 0.9}
対照学習損失: 4.391522452351637e-05
対照学習損失: 7.114528125384822e-05
{'loss': 0.0001, 'grad_norm': 0.0026522018015384674, 'learning_rate': 4.124548736462094e-05, 'epoch': 0.9}
対照学習損失: 0.00011379582429071888
対照学習損失: 8.375893230549991e-05
{'loss': 0.0001, 'grad_norm': 0.003833947703242302, 'learning_rate': 4.122743682310469e-05, 'epoch': 0.91}
対照学習損失: 0.00015748445002827793
対照学習損失: 0.0002538573753554374
{'loss': 0.0002, 'grad_norm': 0.00748045090585947, 'learning_rate': 4.1209386281588453e-05, 'epoch': 0.91}
対照学習損失: 0.018023494631052017
対照学習損失: 0.0002553340164013207
{'loss': 0.0091, 'grad_norm': 0.485490620136261, 'learning_rate': 4.1191335740072204e-05, 'epoch': 0.91}
対照学習損失: 1.897621768875979e-05
対照学習損失: 0.00030499050626531243
{'loss': 0.0002, 'grad_norm': 0.006276087835431099, 'learning_rate': 4.117328519855596e-05, 'epoch': 0.91}
対照学習損失: 6.165080412756652e-05
対照学習損失: 0.0004591774195432663
{'loss': 0.0003, 'grad_norm': 0.013987849466502666, 'learning_rate': 4.115523465703972e-05, 'epoch': 0.91}
対照学習損失: 8.082485874183476e-05
対照学習損失: 4.9443828174844384e-05
{'loss': 0.0001, 'grad_norm': 0.0018398379907011986, 'learning_rate': 4.113718411552347e-05, 'epoch': 0.91}
対照学習損失: 6.525202479679137e-05
対照学習損失: 0.00010370905511081219
{'loss': 0.0001, 'grad_norm': 0.0023916256614029408, 'learning_rate': 4.1119133574007226e-05, 'epoch': 0.91}
対照学習損失: 2.102508005918935e-05
対照学習損失: 0.00015526660718023777
{'loss': 0.0001, 'grad_norm': 0.004373025614768267, 'learning_rate': 4.1101083032490976e-05, 'epoch': 0.91}
対照学習損失: 1.96167420654092e-05
対照学習損失: 3.25944711221382e-05
{'loss': 0.0, 'grad_norm': 0.0006188609404489398, 'learning_rate': 4.1083032490974734e-05, 'epoch': 0.91}
対照学習損失: 3.0373808840522543e-05
対照学習損失: 0.00011067752348026261
{'loss': 0.0001, 'grad_norm': 0.0018908611964434385, 'learning_rate': 4.1064981949458484e-05, 'epoch': 0.91}
対照学習損失: 3.403968730708584e-05
対照学習損失: 0.000334016396664083
{'loss': 0.0002, 'grad_norm': 0.006951943971216679, 'learning_rate': 4.104693140794224e-05, 'epoch': 0.91}
対照学習損失: 3.6631987313739955e-05
対照学習損失: 0.00022357719717547297
{'loss': 0.0001, 'grad_norm': 0.004529542755335569, 'learning_rate': 4.102888086642599e-05, 'epoch': 0.92}
対照学習損失: 3.146160815958865e-05
対照学習損失: 0.00011016787902917713
{'loss': 0.0001, 'grad_norm': 0.0024723587557673454, 'learning_rate': 4.101083032490975e-05, 'epoch': 0.92}
対照学習損失: 0.00016722269356250763
対照学習損失: 0.00021689767891075462
{'loss': 0.0002, 'grad_norm': 0.013605370186269283, 'learning_rate': 4.0992779783393506e-05, 'epoch': 0.92}
対照学習損失: 3.012091292475816e-05
対照学習損失: 1.3559839317167643e-05
{'loss': 0.0, 'grad_norm': 0.0006226010737009346, 'learning_rate': 4.0974729241877256e-05, 'epoch': 0.92}
対照学習損失: 0.0009446857729926705
対照学習損失: 0.0003592147841118276
{'loss': 0.0007, 'grad_norm': 0.048118628561496735, 'learning_rate': 4.0956678700361014e-05, 'epoch': 0.92}
対照学習損失: 0.010474864393472672
対照学習損失: 0.0001518054777989164
{'loss': 0.0053, 'grad_norm': 0.4398467540740967, 'learning_rate': 4.0938628158844764e-05, 'epoch': 0.92}
対照学習損失: 3.2132847991306335e-05
対照学習損失: 6.225054676178843e-05
{'loss': 0.0, 'grad_norm': 0.0015975036658346653, 'learning_rate': 4.092057761732852e-05, 'epoch': 0.92}
対照学習損失: 6.977151497267187e-05
対照学習損失: 8.700244507053867e-05
{'loss': 0.0001, 'grad_norm': 0.004182607866823673, 'learning_rate': 4.090252707581227e-05, 'epoch': 0.92}
対照学習損失: 9.342523844679818e-05
対照学習損失: 2.2008434825693257e-05
{'loss': 0.0001, 'grad_norm': 0.0024316946510225534, 'learning_rate': 4.088447653429603e-05, 'epoch': 0.92}
対照学習損失: 0.00022207925212569535
対照学習損失: 0.0002770391874946654
{'loss': 0.0002, 'grad_norm': 0.012816570699214935, 'learning_rate': 4.0866425992779786e-05, 'epoch': 0.92}
対照学習損失: 0.00014502563863061368
対照学習損失: 3.1335534004028887e-05
{'loss': 0.0001, 'grad_norm': 0.002625111723318696, 'learning_rate': 4.0848375451263537e-05, 'epoch': 0.92}
対照学習損失: 9.357780800200999e-05
対照学習損失: 0.00015638205513823777
{'loss': 0.0001, 'grad_norm': 0.004320425912737846, 'learning_rate': 4.0830324909747294e-05, 'epoch': 0.93}
対照学習損失: 0.0003795388911385089
対照学習損失: 5.326903919922188e-05
{'loss': 0.0002, 'grad_norm': 0.008203656412661076, 'learning_rate': 4.081227436823105e-05, 'epoch': 0.93}
対照学習損失: 0.0034671290777623653
対照学習損失: 0.0004380183236207813
{'loss': 0.002, 'grad_norm': 0.2511553466320038, 'learning_rate': 4.079422382671481e-05, 'epoch': 0.93}
対照学習損失: 0.000264806643826887
対照学習損失: 3.921031020581722e-05
{'loss': 0.0002, 'grad_norm': 0.009862386621534824, 'learning_rate': 4.077617328519856e-05, 'epoch': 0.93}
対照学習損失: 0.0003039234725292772
対照学習損失: 0.00261858943849802
{'loss': 0.0015, 'grad_norm': 0.08123325556516647, 'learning_rate': 4.0758122743682316e-05, 'epoch': 0.93}
対照学習損失: 0.0004247942706570029
対照学習損失: 1.639011134102475e-05
{'loss': 0.0002, 'grad_norm': 0.008479154668748379, 'learning_rate': 4.0740072202166066e-05, 'epoch': 0.93}
対照学習損失: 0.00026447739219293
対照学習損失: 0.00015476756379939616
{'loss': 0.0002, 'grad_norm': 0.007198129314929247, 'learning_rate': 4.0722021660649823e-05, 'epoch': 0.93}
対照学習損失: 6.047525675967336e-05
対照学習損失: 0.00012194113514851779
{'loss': 0.0001, 'grad_norm': 0.0038168877363204956, 'learning_rate': 4.0703971119133574e-05, 'epoch': 0.93}
対照学習損失: 0.00016094832972157747
対照学習損失: 0.000506006006617099
{'loss': 0.0003, 'grad_norm': 0.029086297377943993, 'learning_rate': 4.068592057761733e-05, 'epoch': 0.93}
対照学習損失: 2.5538149202475324e-05
対照学習損失: 3.536483563948423e-05
{'loss': 0.0, 'grad_norm': 0.0008965565939433873, 'learning_rate': 4.066787003610109e-05, 'epoch': 0.93}
対照学習損失: 0.00014943990390747786
対照学習損失: 0.00017730251420289278
{'loss': 0.0002, 'grad_norm': 0.0072913337498903275, 'learning_rate': 4.064981949458484e-05, 'epoch': 0.93}
対照学習損失: 7.076648034853861e-05
対照学習損失: 0.00010224084689980373
{'loss': 0.0001, 'grad_norm': 0.004270699340850115, 'learning_rate': 4.0631768953068596e-05, 'epoch': 0.94}
対照学習損失: 9.948536171577871e-05
対照学習損失: 7.410769467242062e-05
{'loss': 0.0001, 'grad_norm': 0.0029138389509171247, 'learning_rate': 4.0613718411552346e-05, 'epoch': 0.94}
対照学習損失: 0.0004006699600722641
対照学習損失: 0.0001546316489111632
{'loss': 0.0003, 'grad_norm': 0.013065366074442863, 'learning_rate': 4.0595667870036104e-05, 'epoch': 0.94}
対照学習損失: 4.17433402617462e-05
対照学習損失: 0.00018653746519703418
{'loss': 0.0001, 'grad_norm': 0.004307531751692295, 'learning_rate': 4.0577617328519854e-05, 'epoch': 0.94}
対照学習損失: 4.2531773942755535e-05
対照学習損失: 0.00036720847128890455
{'loss': 0.0002, 'grad_norm': 0.007338161580264568, 'learning_rate': 4.055956678700361e-05, 'epoch': 0.94}
対照学習損失: 8.239271846832708e-05
対照学習損失: 0.00019191713363397866
{'loss': 0.0001, 'grad_norm': 0.005698228720575571, 'learning_rate': 4.054151624548737e-05, 'epoch': 0.94}
対照学習損失: 1.7530841432744637e-05
対照学習損失: 7.127456046873704e-05
{'loss': 0.0, 'grad_norm': 0.0012479601427912712, 'learning_rate': 4.052346570397112e-05, 'epoch': 0.94}
対照学習損失: 0.0010569022269919515
対照学習損失: 5.7982550060842186e-05
{'loss': 0.0006, 'grad_norm': 0.029290160164237022, 'learning_rate': 4.0505415162454876e-05, 'epoch': 0.94}
対照学習損失: 0.0005223265034146607
対照学習損失: 0.002007884904742241
{'loss': 0.0013, 'grad_norm': 0.15899841487407684, 'learning_rate': 4.0487364620938626e-05, 'epoch': 0.94}
対照学習損失: 4.336137499194592e-05
対照学習損失: 6.488751387223601e-05
{'loss': 0.0001, 'grad_norm': 0.0016502081416547298, 'learning_rate': 4.0469314079422384e-05, 'epoch': 0.94}
対照学習損失: 4.8017973313108087e-05
対照学習損失: 1.615230030438397e-05
{'loss': 0.0, 'grad_norm': 0.0024062739685177803, 'learning_rate': 4.0451263537906134e-05, 'epoch': 0.94}
対照学習損失: 0.0002181345334975049
対照学習損失: 6.744908750988543e-05
{'loss': 0.0001, 'grad_norm': 0.004819504916667938, 'learning_rate': 4.043321299638989e-05, 'epoch': 0.95}
対照学習損失: 0.00013218491221778095
対照学習損失: 0.00014895408821757883
{'loss': 0.0001, 'grad_norm': 0.008547377772629261, 'learning_rate': 4.041516245487365e-05, 'epoch': 0.95}
対照学習損失: 0.0010375460842624307
対照学習損失: 3.996209125034511e-05
{'loss': 0.0005, 'grad_norm': 0.08512860536575317, 'learning_rate': 4.0397111913357406e-05, 'epoch': 0.95}
対照学習損失: 0.0004786626959685236
対照学習損失: 1.1242782420595177e-05
{'loss': 0.0002, 'grad_norm': 0.01783204823732376, 'learning_rate': 4.0379061371841156e-05, 'epoch': 0.95}
対照学習損失: 1.3403376215137541e-05
対照学習損失: 2.0227706045261584e-05
{'loss': 0.0, 'grad_norm': 0.00048314218292944133, 'learning_rate': 4.036101083032491e-05, 'epoch': 0.95}
対照学習損失: 5.579503340413794e-05
対照学習損失: 0.0002095353847835213
{'loss': 0.0001, 'grad_norm': 0.007997400127351284, 'learning_rate': 4.034296028880867e-05, 'epoch': 0.95}
対照学習損失: 0.00024923987803049386
対照学習損失: 3.632701191236265e-05
{'loss': 0.0001, 'grad_norm': 0.00538145937025547, 'learning_rate': 4.032490974729242e-05, 'epoch': 0.95}
対照学習損失: 9.968352969735861e-05
対照学習損失: 0.00011300444748485461
{'loss': 0.0001, 'grad_norm': 0.00417883787304163, 'learning_rate': 4.030685920577618e-05, 'epoch': 0.95}
対照学習損失: 1.5184027688519564e-05
対照学習損失: 2.545010283938609e-05
{'loss': 0.0, 'grad_norm': 0.0004619582905434072, 'learning_rate': 4.028880866425993e-05, 'epoch': 0.95}
対照学習損失: 0.0006599297630600631
対照学習損失: 9.384597069583833e-05
{'loss': 0.0004, 'grad_norm': 0.026340588927268982, 'learning_rate': 4.0270758122743686e-05, 'epoch': 0.95}
対照学習損失: 0.0011290700640529394
対照学習損失: 0.0008171076187863946
{'loss': 0.001, 'grad_norm': 0.05106672644615173, 'learning_rate': 4.0252707581227436e-05, 'epoch': 0.95}
対照学習損失: 4.4003332732245326e-05
対照学習損失: 7.128438301151618e-05
{'loss': 0.0001, 'grad_norm': 0.0016211848706007004, 'learning_rate': 4.0234657039711193e-05, 'epoch': 0.96}
対照学習損失: 4.932400770485401e-05
対照学習損失: 0.04674538969993591
{'loss': 0.0234, 'grad_norm': 1.8302855491638184, 'learning_rate': 4.021660649819495e-05, 'epoch': 0.96}
対照学習損失: 8.61683365656063e-05
対照学習損失: 6.76957715768367e-05
{'loss': 0.0001, 'grad_norm': 0.0029109467286616564, 'learning_rate': 4.01985559566787e-05, 'epoch': 0.96}
対照学習損失: 2.7894169761566445e-05
対照学習損失: 0.00011031403118977323
{'loss': 0.0001, 'grad_norm': 0.0038491927552968264, 'learning_rate': 4.018050541516246e-05, 'epoch': 0.96}
対照学習損失: 0.0010424774372950196
対照学習損失: 6.993371061980724e-05
{'loss': 0.0006, 'grad_norm': 0.022195009514689445, 'learning_rate': 4.016245487364621e-05, 'epoch': 0.96}
対照学習損失: 2.652989496709779e-05
対照学習損失: 0.00015527536743320525
{'loss': 0.0001, 'grad_norm': 0.0041951765306293964, 'learning_rate': 4.0144404332129966e-05, 'epoch': 0.96}
対照学習損失: 4.226927921990864e-05
対照学習損失: 0.00028484148788265884
{'loss': 0.0002, 'grad_norm': 0.005943323019891977, 'learning_rate': 4.0126353790613716e-05, 'epoch': 0.96}
対照学習損失: 2.5614235710236244e-05
対照学習損失: 0.0001893903681775555
{'loss': 0.0001, 'grad_norm': 0.0051281568594276905, 'learning_rate': 4.0108303249097474e-05, 'epoch': 0.96}
対照学習損失: 0.0004363897314760834
対照学習損失: 8.79755534697324e-05
{'loss': 0.0003, 'grad_norm': 0.022636136040091515, 'learning_rate': 4.0090252707581224e-05, 'epoch': 0.96}
対照学習損失: 0.00010645354632288218
対照学習損失: 0.011219164356589317
{'loss': 0.0057, 'grad_norm': 0.7370168566703796, 'learning_rate': 4.007220216606498e-05, 'epoch': 0.96}
対照学習損失: 0.00012965509085915983
対照学習損失: 0.00010587845463305712
{'loss': 0.0001, 'grad_norm': 0.0059510646387934685, 'learning_rate': 4.005415162454874e-05, 'epoch': 0.96}
対照学習損失: 0.00011854748299811035
対照学習損失: 0.00011114800145151094
{'loss': 0.0001, 'grad_norm': 0.008720481768250465, 'learning_rate': 4.0036101083032496e-05, 'epoch': 0.97}
対照学習損失: 1.4565547644451726e-05
対照学習損失: 0.00016029513790272176
{'loss': 0.0001, 'grad_norm': 0.005734976381063461, 'learning_rate': 4.001805054151625e-05, 'epoch': 0.97}
対照学習損失: 0.0019941378850489855
対照学習損失: 3.579049007385038e-05
{'loss': 0.001, 'grad_norm': 0.171309232711792, 'learning_rate': 4e-05, 'epoch': 0.97}
対照学習損失: 4.781064490089193e-05
対照学習損失: 9.210359712596983e-05
{'loss': 0.0001, 'grad_norm': 0.002574786776676774, 'learning_rate': 3.998194945848376e-05, 'epoch': 0.97}
対照学習損失: 0.0002787638222798705
対照学習損失: 0.038779933005571365
{'loss': 0.0195, 'grad_norm': 0.9688481092453003, 'learning_rate': 3.996389891696751e-05, 'epoch': 0.97}
対照学習損失: 0.00038234482053667307
対照学習損失: 4.090654692845419e-05
{'loss': 0.0002, 'grad_norm': 0.023299183696508408, 'learning_rate': 3.994584837545127e-05, 'epoch': 0.97}
対照学習損失: 8.069790783338249e-05
対照学習損失: 0.00022081294446252286
{'loss': 0.0002, 'grad_norm': 0.006365884095430374, 'learning_rate': 3.992779783393502e-05, 'epoch': 0.97}
対照学習損失: 0.0001115172344725579
対照学習損失: 3.657331399153918e-05
{'loss': 0.0001, 'grad_norm': 0.0028423296753317118, 'learning_rate': 3.9909747292418776e-05, 'epoch': 0.97}
対照学習損失: 0.007823287509381771
対照学習損失: 0.00020298174058552831
{'loss': 0.004, 'grad_norm': 0.21092726290225983, 'learning_rate': 3.989169675090253e-05, 'epoch': 0.97}
対照学習損失: 0.000337093835696578
対照学習損失: 0.00136999087408185
{'loss': 0.0009, 'grad_norm': 0.03979812562465668, 'learning_rate': 3.987364620938628e-05, 'epoch': 0.97}
対照学習損失: 0.00015867521869949996
対照学習損失: 3.7720183172496036e-05
{'loss': 0.0001, 'grad_norm': 0.005088380072265863, 'learning_rate': 3.985559566787004e-05, 'epoch': 0.97}
対照学習損失: 0.0006887288764119148
対照学習損失: 0.00010607072181301191
{'loss': 0.0004, 'grad_norm': 0.03590695559978485, 'learning_rate': 3.983754512635379e-05, 'epoch': 0.98}
対照学習損失: 0.0009747768053784966
対照学習損失: 0.00010864781506825238
{'loss': 0.0005, 'grad_norm': 0.02185041457414627, 'learning_rate': 3.981949458483755e-05, 'epoch': 0.98}
対照学習損失: 4.4913413148606196e-05
対照学習損失: 0.00019393870024941862
{'loss': 0.0001, 'grad_norm': 0.00873766839504242, 'learning_rate': 3.98014440433213e-05, 'epoch': 0.98}
対照学習損失: 0.010149410925805569
対照学習損失: 9.041112207341939e-05
{'loss': 0.0051, 'grad_norm': 0.41025519371032715, 'learning_rate': 3.9783393501805056e-05, 'epoch': 0.98}
対照学習損失: 4.988388900528662e-05
対照学習損失: 3.789881520788185e-05
{'loss': 0.0, 'grad_norm': 0.0024554573465138674, 'learning_rate': 3.9765342960288806e-05, 'epoch': 0.98}
対照学習損失: 2.346872861380689e-05
対照学習損失: 6.62085076328367e-05
{'loss': 0.0, 'grad_norm': 0.0015325790736824274, 'learning_rate': 3.9747292418772563e-05, 'epoch': 0.98}
対照学習損失: 0.0025303461588919163
対照学習損失: 0.0009155806037597358
{'loss': 0.0017, 'grad_norm': 0.24219730496406555, 'learning_rate': 3.972924187725632e-05, 'epoch': 0.98}
対照学習損失: 0.0006293646874837577
対照学習損失: 9.120539471041411e-05
{'loss': 0.0004, 'grad_norm': 0.019728053361177444, 'learning_rate': 3.971119133574007e-05, 'epoch': 0.98}
対照学習損失: 0.0005267896922305226
対照学習損失: 0.0001659919653320685
{'loss': 0.0003, 'grad_norm': 0.018762946128845215, 'learning_rate': 3.969314079422383e-05, 'epoch': 0.98}
対照学習損失: 0.00022089248523116112
対照学習損失: 2.2269299734034576e-05
{'loss': 0.0001, 'grad_norm': 0.005282477010041475, 'learning_rate': 3.967509025270758e-05, 'epoch': 0.98}
対照学習損失: 0.0017707953229546547
対照学習損失: 0.00016913338913582265
{'loss': 0.001, 'grad_norm': 0.035113029181957245, 'learning_rate': 3.9657039711191336e-05, 'epoch': 0.98}
対照学習損失: 0.0002851509489119053
対照学習損失: 5.137742846272886e-05
{'loss': 0.0002, 'grad_norm': 0.013675046153366566, 'learning_rate': 3.963898916967509e-05, 'epoch': 0.99}
対照学習損失: 8.670393435750157e-05
対照学習損失: 0.010659139603376389
{'loss': 0.0054, 'grad_norm': 0.5577902793884277, 'learning_rate': 3.962093862815885e-05, 'epoch': 0.99}
対照学習損失: 0.00010770211520139128
対照学習損失: 8.520710980519652e-05
{'loss': 0.0001, 'grad_norm': 0.0038886088877916336, 'learning_rate': 3.96028880866426e-05, 'epoch': 0.99}
対照学習損失: 0.0002040033577941358
対照学習損失: 9.12421164684929e-05
{'loss': 0.0001, 'grad_norm': 0.008060487918555737, 'learning_rate': 3.958483754512636e-05, 'epoch': 0.99}
対照学習損失: 4.0265193092636764e-05
対照学習損失: 0.00020102871349081397
{'loss': 0.0001, 'grad_norm': 0.0105793671682477, 'learning_rate': 3.956678700361011e-05, 'epoch': 0.99}
対照学習損失: 0.0002947771572507918
対照学習損失: 0.002676562638953328
{'loss': 0.0015, 'grad_norm': 0.1837589293718338, 'learning_rate': 3.9548736462093866e-05, 'epoch': 0.99}
対照学習損失: 5.8889942010864615e-05
対照学習損失: 0.0022194760385900736
{'loss': 0.0011, 'grad_norm': 0.11905750632286072, 'learning_rate': 3.953068592057762e-05, 'epoch': 0.99}
対照学習損失: 2.1710318833356723e-05
対照学習損失: 0.0001764464395819232
{'loss': 0.0001, 'grad_norm': 0.003913015127182007, 'learning_rate': 3.951263537906137e-05, 'epoch': 0.99}
対照学習損失: 6.900455628056079e-05
対照学習損失: 0.00015778322995174676
{'loss': 0.0001, 'grad_norm': 0.00418900279328227, 'learning_rate': 3.949458483754513e-05, 'epoch': 0.99}
対照学習損失: 0.00015346422151196748
対照学習損失: 0.00020518201927188784
{'loss': 0.0002, 'grad_norm': 0.00860193558037281, 'learning_rate': 3.947653429602888e-05, 'epoch': 0.99}
対照学習損失: 7.05152633599937e-05
対照学習損失: 0.0014255072455853224
{'loss': 0.0007, 'grad_norm': 0.09738747775554657, 'learning_rate': 3.945848375451264e-05, 'epoch': 0.99}
対照学習損失: 8.501048796460964e-06
対照学習損失: 0.00042582981404848397
{'loss': 0.0002, 'grad_norm': 0.010652987286448479, 'learning_rate': 3.944043321299639e-05, 'epoch': 1.0}
対照学習損失: 3.064109478145838e-05
対照学習損失: 7.363491749856621e-05
{'loss': 0.0001, 'grad_norm': 0.0022895464207977057, 'learning_rate': 3.9422382671480146e-05, 'epoch': 1.0}
対照学習損失: 1.3403318007476628e-05
対照学習損失: 2.594223406049423e-05
{'loss': 0.0, 'grad_norm': 0.00042366571142338216, 'learning_rate': 3.94043321299639e-05, 'epoch': 1.0}
対照学習損失: 0.00018961964815389365
対照学習損失: 5.97704311076086e-05
{'loss': 0.0001, 'grad_norm': 0.0053124697878956795, 'learning_rate': 3.938628158844765e-05, 'epoch': 1.0}
対照学習損失: 8.97196659934707e-05
対照学習損失: 0.0005451736506074667
{'loss': 0.0003, 'grad_norm': 0.018910786136984825, 'learning_rate': 3.936823104693141e-05, 'epoch': 1.0}
対照学習損失: 3.961488982895389e-05
対照学習損失: 5.364386197470594e-06
  7%|▋         | 32/485 [00:03<00:54,  8.28it/s]
対照学習損失: 0.00018053888925351202
対照学習損失: 1.3082913028483745e-05
対照学習損失: 1.2263395547051914e-05
対照学習損失: 0.0001101890520658344
対照学習損失: 6.1541413742816076e-06
対照学習損失: 0.00011712232662830502
対照学習損失: 1.7612754163565114e-05
対照学習損失: 1.3127712009008974e-05
対照学習損失: 0.00028693597414530814
対照学習損失: 1.1965535122726578e-05
対照学習損失: 1.9534789316821843e-05
対照学習損失: 3.4868589864345267e-06
対照学習損失: 0.0009990357793867588
対照学習損失: 2.197843605244998e-05
対照学習損失: 6.571365702257026e-06
対照学習損失: 0.00010652202035998926
対照学習損失: 3.818740515271202e-05
対照学習損失: 6.899192158016376e-06
対照学習損失: 0.004224603064358234
対照学習損失: 0.0006559891044162214
対照学習損失: 0.00024511339142918587
対照学習損失: 5.8710338635137305e-06
対照学習損失: 0.00018081802409142256
対照学習損失: 1.3947258594271261e-05
対照学習損失: 0.00010612623736960813
対照学習損失: 6.0796346588176675e-06
対照学習損失: 4.1276025513070635e-06
対照学習損失: 1.9862143744830973e-05
対照学習損失: 3.6954718325432623e-06
対照学習損失: 0.011766715906560421
対照学習損失: 9.804884030018002e-06
対照学習損失: 9.313040209235623e-06
対照学習損失: 2.695500552363228e-05
対照学習損失: 0.0008387733832933009
対照学習損失: 8.582956979807932e-06
対照学習損失: 9.804834917304106e-06
対照学習損失: 0.00023017746570985764
対照学習損失: 1.9728402548935264e-05
対照学習損失: 9.015105206344742e-06
対照学習損失: 0.00011352417641319335
対照学習損失: 2.7596013751463033e-05
対照学習損失: 0.0007045279489830136
対照学習損失: 2.252957347081974e-05
対照学習損失: 9.104531272896565e-06
対照学習損失: 0.0006205533863976598
対照学習損失: 1.3440556358546019e-05
対照学習損失: 3.785905209952034e-05
対照学習損失: 5.0663729780353606e-06
対照学習損失: 2.3245775082614273e-06
対照学習損失: 2.2380059817805886e-05
対照学習損失: 0.00010083777306135744
対照学習損失: 3.988801472587511e-05
対照学習損失: 1.326175970461918e-05
対照学習損失: 1.6047979443101212e-05
対照学習損失: 0.00020774926815647632
対照学習損失: 3.1155956094153225e-05
対照学習損失: 0.00010931443830486387
対照学習損失: 4.7069290303625166e-05
対照学習損失: 9.81973971647676e-06
対照学習損失: 2.3289945602300577e-05
対照学習損失: 9.282933751819655e-05
対照学習損失: 0.0053946650587022305
対照学習損失: 1.4409182767849416e-05
対照学習損失: 0.1361553817987442
対照学習損失: 1.2054721082677133e-05
対照学習損失: 5.1747134421020746e-05
対照学習損失: 1.887912549136672e-05
対照学習損失: 3.2020165235735476e-05
対照学習損失: 0.042360350489616394
対照学習損失: 5.319691808836069e-06
対照学習損失: 5.430593591881916e-05
対照学習損失: 7.44648277759552e-05
対照学習損失: 4.611559779732488e-05
対照学習損失: 2.753509397734888e-05
対照学習損失: 6.610590207856148e-05
対照学習損失: 0.0001491651200922206
対照学習損失: 0.00013901741476729512
対照学習損失: 6.382563151419163e-05
対照学習損失: 6.541561106132576e-06
対照学習損失: 4.589530817611376e-06
対照学習損失: 1.8074861145578325e-05
対照学習損失: 7.4802987910516094e-06
対照学習損失: 0.00011190075747435912
対照学習損失: 1.0117806596099399e-05
対照学習損失: 4.738547886518063e-06
対照学習損失: 0.0003653883177321404
対照学習損失: 0.00010883323557209224
対照学習損失: 2.805792973958887e-05
対照学習損失: 4.5597407734021544e-06
対照学習損失: 0.00012754384079016745
対照学習損失: 9.730329111334868e-06
対照学習損失: 1.7597831174498424e-05
対照学習損失: 1.0713833944464568e-05
対照学習損失: 7.226982688735006e-06
対照学習損失: 4.394034476717934e-05
対照学習損失: 2.922020576079376e-05
対照学習損失: 3.398804255994037e-05
対照学習損失: 9.134310857916716e-06
対照学習損失: 0.000188063204404898
対照学習損失: 0.00043908116640523076
対照学習損失: 0.011654510162770748
対照学習損失: 5.674761996488087e-05
対照学習損失: 0.0018203327199444175
対照学習損失: 0.0002430574968457222
対照学習損失: 0.002717193216085434
対照学習損失: 3.6805736272071954e-06
対照学習損失: 2.0473627955652773e-05
対照学習損失: 3.1469146051676944e-05
対照学習損失: 0.001105578034184873
対照学習損失: 7.337968418141827e-05
対照学習損失: 4.483361408347264e-05
対照学習損失: 7.197218110377435e-06
対照学習損失: 8.297911699628457e-05
対照学習損失: 2.2008376618032344e-05
対照学習損失: 2.349801070522517e-05
対照学習損失: 4.142317993682809e-05
対照学習損失: 0.00015790386532898992
対照学習損失: 1.1280080798314884e-05
対照学習損失: 8.375100878765807e-05
対照学習損失: 8.360239007743075e-05
対照学習損失: 8.180611985153519e-06
対照学習損失: 3.933896550734062e-06
対照学習損失: 5.824138497700915e-05
対照学習損失: 8.923394489102066e-05
対照学習損失: 0.00040314183570444584
対照学習損失: 8.4785988292424e-06
対照学習損失: 1.8267854102305137e-05
対照学習損失: 0.0005649428931064904
対照学習損失: 2.553896229073871e-05
対照学習損失: 4.379196252557449e-05
対照学習損失: 2.129320455424022e-05
対照学習損失: 0.005169996060431004
対照学習損失: 4.515031832852401e-06
対照学習損失: 2.4243126972578466e-05
対照学習損失: 3.586517050280236e-05
対照学習損失: 0.00010763336467789486
対照学習損失: 3.1246119760908186e-05
対照学習損失: 4.4222128053661436e-05
対照学習損失: 0.0002267735981149599
対照学習損失: 3.993483460362768e-06
対照学習損失: 1.78211739694234e-05
対照学習損失: 2.838429281837307e-05
対照学習損失: 5.8182482462143525e-05
対照学習損失: 1.2263471944606863e-05
対照学習損失: 0.00041034939931705594
対照学習損失: 9.50528628891334e-05
対照学習損失: 9.832350770011544e-05
対照学習損失: 7.567353895865381e-05
対照学習損失: 5.915725978411501e-06
対照学習損失: 2.0548222892102785e-05
対照学習損失: 0.0007143650436773896
対照学習損失: 4.3270018068142235e-05
対照学習損失: 3.745726644410752e-05
対照学習損失: 4.894216908724047e-05
対照学習損失: 5.9604244597721845e-06
対照学習損失: 4.9620630306890234e-06
対照学習損失: 5.019772288505919e-05
対照学習損失: 1.7940621546586044e-05
対照学習損失: 0.0002671027323231101
対照学習損失: 7.3908199738070834e-06
対照学習損失: 2.4182974811992608e-05
対照学習損失: 0.00036435312358662486
対照学習損失: 1.1518350220285356e-05
対照学習損失: 2.4421789930784144e-05
対照学習損失: 4.2701507481979206e-05
対照学習損失: 6.139243851066567e-06
対照学習損失: 4.315041587688029e-05
対照学習損失: 2.533123188186437e-05
対照学習損失: 0.00048442831030115485
対照学習損失: 0.0006457541021518409
対照学習損失: 0.00010466710227774456
対照学習損失: 4.991863534087315e-06
対照学習損失: 2.00864360522246e-05
対照学習損失: 3.3823253033915535e-05
対照学習損失: 3.2480784284416586e-05
対照学習損失: 1.4468755580310244e-05
対照学習損失: 1.3961982403998263e-05
対照学習損失: 0.00010830234532477334
対照学習損失: 5.930629413342103e-06
対照学習損失: 0.0001892554573714733
対照学習損失: 0.00017305929213762283
対照学習損失: 0.00011735723819583654
対照学習損失: 1.9907471141777933e-05
対照学習損失: 0.0003076537686865777
対照学習損失: 3.5017642403545324e-06
対照学習損失: 1.6629306628601626e-05
対照学習損失: 2.8610140816454077e-06
対照学習損失: 0.004252253565937281
対照学習損失: 7.158757216529921e-05
対照学習損失: 2.4734166800044477e-05
対照学習損失: 0.0024184926878660917
対照学習損失: 2.9501836252165958e-05
対照学習損失: 1.8059763533528894e-05
対照学習損失: 0.00011753800208680332
対照学習損失: 2.7119225705973804e-05
対照学習損失: 6.7948644755233545e-06
対照学習損失: 3.258635842939839e-05
対照学習損失: 0.00025349317002110183
対照学習損失: 2.813228638842702e-05
対照学習損失: 2.2216940124053508e-05
対照学習損失: 1.2308129953453317e-05
対照学習損失: 6.662168743787333e-05
対照学習損失: 5.371417501010001e-05
対照学習損失: 2.7759751901612617e-05
対照学習損失: 5.2223880629753694e-05
対照学習損失: 0.006997936405241489
対照学習損失: 4.328190698288381e-05
対照学習損失: 3.671488957479596e-05
対照学習損失: 0.0002764614764600992
対照学習損失: 7.584635568491649e-06
対照学習損失: 1.7269783711526543e-05
対照学習損失: 8.970408998720814e-06
対照学習損失: 1.1563171028683428e-05
対照学習損失: 4.3298787204548717e-05
対照学習損失: 3.541738260537386e-05
対照学習損失: 0.03978428989648819
対照学習損失: 4.554925544653088e-05
対照学習損失: 1.7880829545902088e-05
対照学習損失: 3.601442585932091e-05
対照学習損失: 0.00012113816774217412
対照学習損失: 1.2978774066141341e-05
対照学習損失: 0.0001246100728167221
対照学習損失: 1.6674068319844082e-05
対照学習損失: 0.0001159370003733784
対照学習損失: 6.69055589241907e-06
対照学習損失: 0.0008966017630882561
対照学習損失: 5.990233603370143e-06
対照学習損失: 6.64585923004779e-06
対照学習損失: 2.7506262995302677e-05
対照学習損失: 0.000427692080847919
対照学習損失: 0.00012104450433980674
対照学習損失: 0.00039626419311389327
対照学習損失: 4.1497954953229055e-05
対照学習損失: 1.853641697380226e-05
対照学習損失: 5.204498302191496e-05
対照学習損失: 7.833553536329418e-05
対照学習損失: 5.779332059319131e-05
対照学習損失: 2.041360130533576e-05
対照学習損失: 3.206474502803758e-05
対照学習損失: 0.0034053921699523926
対照学習損失: 8.036685903789476e-05
対照学習損失: 0.00017524423310533166
対照学習損失: 0.00019613062613643706
対照学習損失: 0.003802115563303232
対照学習損失: 0.0008879495435394347
対照学習損失: 0.0010752679081633687
対照学習損失: 2.3364231310551986e-05
対照学習損失: 8.180685654224362e-06
対照学習損失: 2.6195044483756647e-05
対照学習損失: 1.3380844393395819e-05
対照学習損失: 0.0002725611557252705
対照学習損失: 0.02341809868812561
対照学習損失: 0.0028196226339787245
対照学習損失: 2.1784062482765876e-05
対照学習損失: 3.7103764043422416e-06
対照学習損失: 6.2286526372190565e-06
対照学習損失: 3.30034781654831e-05
対照学習損失: 0.00023556708765681833
対照学習損失: 7.32205153326504e-05
対照学習損失: 0.00012727089051622897
対照学習損失: 2.840039633156266e-05
対照学習損失: 1.2695447367150337e-05
対照学習損失: 2.9487173378583975e-05
対照学習損失: 6.899305299157277e-05
対照学習損失: 0.00037749047623947263
対照学習損失: 1.0341273991798516e-05
対照学習損失: 5.036571565142367e-06
対照学習損失: 6.360073894029483e-05
対照学習損失: 0.00043090261169709265
対照学習損失: 7.584640570712509e-06
対照学習損失: 0.0010551479645073414
対照学習損失: 9.432351362193003e-06
対照学習損失: 0.0008767028339207172
対照学習損失: 3.343460411997512e-05
対照学習損失: 0.00032726203789934516
対照学習損失: 8.478640665998682e-06
対照学習損失: 3.844479579129256e-06
対照学習損失: 2.5763274607015774e-05
対照学習損失: 0.004533702507615089
対照学習損失: 0.000204818497877568
対照学習損失: 2.160608346457593e-05
対照学習損失: 0.00013949564890936017
対照学習損失: 3.9338938222499564e-06
対照学習損失: 6.020033652021084e-06
対照学習損失: 4.917360001854831e-06
対照学習損失: 0.000265523383859545
対照学習損失: 1.443891778762918e-05
対照学習損失: 4.7308352804975584e-05
対照学習損失: 0.00015048569184727967
対照学習損失: 2.302147913724184e-05
対照学習損失: 6.1392097450152505e-06
対照学習損失: 0.0013005518121644855
対照学習損失: 3.924686461687088e-05
対照学習損失: 3.181168722221628e-05
対照学習損失: 7.077838381519541e-05
対照学習損失: 4.380922291602474e-06
対照学習損失: 3.784885393542936e-06
対照学習損失: 3.918990842066705e-06
対照学習損失: 7.422463386319578e-05
対照学習損失: 0.00021802607807330787
対照学習損失: 1.461783904233016e-05
対照学習損失: 5.4537958931177855e-06
対照学習損失: 1.2874430467491038e-05
対照学習損失: 0.00015484170580748469
対照学習損失: 1.6778441931819543e-05
対照学習損失: 1.2680734471359756e-05
対照学習損失: 8.091435302048922e-05
対照学習損失: 0.0015109074302017689
対照学習損失: 5.268487075227313e-05
対照学習損失: 5.261031037662178e-05
対照学習損失: 2.607698661449831e-06
対照学習損失: 0.0004723913152702153
対照学習損失: 1.7121001292252913e-05
対照学習損失: 8.836317647364922e-06
対照学習損失: 6.0349470913934056e-06
対照学習損失: 9.611128007236402e-06
対照学習損失: 9.109058737521991e-05
対照学習損失: 4.8603695177007467e-05
対照学習損失: 4.589532636600779e-06
対照学習損失: 3.16769364872016e-05
対照学習損失: 0.00224508554674685
対照学習損失: 8.612744750280399e-06
対照学習損失: 1.250194964086404e-05
対照学習損失: 0.00014838867355138063
対照学習損失: 3.3676542443572544e-06
対照学習損失: 0.00014282559277489781
対照学習損失: 1.3962118828203529e-05
対照学習損失: 0.005276840180158615
対照学習損失: 2.1054980607004836e-05
対照学習損失: 0.0004411384288687259
対照学習損失: 1.023699496727204e-05
対照学習損失: 1.2934066035086289e-05
対照学習損失: 1.075851105269976e-05
対照学習損失: 4.142211764701642e-05
対照学習損失: 7.14464986231178e-05
対照学習損失: 3.112680133199319e-05
対照学習損失: 4.634248398360796e-06
対照学習損失: 2.2410520614357665e-05
対照学習損失: 0.0014200852019712329
対照学習損失: 1.7910744645632803e-05
対照学習損失: 0.00043313344940543175
対照学習損失: 0.00020929882884956896
対照学習損失: 0.0036500694695860147
対照学習損失: 1.58097409439506e-05
対照学習損失: 5.418881482910365e-05
対照学習損失: 4.210685801808722e-05
対照学習損失: 8.925719157559797e-06
対照学習損失: 5.572652298724279e-05
対照学習損失: 2.8460126486606896e-05
対照学習損失: 6.524167110910639e-05
対照学習損失: 1.1190462828380987e-05
対照学習損失: 1.9222128685214557e-05
対照学習損失: 9.901975136017427e-05
対照学習損失: 0.0013016511220484972
対照学習損失: 1.083302777260542e-05
対照学習損失: 1.0236992238787934e-05
対照学習損失: 5.3530093282461166e-05
対照学習損失: 0.16599290072917938
対照学習損失: 1.9638717276393436e-05
対照学習損失: 3.429947901167907e-05
対照学習損失: 6.198847131599905e-06
対照学習損失: 0.00012116409197915345
対照学習損失: 1.187610632769065e-05
対照学習損失: 2.95328936772421e-05
対照学習損失: 3.097833177889697e-05
対照学習損失: 4.142507350479718e-06
対照学習損失: 0.0019602838438004255
対照学習損失: 0.0010070273419842124
対照学習損失: 0.012979721650481224
対照学習損失: 8.34456477605272e-06
対照学習損失: 5.201389649300836e-05
対照学習損失: 3.8769350794609636e-05
対照学習損失: 9.683999815024436e-05
対照学習損失: 1.068394794856431e-05
対照学習損失: 6.247606506804004e-05
対照学習損失: 0.00010500581993255764
対照学習損失: 0.00013443888747133315
対照学習損失: 6.437227966671344e-06
対照学習損失: 0.002179396338760853
対照学習損失: 5.702986163669266e-05
対照学習損失: 1.582468394190073e-05
対照学習損失: 0.024219278246164322
対照学習損失: 3.591024869820103e-05
対照学習損失: 2.232101178378798e-05
対照学習損失: 1.2457228876883164e-05
対照学習損失: 6.899162599438569e-06
対照学習損失: 9.536642028251663e-05
対照学習損失: 6.62588863633573e-05
対照学習損失: 3.893279790645465e-05
対照学習損失: 2.1888921764912084e-05
対照学習損失: 8.198035357054323e-05
対照学習損失: 7.669020851608366e-05
対照学習損失: 4.570961755234748e-05
対照学習損失: 0.00010135144839296117
対照学習損失: 0.0005626673810184002
対照学習損失: 3.362866482348181e-05
対照学習損失: 2.409320222795941e-05
対照学習損失: 1.1846182133012917e-05
対照学習損失: 5.4295473091769964e-05
対照学習損失: 3.591165295802057e-06
対照学習損失: 6.211370055098087e-05
対照学習損失: 7.168010779423639e-05
対照学習損失: 0.00029975935467518866
対照学習損失: 8.746813364268746e-06
対照学習損失: 0.0007192820776253939
対照学習損失: 3.744412970263511e-05
対照学習損失: 1.4528412066283636e-05
対照学習損失: 0.0009898364078253508
対照学習損失: 7.850797555875033e-05
対照学習損失: 5.3222174756228924e-05
対照学習損失: 1.1369470485078637e-05
対照学習損失: 0.0004064051026944071
対照学習損失: 9.826752648223191e-05
対照学習損失: 0.00019479442562442273
対照学習損失: 7.785767957102507e-05
対照学習損失: 5.066372068540659e-06
対照学習損失: 2.2901345801074058e-05
対照学習損失: 4.511923543759622e-05
対照学習損失: 1.9117371266474947e-05
対照学習損失: 0.00012878891720902175
対照学習損失: 1.794058880477678e-05
対照学習損失: 1.6822979887365364e-05
対照学習損失: 0.0007791564567014575
対照学習損失: 1.2099454579583835e-05
対照学習損失: 3.537427983246744e-05
対照学習損失: 0.000274756079306826
対照学習損失: 1.7180602299049497e-05
対照学習損失: 0.0002865527058020234
対照学習損失: 3.8681551814079285e-05
対照学習損失: 0.00030058209085837007
対照学習損失: 1.749359944369644e-05
対照学習損失: 2.960651363537181e-05
対照学習損失: 3.9023547287797555e-05
対照学習損失: 1.1399166396586224e-05
対照学習損失: 8.895884093362838e-06
対照学習損失: 7.389952952507883e-05
対照学習損失: 0.00138301239348948
対照学習損失: 7.692779036005959e-05
対照学習損失: 5.7369115893379785e-06
対照学習損失: 0.0002545595052652061
対照学習損失: 2.077188401017338e-05
対照学習損失: 3.8129292079247534e-05
対照学習損失: 6.017808482283726e-05
対照学習損失: 0.0007988564902916551
対照学習損失: 9.935494017554447e-05
対照学習損失: 7.408323290292174e-05
対照学習損失: 4.265977622708306e-05
対照学習損失: 2.409440276096575e-05
対照学習損失: 5.977680120849982e-05
対照学習損失: 0.00027649544063024223
対照学習損失: 0.0005401765229180455
対照学習損失: 0.0036323408130556345
対照学習損失: 2.3453332687495276e-05
対照学習損失: 0.00016930000856518745
対照学習損失: 5.544334271689877e-05
対照学習損失: 3.4701912227319553e-05
対照学習損失: 1.3530006071960088e-05
対照学習損失: 0.00017345734522677958
対照学習損失: 0.00787865836173296
対照学習損失: 1.4468551853497047e-05
対照学習損失: 0.00015439461276400834
対照学習損失: 0.0001242457947228104
対照学習損失: 1.3261847925605252e-05
対照学習損失: 5.602800229098648e-06
対照学習損失: 7.931095751700923e-05
対照学習損失: 8.433925358986016e-06
対照学習損失: 2.7372441763873212e-05
対照学習損失: 1.7106109226006083e-05
対照学習損失: 3.704028495121747e-05
対照学習損失: 8.572338265366852e-05
対照学習損失: 0.0017825411632657051
対照学習損失: 0.00017434346955269575
対照学習損失: 4.234713924233802e-05
対照学習損失: 3.442156639721361e-06
対照学習損失: 9.477033017901704e-06
対照学習損失: 5.140859684615862e-06
対照学習損失: 0.001983013702556491
対照学習損失: 1.2889289791928604e-05
対照学習損失: 1.0654061952664051e-05
対照学習損失: 0.0003005744074471295
対照学習損失: 7.167406693042722e-06
対照学習損失: 1.0400897735962644e-05
対照学習損失: 2.086068707285449e-05
対照学習損失: 1.6391247754654614e-06
{'eval_loss': 0.001235751318745315, 'eval_runtime': 56.5109, 'eval_samples_per_second': 68.589, 'eval_steps_per_second': 8.582, 'epoch': 1.0}
対照学習損失: 0.00024951284285634756
{'loss': 0.0001, 'grad_norm': 0.010690014809370041, 'learning_rate': 3.935018050541516e-05, 'epoch': 1.0}
対照学習損失: 3.31906157953199e-05
対照学習損失: 0.0004720651777461171
{'loss': 0.0003, 'grad_norm': 0.012355344370007515, 'learning_rate': 3.933212996389892e-05, 'epoch': 1.0}
対照学習損失: 2.7715295800589956e-05
対照学習損失: 0.00011405207624193281
{'loss': 0.0001, 'grad_norm': 0.004709614906460047, 'learning_rate': 3.931407942238267e-05, 'epoch': 1.0}
対照学習損失: 6.940116873010993e-05
対照学習損失: 4.97046930831857e-05
{'loss': 0.0001, 'grad_norm': 0.0018140948377549648, 'learning_rate': 3.9296028880866426e-05, 'epoch': 1.0}
対照学習損失: 0.0013061307836323977
対照学習損失: 0.00017788910190574825
{'loss': 0.0007, 'grad_norm': 0.037897106260061264, 'learning_rate': 3.927797833935018e-05, 'epoch': 1.0}
対照学習損失: 0.0006367092719301581
対照学習損失: 0.0009276937344111502
{'loss': 0.0008, 'grad_norm': 0.03994491323828697, 'learning_rate': 3.9259927797833933e-05, 'epoch': 1.01}
対照学習損失: 0.00048084522131830454
対照学習損失: 8.697842713445425e-05
{'loss': 0.0003, 'grad_norm': 0.015163381583988667, 'learning_rate': 3.924187725631769e-05, 'epoch': 1.01}
対照学習損失: 2.8207026844029315e-05
対照学習損失: 0.0002549045893829316
{'loss': 0.0001, 'grad_norm': 0.010607444681227207, 'learning_rate': 3.922382671480145e-05, 'epoch': 1.01}
対照学習損失: 0.0004508441488724202
対照学習損失: 2.2707899915985763e-05
{'loss': 0.0002, 'grad_norm': 0.026118023321032524, 'learning_rate': 3.9205776173285205e-05, 'epoch': 1.01}
対照学習損失: 0.00012713277828879654
対照学習損失: 2.6484314730623737e-05
{'loss': 0.0001, 'grad_norm': 0.003754837205633521, 'learning_rate': 3.9187725631768956e-05, 'epoch': 1.01}
対照学習損失: 0.0023094099014997482
対照学習損失: 7.83241557655856e-05
{'loss': 0.0012, 'grad_norm': 0.0724840983748436, 'learning_rate': 3.916967509025271e-05, 'epoch': 1.01}
対照学習損失: 0.000811411184258759
対照学習損失: 5.852692265762016e-05
{'loss': 0.0004, 'grad_norm': 0.0218101367354393, 'learning_rate': 3.915162454873646e-05, 'epoch': 1.01}
対照学習損失: 0.00033343900577165186
対照学習損失: 2.114374001394026e-05
{'loss': 0.0002, 'grad_norm': 0.007360678631812334, 'learning_rate': 3.913357400722022e-05, 'epoch': 1.01}
対照学習損失: 1.2695652912952937e-05
対照学習損失: 0.001412011100910604
{'loss': 0.0007, 'grad_norm': 0.10088463872671127, 'learning_rate': 3.911552346570397e-05, 'epoch': 1.01}
対照学習損失: 0.00011089950567111373
対照学習損失: 9.186934039462358e-05
{'loss': 0.0001, 'grad_norm': 0.004291641991585493, 'learning_rate': 3.909747292418773e-05, 'epoch': 1.01}
対照学習損失: 6.194568413775414e-05
対照学習損失: 9.551606490276754e-05
{'loss': 0.0001, 'grad_norm': 0.002301827073097229, 'learning_rate': 3.9079422382671485e-05, 'epoch': 1.01}
対照学習損失: 5.61610322620254e-05
対照学習損失: 5.963349758530967e-05
{'loss': 0.0001, 'grad_norm': 0.002145268488675356, 'learning_rate': 3.9061371841155236e-05, 'epoch': 1.02}
対照学習損失: 0.00010223004210274667
対照学習損失: 0.00010437080345582217
{'loss': 0.0001, 'grad_norm': 0.0031200279481709003, 'learning_rate': 3.904332129963899e-05, 'epoch': 1.02}
対照学習損失: 2.2366064513335004e-05
対照学習損失: 0.0001643684081500396
{'loss': 0.0001, 'grad_norm': 0.0056260316632688046, 'learning_rate': 3.902527075812274e-05, 'epoch': 1.02}
対照学習損失: 0.0013999557122588158
対照学習損失: 1.4408811694011092e-05
{'loss': 0.0007, 'grad_norm': 0.02734631858766079, 'learning_rate': 3.90072202166065e-05, 'epoch': 1.02}
対照学習損失: 1.838747994042933e-05
対照学習損失: 0.00032934837508946657
{'loss': 0.0002, 'grad_norm': 0.008144362829625607, 'learning_rate': 3.898916967509025e-05, 'epoch': 1.02}
対照学習損失: 3.633914820966311e-05
対照学習損失: 2.0943101844750345e-05
{'loss': 0.0, 'grad_norm': 0.0009670139406807721, 'learning_rate': 3.897111913357401e-05, 'epoch': 1.02}
対照学習損失: 0.005558386445045471
対照学習損失: 5.050892650615424e-05
{'loss': 0.0028, 'grad_norm': 0.2093859165906906, 'learning_rate': 3.8953068592057765e-05, 'epoch': 1.02}
対照学習損失: 1.8842085410142317e-05
対照学習損失: 1.3671579836227465e-05
{'loss': 0.0, 'grad_norm': 0.00031999769271351397, 'learning_rate': 3.8935018050541516e-05, 'epoch': 1.02}
対照学習損失: 6.801191921113059e-05
対照学習損失: 1.396218067384325e-05
{'loss': 0.0, 'grad_norm': 0.0017622041050344706, 'learning_rate': 3.891696750902527e-05, 'epoch': 1.02}
対照学習損失: 0.00031999588827602565
対照学習損失: 6.250508886296302e-05
{'loss': 0.0002, 'grad_norm': 0.005885421298444271, 'learning_rate': 3.889891696750902e-05, 'epoch': 1.02}
対照学習損失: 6.55979456496425e-05
対照学習損失: 0.00012518528092186898
{'loss': 0.0001, 'grad_norm': 0.004233240615576506, 'learning_rate': 3.888086642599278e-05, 'epoch': 1.02}
対照学習損失: 0.00011917616939172149
対照学習損失: 7.506059773731977e-05
{'loss': 0.0001, 'grad_norm': 0.004677977878600359, 'learning_rate': 3.886281588447654e-05, 'epoch': 1.03}
対照学習損失: 0.00023065773712005466
対照学習損失: 1.528810571471695e-05
{'loss': 0.0001, 'grad_norm': 0.005587927997112274, 'learning_rate': 3.8844765342960295e-05, 'epoch': 1.03}
対照学習損失: 0.00012248985876794904
対照学習損失: 5.4912892665015534e-05
{'loss': 0.0001, 'grad_norm': 0.0034884095657616854, 'learning_rate': 3.8826714801444045e-05, 'epoch': 1.03}
対照学習損失: 9.779764513950795e-05
対照学習損失: 2.6522204279899597e-05
{'loss': 0.0001, 'grad_norm': 0.0021877018734812737, 'learning_rate': 3.88086642599278e-05, 'epoch': 1.03}
対照学習損失: 3.770374314626679e-05
対照学習損失: 7.410075340885669e-05
{'loss': 0.0001, 'grad_norm': 0.0020849579013884068, 'learning_rate': 3.879061371841155e-05, 'epoch': 1.03}
対照学習損失: 3.78606200683862e-05
対照学習損失: 1.4573104635928757e-05
{'loss': 0.0, 'grad_norm': 0.0007990613230504096, 'learning_rate': 3.877256317689531e-05, 'epoch': 1.03}
対照学習損失: 4.498865382629447e-05
対照学習損失: 5.708062963094562e-05
{'loss': 0.0001, 'grad_norm': 0.0018014449160546064, 'learning_rate': 3.875451263537907e-05, 'epoch': 1.03}
対照学習損失: 0.00023967624292708933
対照学習損失: 1.329907445324352e-05
{'loss': 0.0001, 'grad_norm': 0.011982811614871025, 'learning_rate': 3.873646209386282e-05, 'epoch': 1.03}
対照学習損失: 0.00037912323023192585
対照学習損失: 0.0005680322647094727
{'loss': 0.0005, 'grad_norm': 0.028521977365016937, 'learning_rate': 3.8718411552346575e-05, 'epoch': 1.03}
対照学習損失: 0.00022359902504831553
対照学習損失: 0.00011822498345281929
{'loss': 0.0002, 'grad_norm': 0.010056932456791401, 'learning_rate': 3.8700361010830326e-05, 'epoch': 1.03}
対照学習損失: 4.823585186386481e-05
対照学習損失: 2.5107729015871882e-05
{'loss': 0.0, 'grad_norm': 0.0011351597495377064, 'learning_rate': 3.868231046931408e-05, 'epoch': 1.03}
対照学習損失: 8.935184450820088e-05
対照学習損失: 1.4565647688868921e-05
{'loss': 0.0001, 'grad_norm': 0.0018600489711388946, 'learning_rate': 3.866425992779783e-05, 'epoch': 1.04}
対照学習損失: 0.06921547651290894
対照学習損失: 0.00020482423133216798
{'loss': 0.0347, 'grad_norm': 1.2479060888290405, 'learning_rate': 3.864620938628159e-05, 'epoch': 1.04}
対照学習損失: 4.6227974962675944e-05
対照学習損失: 0.00017468465375714004
{'loss': 0.0001, 'grad_norm': 0.004988743923604488, 'learning_rate': 3.862815884476535e-05, 'epoch': 1.04}
対照学習損失: 0.00015382908168248832
対照学習損失: 3.0278102713054977e-05
{'loss': 0.0001, 'grad_norm': 0.004911331459879875, 'learning_rate': 3.86101083032491e-05, 'epoch': 1.04}
対照学習損失: 0.00012398083345033228
対照学習損失: 4.965417610947043e-05
{'loss': 0.0001, 'grad_norm': 0.006028993986546993, 'learning_rate': 3.8592057761732855e-05, 'epoch': 1.04}
対照学習損失: 0.0001999932574108243
対照学習損失: 0.00011692617408698425
{'loss': 0.0002, 'grad_norm': 0.004435597453266382, 'learning_rate': 3.8574007220216606e-05, 'epoch': 1.04}
対照学習損失: 9.678264905232936e-05
対照学習損失: 0.00024110529921017587
{'loss': 0.0002, 'grad_norm': 0.007057818118482828, 'learning_rate': 3.855595667870036e-05, 'epoch': 1.04}
対照学習損失: 0.00019838081789202988
対照学習損失: 0.00032011058647185564
{'loss': 0.0003, 'grad_norm': 0.012350406497716904, 'learning_rate': 3.853790613718411e-05, 'epoch': 1.04}
対照学習損失: 0.00023479991068597883
対照学習損失: 8.36880280985497e-05
{'loss': 0.0002, 'grad_norm': 0.0066469390876591206, 'learning_rate': 3.851985559566787e-05, 'epoch': 1.04}
対照学習損失: 4.8677211452741176e-05
対照学習損失: 9.563114144839346e-05
{'loss': 0.0001, 'grad_norm': 0.0028989766724407673, 'learning_rate': 3.850180505415162e-05, 'epoch': 1.04}
対照学習損失: 0.00010111716255778447
対照学習損失: 0.00015372407506220043
{'loss': 0.0001, 'grad_norm': 0.003879083786159754, 'learning_rate': 3.848375451263538e-05, 'epoch': 1.04}
対照学習損失: 2.9532717235269956e-05
対照学習損失: 0.00011421424278523773
{'loss': 0.0001, 'grad_norm': 0.004774548579007387, 'learning_rate': 3.8465703971119135e-05, 'epoch': 1.05}
対照学習損失: 9.62490084930323e-05
対照学習損失: 1.9885224901372567e-05
{'loss': 0.0001, 'grad_norm': 0.002305265748873353, 'learning_rate': 3.844765342960289e-05, 'epoch': 1.05}
対照学習損失: 0.0024151622783392668
対照学習損失: 0.0006198844057507813
{'loss': 0.0015, 'grad_norm': 0.24084624648094177, 'learning_rate': 3.842960288808665e-05, 'epoch': 1.05}
対照学習損失: 3.115704021183774e-05
対照学習損失: 0.0020152900833636522
{'loss': 0.001, 'grad_norm': 0.06947353482246399, 'learning_rate': 3.84115523465704e-05, 'epoch': 1.05}
対照学習損失: 5.489283648785204e-05
対照学習損失: 0.00023382830841001123
{'loss': 0.0001, 'grad_norm': 0.011988085694611073, 'learning_rate': 3.839350180505416e-05, 'epoch': 1.05}
対照学習損失: 0.00022651167819276452
対照学習損失: 0.00013557658530771732
{'loss': 0.0002, 'grad_norm': 0.006073629483580589, 'learning_rate': 3.837545126353791e-05, 'epoch': 1.05}
対照学習損失: 0.000521823822055012
対照学習損失: 8.808105485513806e-05
{'loss': 0.0003, 'grad_norm': 0.015102140605449677, 'learning_rate': 3.8357400722021665e-05, 'epoch': 1.05}
対照学習損失: 6.020288492436521e-05
対照学習損失: 0.00012069247895851731
{'loss': 0.0001, 'grad_norm': 0.0024679102934896946, 'learning_rate': 3.8339350180505415e-05, 'epoch': 1.05}
対照学習損失: 0.00029974611243233085
対照学習損失: 0.00011842267122119665
{'loss': 0.0002, 'grad_norm': 0.010615761391818523, 'learning_rate': 3.832129963898917e-05, 'epoch': 1.05}
対照学習損失: 0.00011669432569760829
対照学習損失: 2.104733357555233e-05
{'loss': 0.0001, 'grad_norm': 0.003564953338354826, 'learning_rate': 3.830324909747292e-05, 'epoch': 1.05}
対照学習損失: 8.804050594335422e-05
対照学習損失: 1.0035751984105445e-05
{'loss': 0.0, 'grad_norm': 0.0016995652113109827, 'learning_rate': 3.828519855595668e-05, 'epoch': 1.05}
対照学習損失: 0.0002915777440648526
対照学習損失: 9.155418956652284e-05
{'loss': 0.0002, 'grad_norm': 0.012883120216429234, 'learning_rate': 3.826714801444044e-05, 'epoch': 1.06}
対照学習損失: 5.658771988237277e-05
対照学習損失: 0.0001406259398208931
{'loss': 0.0001, 'grad_norm': 0.0033959979191422462, 'learning_rate': 3.824909747292419e-05, 'epoch': 1.06}
対照学習損失: 0.0002560634457040578
対照学習損失: 0.00047527969582006335
{'loss': 0.0004, 'grad_norm': 0.01217557117342949, 'learning_rate': 3.8231046931407945e-05, 'epoch': 1.06}
対照学習損失: 0.00020182106527499855
対照学習損失: 8.480610267724842e-05
{'loss': 0.0001, 'grad_norm': 0.016253286972641945, 'learning_rate': 3.8212996389891696e-05, 'epoch': 1.06}
対照学習損失: 5.501887426362373e-05
対照学習損失: 0.00017125005251727998
{'loss': 0.0001, 'grad_norm': 0.0033287564292550087, 'learning_rate': 3.819494584837545e-05, 'epoch': 1.06}
対照学習損失: 0.0001347613288089633
対照学習損失: 0.00029727997025474906
{'loss': 0.0002, 'grad_norm': 0.006875185761600733, 'learning_rate': 3.81768953068592e-05, 'epoch': 1.06}
対照学習損失: 2.97113620035816e-05
対照学習損失: 0.0004659234546124935
{'loss': 0.0002, 'grad_norm': 0.01630857214331627, 'learning_rate': 3.815884476534296e-05, 'epoch': 1.06}
対照学習損失: 3.629393904702738e-05
対照学習損失: 2.343894993828144e-05
{'loss': 0.0, 'grad_norm': 0.0010151584865525365, 'learning_rate': 3.814079422382672e-05, 'epoch': 1.06}
対照学習損失: 4.6276094508357346e-05
対照学習損失: 4.638320388039574e-05
{'loss': 0.0, 'grad_norm': 0.001955127576366067, 'learning_rate': 3.812274368231047e-05, 'epoch': 1.06}
対照学習損失: 2.813189166772645e-05
対照学習損失: 7.638837269041687e-05
{'loss': 0.0001, 'grad_norm': 0.002469303784891963, 'learning_rate': 3.8104693140794225e-05, 'epoch': 1.06}
対照学習損失: 0.002421535085886717
対照学習損失: 0.00019201963732484728
{'loss': 0.0013, 'grad_norm': 0.08504994213581085, 'learning_rate': 3.8086642599277976e-05, 'epoch': 1.06}
対照学習損失: 0.00012420720304362476
対照学習損失: 5.388346471590921e-05
{'loss': 0.0001, 'grad_norm': 0.006593326572328806, 'learning_rate': 3.806859205776174e-05, 'epoch': 1.07}
対照学習損失: 1.5451736544491723e-05
対照学習損失: 2.980728095280938e-05
{'loss': 0.0, 'grad_norm': 0.0008512982167303562, 'learning_rate': 3.805054151624549e-05, 'epoch': 1.07}
対照学習損失: 0.0018576453439891338
対照学習損失: 8.791315485723317e-05
{'loss': 0.001, 'grad_norm': 0.0892481580376625, 'learning_rate': 3.803249097472925e-05, 'epoch': 1.07}
対照学習損失: 2.968210174003616e-05
対照学習損失: 0.00047072843881323934
{'loss': 0.0003, 'grad_norm': 0.01082936953753233, 'learning_rate': 3.8014440433213e-05, 'epoch': 1.07}
対照学習損失: 0.0001944848772836849
対照学習損失: 5.383461757446639e-05
{'loss': 0.0001, 'grad_norm': 0.004881064873188734, 'learning_rate': 3.7996389891696755e-05, 'epoch': 1.07}
対照学習損失: 2.3706892534391955e-05
対照学習損失: 4.163734774920158e-05
{'loss': 0.0, 'grad_norm': 0.0009618903277441859, 'learning_rate': 3.7978339350180505e-05, 'epoch': 1.07}
対照学習損失: 1.996643732127268e-05
対照学習損失: 1.2181546480860561e-05
{'loss': 0.0, 'grad_norm': 0.00042044135625474155, 'learning_rate': 3.796028880866426e-05, 'epoch': 1.07}
対照学習損失: 0.0008465932332910597
対照学習損失: 7.917080074548721e-05
{'loss': 0.0005, 'grad_norm': 0.03148947283625603, 'learning_rate': 3.794223826714802e-05, 'epoch': 1.07}
対照学習損失: 9.529141607345082e-06
対照学習損失: 0.0010808054357767105
{'loss': 0.0005, 'grad_norm': 0.05405230075120926, 'learning_rate': 3.792418772563177e-05, 'epoch': 1.07}
対照学習損失: 4.6155335439834744e-05
対照学習損失: 5.5793701903894544e-05
{'loss': 0.0001, 'grad_norm': 0.0017195228720083833, 'learning_rate': 3.790613718411553e-05, 'epoch': 1.07}
対照学習損失: 2.8929363907082006e-05
対照学習損失: 0.00016099447384476662
{'loss': 0.0001, 'grad_norm': 0.0036500387359410524, 'learning_rate': 3.788808664259928e-05, 'epoch': 1.07}
対照学習損失: 0.00010153707262361422
対照学習損失: 0.00010977555939462036
{'loss': 0.0001, 'grad_norm': 0.003940893337130547, 'learning_rate': 3.7870036101083035e-05, 'epoch': 1.08}
対照学習損失: 7.582160469610244e-05
対照学習損失: 0.00016131023585330695
{'loss': 0.0001, 'grad_norm': 0.0043028597719967365, 'learning_rate': 3.7851985559566785e-05, 'epoch': 1.08}
対照学習損失: 5.0449802074581385e-05
対照学習損失: 4.1951119783334434e-05
{'loss': 0.0, 'grad_norm': 0.0017770575359463692, 'learning_rate': 3.783393501805054e-05, 'epoch': 1.08}
対照学習損失: 2.764693090284709e-05
対照学習損失: 3.8196925743250176e-05
{'loss': 0.0, 'grad_norm': 0.0009732498438097537, 'learning_rate': 3.78158844765343e-05, 'epoch': 1.08}
対照学習損失: 0.0025796520058065653
対照学習損失: 7.948777783894911e-05
{'loss': 0.0013, 'grad_norm': 0.17087122797966003, 'learning_rate': 3.779783393501805e-05, 'epoch': 1.08}
対照学習損失: 4.542100941762328e-05
対照学習損失: 0.00013147648132871836
{'loss': 0.0001, 'grad_norm': 0.005245610605925322, 'learning_rate': 3.777978339350181e-05, 'epoch': 1.08}
対照学習損失: 4.854946382693015e-05
対照学習損失: 4.603478737408295e-05
{'loss': 0.0, 'grad_norm': 0.00126678089145571, 'learning_rate': 3.776173285198556e-05, 'epoch': 1.08}
対照学習損失: 1.4148317859508097e-05
対照学習損失: 6.338683306239545e-05
{'loss': 0.0, 'grad_norm': 0.0013349248329177499, 'learning_rate': 3.7743682310469315e-05, 'epoch': 1.08}
対照学習損失: 1.1980298950220458e-05
対照学習損失: 0.0005504474975168705
{'loss': 0.0003, 'grad_norm': 0.013552035205066204, 'learning_rate': 3.7725631768953066e-05, 'epoch': 1.08}
対照学習損失: 0.0001362527400488034
対照学習損失: 8.387714478885755e-05
{'loss': 0.0001, 'grad_norm': 0.009333967231214046, 'learning_rate': 3.770758122743682e-05, 'epoch': 1.08}
対照学習損失: 0.0001234729279531166
対照学習損失: 6.427095649996772e-05
{'loss': 0.0001, 'grad_norm': 0.005291699897497892, 'learning_rate': 3.768953068592058e-05, 'epoch': 1.08}
対照学習損失: 2.9152113711461425e-05
対照学習損失: 3.80544806830585e-05
{'loss': 0.0, 'grad_norm': 0.0014053680934011936, 'learning_rate': 3.767148014440434e-05, 'epoch': 1.09}
対照学習損失: 0.00022729078773409128
対照学習損失: 2.3699463781667873e-05
{'loss': 0.0001, 'grad_norm': 0.006441464181989431, 'learning_rate': 3.765342960288809e-05, 'epoch': 1.09}
対照学習損失: 0.00011610016372287646
対照学習損失: 3.575793016352691e-05
{'loss': 0.0001, 'grad_norm': 0.005197280552238226, 'learning_rate': 3.7635379061371845e-05, 'epoch': 1.09}
対照学習損失: 0.00029152166098356247
対照学習損失: 0.0005033513880334795
{'loss': 0.0004, 'grad_norm': 0.03711867704987526, 'learning_rate': 3.76173285198556e-05, 'epoch': 1.09}
対照学習損失: 2.0950395992258564e-05
対照学習損失: 0.0001511771115474403
{'loss': 0.0001, 'grad_norm': 0.003478774568066001, 'learning_rate': 3.759927797833935e-05, 'epoch': 1.09}
対照学習損失: 0.0001287059421883896
対照学習損失: 5.900080213905312e-05
{'loss': 0.0001, 'grad_norm': 0.004071312490850687, 'learning_rate': 3.758122743682311e-05, 'epoch': 1.09}
対照学習損失: 0.0008202098542824388
対照学習損失: 0.00021523251780308783
{'loss': 0.0005, 'grad_norm': 0.01868712157011032, 'learning_rate': 3.756317689530686e-05, 'epoch': 1.09}
対照学習損失: 6.479796138592064e-05
対照学習損失: 3.1126608519116417e-05
{'loss': 0.0, 'grad_norm': 0.002179395640268922, 'learning_rate': 3.754512635379062e-05, 'epoch': 1.09}
対照学習損失: 1.4789240594836883e-05
対照学習損失: 0.00010804153134813532
{'loss': 0.0001, 'grad_norm': 0.003716087667271495, 'learning_rate': 3.752707581227437e-05, 'epoch': 1.09}
対照学習損失: 0.00032930891029536724
対照学習損失: 1.4275132343755104e-05
{'loss': 0.0002, 'grad_norm': 0.008924118243157864, 'learning_rate': 3.7509025270758125e-05, 'epoch': 1.09}
対照学習損失: 4.6650417061755434e-05
対照学習損失: 9.408107143826783e-05
{'loss': 0.0001, 'grad_norm': 0.005912547465413809, 'learning_rate': 3.749097472924188e-05, 'epoch': 1.09}
対照学習損失: 0.0005744007648900151
対照学習損失: 8.253689156845212e-05
{'loss': 0.0003, 'grad_norm': 0.031048212200403214, 'learning_rate': 3.747292418772563e-05, 'epoch': 1.1}
対照学習損失: 6.220980867510661e-05
対照学習損失: 1.0020937224908266e-05
{'loss': 0.0, 'grad_norm': 0.004657565616071224, 'learning_rate': 3.745487364620939e-05, 'epoch': 1.1}
対照学習損失: 5.952866922598332e-05
対照学習損失: 0.00012097642320441082
{'loss': 0.0001, 'grad_norm': 0.004465804900974035, 'learning_rate': 3.743682310469314e-05, 'epoch': 1.1}
対照学習損失: 0.00010232575004920363
対照学習損失: 0.0002748440601862967
{'loss': 0.0002, 'grad_norm': 0.007251403760164976, 'learning_rate': 3.74187725631769e-05, 'epoch': 1.1}
対照学習損失: 9.57176525844261e-05
対照学習損失: 0.00011656846618279815
{'loss': 0.0001, 'grad_norm': 0.006236363667994738, 'learning_rate': 3.740072202166065e-05, 'epoch': 1.1}
対照学習損失: 0.00015927806089166552
対照学習損失: 5.4348522098734975e-05
{'loss': 0.0001, 'grad_norm': 0.005669582635164261, 'learning_rate': 3.7382671480144405e-05, 'epoch': 1.1}
対照学習損失: 0.0001459112245356664
対照学習損失: 2.9323800845304504e-05
{'loss': 0.0001, 'grad_norm': 0.0029827565886080265, 'learning_rate': 3.7364620938628155e-05, 'epoch': 1.1}
対照学習損失: 0.0004983319668099284
対照学習損失: 4.0275401261169463e-05
{'loss': 0.0003, 'grad_norm': 0.01166245061904192, 'learning_rate': 3.734657039711191e-05, 'epoch': 1.1}
対照学習損失: 1.1220308806514367e-05
対照学習損失: 7.054118032101542e-05
{'loss': 0.0, 'grad_norm': 0.002300670836120844, 'learning_rate': 3.732851985559567e-05, 'epoch': 1.1}
対照学習損失: 1.803741542971693e-05
対照学習損失: 1.9311110008857213e-05
{'loss': 0.0, 'grad_norm': 0.0005673890118487179, 'learning_rate': 3.731046931407942e-05, 'epoch': 1.1}
対照学習損失: 6.142163329059258e-05
対照学習損失: 0.00037519013858400285
{'loss': 0.0002, 'grad_norm': 0.011244062334299088, 'learning_rate': 3.7292418772563184e-05, 'epoch': 1.1}
対照学習損失: 0.00014084397116675973
対照学習損失: 2.4444025257253088e-05
{'loss': 0.0001, 'grad_norm': 0.004966761916875839, 'learning_rate': 3.7274368231046935e-05, 'epoch': 1.11}
対照学習損失: 2.3155736926128156e-05
対照学習損失: 0.00027191900881007314
{'loss': 0.0001, 'grad_norm': 0.013766990974545479, 'learning_rate': 3.725631768953069e-05, 'epoch': 1.11}
対照学習損失: 0.0004447445389814675
対照学習損失: 6.64531034999527e-05
{'loss': 0.0003, 'grad_norm': 0.009456499479711056, 'learning_rate': 3.723826714801444e-05, 'epoch': 1.11}
対照学習損失: 0.0005222386680543423
対照学習損失: 4.209937833365984e-05
{'loss': 0.0003, 'grad_norm': 0.018488790839910507, 'learning_rate': 3.72202166064982e-05, 'epoch': 1.11}
対照学習損失: 2.7378640879760496e-05
対照学習損失: 0.00020114849030505866
{'loss': 0.0001, 'grad_norm': 0.00551220728084445, 'learning_rate': 3.720216606498195e-05, 'epoch': 1.11}
対照学習損失: 2.6396381144877523e-05
対照学習損失: 0.021441180258989334
{'loss': 0.0107, 'grad_norm': 0.4579600393772125, 'learning_rate': 3.718411552346571e-05, 'epoch': 1.11}
対照学習損失: 0.00017150882922578603
対照学習損失: 0.0005373026942834258
{'loss': 0.0004, 'grad_norm': 0.014361031353473663, 'learning_rate': 3.7166064981949464e-05, 'epoch': 1.11}
対照学習損失: 0.00018692982848733664
対照学習損失: 0.00023070734459906816
{'loss': 0.0002, 'grad_norm': 0.009292247705161572, 'learning_rate': 3.7148014440433215e-05, 'epoch': 1.11}
対照学習損失: 0.0003062692703679204
対照学習損失: 2.2589447326026857e-05
{'loss': 0.0002, 'grad_norm': 0.012846667319536209, 'learning_rate': 3.712996389891697e-05, 'epoch': 1.11}
対照学習損失: 2.5621091481298208e-05
対照学習損失: 4.76427776447963e-05
{'loss': 0.0, 'grad_norm': 0.00116265332326293, 'learning_rate': 3.711191335740072e-05, 'epoch': 1.11}
対照学習損失: 0.00010238896356895566
対照学習損失: 4.591975448420271e-05
{'loss': 0.0001, 'grad_norm': 0.0024327384307980537, 'learning_rate': 3.709386281588448e-05, 'epoch': 1.12}
対照学習損失: 0.0001490503636887297
対照学習損失: 6.874003156553954e-05
{'loss': 0.0001, 'grad_norm': 0.003959800116717815, 'learning_rate': 3.707581227436823e-05, 'epoch': 1.12}
対照学習損失: 0.00028925423976033926
対照学習損失: 0.0001702442968962714
{'loss': 0.0002, 'grad_norm': 0.008922465145587921, 'learning_rate': 3.705776173285199e-05, 'epoch': 1.12}
対照学習損失: 0.0001893833396025002
対照学習損失: 0.00034601613879203796
{'loss': 0.0003, 'grad_norm': 0.010107044130563736, 'learning_rate': 3.703971119133574e-05, 'epoch': 1.12}
対照学習損失: 5.319567208061926e-05
対照学習損失: 2.5226432626368478e-05
{'loss': 0.0, 'grad_norm': 0.002711058594286442, 'learning_rate': 3.7021660649819495e-05, 'epoch': 1.12}
対照学習損失: 3.424051101319492e-05
対照学習損失: 0.00028878802550025284
{'loss': 0.0002, 'grad_norm': 0.007881652563810349, 'learning_rate': 3.700361010830325e-05, 'epoch': 1.12}
対照学習損失: 3.0568735382985324e-05
対照学習損失: 0.00011342670040903613
{'loss': 0.0001, 'grad_norm': 0.002434466267004609, 'learning_rate': 3.6985559566787e-05, 'epoch': 1.12}
対照学習損失: 7.49157479731366e-05
対照学習損失: 0.00010199997632298619
{'loss': 0.0001, 'grad_norm': 0.00503412913531065, 'learning_rate': 3.696750902527076e-05, 'epoch': 1.12}
対照学習損失: 9.35821735765785e-05
対照学習損失: 0.00010482317884452641
{'loss': 0.0001, 'grad_norm': 0.00800967775285244, 'learning_rate': 3.694945848375451e-05, 'epoch': 1.12}
対照学習損失: 4.3298838136252016e-05
対照学習損失: 1.3917389878770337e-05
{'loss': 0.0, 'grad_norm': 0.001830584486015141, 'learning_rate': 3.693140794223827e-05, 'epoch': 1.12}
対照学習損失: 1.85140706889797e-05
対照学習損失: 0.00015654912567697465
{'loss': 0.0001, 'grad_norm': 0.00722719170153141, 'learning_rate': 3.6913357400722025e-05, 'epoch': 1.12}
対照学習損失: 0.0004308978677727282
対照学習損失: 2.2409356461139396e-05
{'loss': 0.0002, 'grad_norm': 0.019505277276039124, 'learning_rate': 3.689530685920578e-05, 'epoch': 1.13}
対照学習損失: 2.3520355171058327e-05
対照学習損失: 7.994769839569926e-05
{'loss': 0.0001, 'grad_norm': 0.0024205120280385017, 'learning_rate': 3.687725631768953e-05, 'epoch': 1.13}
対照学習損失: 6.142295023892075e-05
対照学習損失: 1.3157496141502634e-05
{'loss': 0.0, 'grad_norm': 0.0021116777788847685, 'learning_rate': 3.685920577617329e-05, 'epoch': 1.13}
対照学習損失: 6.842977018095553e-05
対照学習損失: 7.013724825810641e-05
{'loss': 0.0001, 'grad_norm': 0.002953797811642289, 'learning_rate': 3.684115523465704e-05, 'epoch': 1.13}
対照学習損失: 2.4779999876045622e-05
対照学習損失: 0.0001396666484652087
{'loss': 0.0001, 'grad_norm': 0.00321981287561357, 'learning_rate': 3.68231046931408e-05, 'epoch': 1.13}
対照学習損失: 2.362458690186031e-05
対照学習損失: 3.201382787665352e-05
{'loss': 0.0, 'grad_norm': 0.0007679149857722223, 'learning_rate': 3.6805054151624554e-05, 'epoch': 1.13}
対照学習損失: 7.454867591150105e-05
対照学習損失: 4.245607851771638e-05
{'loss': 0.0001, 'grad_norm': 0.0025452300906181335, 'learning_rate': 3.6787003610108305e-05, 'epoch': 1.13}
対照学習損失: 7.572952017653733e-05
対照学習損失: 1.7091237168642692e-05
{'loss': 0.0, 'grad_norm': 0.0016698100371286273, 'learning_rate': 3.676895306859206e-05, 'epoch': 1.13}
対照学習損失: 8.388252899749205e-05
対照学習損失: 1.7620148355490528e-05
{'loss': 0.0001, 'grad_norm': 0.0031285081058740616, 'learning_rate': 3.675090252707581e-05, 'epoch': 1.13}
対照学習損失: 0.0001377486769342795
対照学習損失: 5.290338594932109e-05
{'loss': 0.0001, 'grad_norm': 0.0026910030283033848, 'learning_rate': 3.673285198555957e-05, 'epoch': 1.13}
対照学習損失: 7.381636532954872e-05
対照学習損失: 0.00027069912175647914
{'loss': 0.0002, 'grad_norm': 0.01084597222507, 'learning_rate': 3.671480144404332e-05, 'epoch': 1.13}
対照学習損失: 6.493314867839217e-05
対照学習損失: 0.002795797074213624
{'loss': 0.0014, 'grad_norm': 0.08993005007505417, 'learning_rate': 3.669675090252708e-05, 'epoch': 1.14}
対照学習損失: 2.842057256202679e-05
対照学習損失: 2.882433545892127e-05
{'loss': 0.0, 'grad_norm': 0.0008247424848377705, 'learning_rate': 3.6678700361010834e-05, 'epoch': 1.14}
対照学習損失: 5.4449632443720475e-05
対照学習損失: 8.138323028106242e-05
{'loss': 0.0001, 'grad_norm': 0.0028120132628828287, 'learning_rate': 3.6660649819494585e-05, 'epoch': 1.14}
対照学習損失: 7.28233135305345e-05
対照学習損失: 0.012159165926277637
{'loss': 0.0061, 'grad_norm': 0.3244973421096802, 'learning_rate': 3.664259927797834e-05, 'epoch': 1.14}
対照学習損失: 8.419001824222505e-05
対照学習損失: 0.00016391831741202623
{'loss': 0.0001, 'grad_norm': 0.005963420961052179, 'learning_rate': 3.662454873646209e-05, 'epoch': 1.14}
対照学習損失: 4.451275890460238e-05
対照学習損失: 0.0002906788431573659
{'loss': 0.0002, 'grad_norm': 0.009373188950121403, 'learning_rate': 3.660649819494585e-05, 'epoch': 1.14}
対照学習損失: 8.219535084208474e-05
対照学習損失: 0.00016444330685772002
{'loss': 0.0001, 'grad_norm': 0.004441583529114723, 'learning_rate': 3.65884476534296e-05, 'epoch': 1.14}
対照学習損失: 3.140241824439727e-05
対照学習損失: 3.806829408858903e-05
{'loss': 0.0, 'grad_norm': 0.0009305630810558796, 'learning_rate': 3.657039711191336e-05, 'epoch': 1.14}
対照学習損失: 4.954257019562647e-05
対照学習損失: 0.0003020420263055712
{'loss': 0.0002, 'grad_norm': 0.007544259075075388, 'learning_rate': 3.6552346570397114e-05, 'epoch': 1.14}
対照学習損失: 3.960993853979744e-05
対照学習損失: 2.8422578907338902e-05
{'loss': 0.0, 'grad_norm': 0.0018764704000204802, 'learning_rate': 3.6534296028880865e-05, 'epoch': 1.14}
対照学習損失: 4.2908646719297394e-05
対照学習損失: 0.00017165526514872909
{'loss': 0.0001, 'grad_norm': 0.005281595978885889, 'learning_rate': 3.651624548736462e-05, 'epoch': 1.14}
対照学習損失: 2.162822420359589e-05
対照学習損失: 4.667354733101092e-05
{'loss': 0.0, 'grad_norm': 0.0012555185239762068, 'learning_rate': 3.649819494584838e-05, 'epoch': 1.15}
対照学習損失: 4.437751340446994e-05
対照学習損失: 0.0003380493726581335
{'loss': 0.0002, 'grad_norm': 0.01557551696896553, 'learning_rate': 3.6480144404332137e-05, 'epoch': 1.15}
対照学習損失: 0.00013441758346743882
対照学習損失: 3.5439454222796485e-05
{'loss': 0.0001, 'grad_norm': 0.007767208386212587, 'learning_rate': 3.646209386281589e-05, 'epoch': 1.15}
対照学習損失: 4.751668166136369e-05
対照学習損失: 8.281198097392917e-05
{'loss': 0.0001, 'grad_norm': 0.0023283122573047876, 'learning_rate': 3.6444043321299644e-05, 'epoch': 1.15}
対照学習損失: 7.41284602554515e-05
対照学習損失: 5.1632259783218615e-06
{'loss': 0.0, 'grad_norm': 0.0018642140785232186, 'learning_rate': 3.6425992779783395e-05, 'epoch': 1.15}
対照学習損失: 9.536652942188084e-05
対照学習損失: 3.337566522532143e-05
{'loss': 0.0001, 'grad_norm': 0.0029653264209628105, 'learning_rate': 3.640794223826715e-05, 'epoch': 1.15}
対照学習損失: 0.00028265718719922006
対照学習損失: 8.64974717842415e-05
{'loss': 0.0002, 'grad_norm': 0.008985854685306549, 'learning_rate': 3.63898916967509e-05, 'epoch': 1.15}
対照学習損失: 1.802998667699285e-05
対照学習損失: 2.4406796001130715e-05
{'loss': 0.0, 'grad_norm': 0.0005648562801070511, 'learning_rate': 3.637184115523466e-05, 'epoch': 1.15}
対照学習損失: 3.587874016375281e-05
対照学習損失: 0.00017340484191663563
{'loss': 0.0001, 'grad_norm': 0.005239846184849739, 'learning_rate': 3.6353790613718417e-05, 'epoch': 1.15}
対照学習損失: 7.548770372522995e-05
対照学習損失: 6.45492400508374e-05
{'loss': 0.0001, 'grad_norm': 0.002172025153413415, 'learning_rate': 3.633574007220217e-05, 'epoch': 1.15}
対照学習損失: 2.8205760827404447e-05
対照学習損失: 8.551192149752751e-05
{'loss': 0.0001, 'grad_norm': 0.0027692040894180536, 'learning_rate': 3.6317689530685924e-05, 'epoch': 1.15}
対照学習損失: 0.00033787291613407433
対照学習損失: 2.6991887352778576e-05
{'loss': 0.0002, 'grad_norm': 0.01079613622277975, 'learning_rate': 3.6299638989169675e-05, 'epoch': 1.16}
対照学習損失: 8.163270831573755e-05
対照学習損失: 3.298238880233839e-05
{'loss': 0.0001, 'grad_norm': 0.0023552842903882265, 'learning_rate': 3.628158844765343e-05, 'epoch': 1.16}
対照学習損失: 0.0001611727348063141
対照学習損失: 0.0003080847964156419
{'loss': 0.0002, 'grad_norm': 0.018534161150455475, 'learning_rate': 3.626353790613718e-05, 'epoch': 1.16}
対照学習損失: 1.1957930837525055e-05
対照学習損失: 4.9256967031396925e-05
{'loss': 0.0, 'grad_norm': 0.0009099814342334867, 'learning_rate': 3.624548736462094e-05, 'epoch': 1.16}
対照学習損失: 7.713897502981126e-05
対照学習損失: 0.0009412072831764817
{'loss': 0.0005, 'grad_norm': 0.035093940794467926, 'learning_rate': 3.62274368231047e-05, 'epoch': 1.16}
対照学習損失: 8.684572094352916e-05
対照学習損失: 1.4528188330587e-05
{'loss': 0.0001, 'grad_norm': 0.004792763385921717, 'learning_rate': 3.620938628158845e-05, 'epoch': 1.16}
対照学習損失: 1.1637652278295718e-05
対照学習損失: 0.0003575164300855249
{'loss': 0.0002, 'grad_norm': 0.014049053192138672, 'learning_rate': 3.6191335740072204e-05, 'epoch': 1.16}
対照学習損失: 9.180092456517741e-05
対照学習損失: 9.368870814796537e-05
{'loss': 0.0001, 'grad_norm': 0.004306436516344547, 'learning_rate': 3.6173285198555955e-05, 'epoch': 1.16}
対照学習損失: 1.2971087016921956e-05
対照学習損失: 9.038560529006645e-05
{'loss': 0.0001, 'grad_norm': 0.002490669023245573, 'learning_rate': 3.615523465703971e-05, 'epoch': 1.16}
対照学習損失: 9.894190043269191e-06
対照学習損失: 6.936442787264241e-06
{'loss': 0.0, 'grad_norm': 0.00019290170166641474, 'learning_rate': 3.613718411552346e-05, 'epoch': 1.16}
対照学習損失: 9.248725837096572e-05
対照学習損失: 6.998790195211768e-05
{'loss': 0.0001, 'grad_norm': 0.0040118820033967495, 'learning_rate': 3.6119133574007226e-05, 'epoch': 1.16}
対照学習損失: 0.0004185258294455707
対照学習損失: 5.213525582803413e-05
{'loss': 0.0002, 'grad_norm': 0.015880225226283073, 'learning_rate': 3.610108303249098e-05, 'epoch': 1.17}
対照学習損失: 9.126881195697933e-06
対照学習損失: 5.958602559985593e-05
{'loss': 0.0, 'grad_norm': 0.001496820361353457, 'learning_rate': 3.6083032490974734e-05, 'epoch': 1.17}
対照学習損失: 0.006130993366241455
対照学習損失: 8.880990208126605e-06
{'loss': 0.0031, 'grad_norm': 0.533865749835968, 'learning_rate': 3.6064981949458484e-05, 'epoch': 1.17}
対照学習損失: 7.898941112216562e-05
対照学習損失: 0.00010882087372010574
{'loss': 0.0001, 'grad_norm': 0.0037540283519774675, 'learning_rate': 3.604693140794224e-05, 'epoch': 1.17}
対照学習損失: 0.0004897991311736405
対照学習損失: 0.00020999767002649605
{'loss': 0.0003, 'grad_norm': 0.028507240116596222, 'learning_rate': 3.6028880866426e-05, 'epoch': 1.17}
対照学習損失: 8.314725164382253e-06
対照学習損失: 2.2514179363497533e-05
{'loss': 0.0, 'grad_norm': 0.0005642322939820588, 'learning_rate': 3.601083032490975e-05, 'epoch': 1.17}
対照学習損失: 1.3120183211867698e-05
対照学習損失: 9.871930160443299e-06
{'loss': 0.0, 'grad_norm': 0.00044712063390761614, 'learning_rate': 3.5992779783393507e-05, 'epoch': 1.17}
対照学習損失: 0.00022166378039401025
対照学習損失: 0.0015342938713729382
{'loss': 0.0009, 'grad_norm': 0.12227959930896759, 'learning_rate': 3.597472924187726e-05, 'epoch': 1.17}
対照学習損失: 0.006441853940486908
対照学習損失: 0.00015676293696742505
{'loss': 0.0033, 'grad_norm': 0.3895465135574341, 'learning_rate': 3.5956678700361014e-05, 'epoch': 1.17}
対照学習損失: 5.042113843956031e-05
対照学習損失: 1.741899541229941e-05
{'loss': 0.0, 'grad_norm': 0.001034311018884182, 'learning_rate': 3.5938628158844765e-05, 'epoch': 1.17}
対照学習損失: 6.854758248664439e-05
対照学習損失: 0.0007326950435526669
{'loss': 0.0004, 'grad_norm': 0.04583944007754326, 'learning_rate': 3.592057761732852e-05, 'epoch': 1.17}
対照学習損失: 1.9408234948059544e-05
対照学習損失: 2.3244097974384204e-05
{'loss': 0.0, 'grad_norm': 0.0007405474316328764, 'learning_rate': 3.590252707581228e-05, 'epoch': 1.18}
対照学習損失: 0.0029152510687708855
対照学習損失: 0.0006664646789431572
{'loss': 0.0018, 'grad_norm': 0.11801709234714508, 'learning_rate': 3.588447653429603e-05, 'epoch': 1.18}
対照学習損失: 2.6551380869932473e-05
対照学習損失: 9.617773321224377e-05
{'loss': 0.0001, 'grad_norm': 0.003459878731518984, 'learning_rate': 3.5866425992779787e-05, 'epoch': 1.18}
対照学習損失: 4.1051996959140524e-05
対照学習損失: 2.3178097762865946e-05
{'loss': 0.0, 'grad_norm': 0.0011125655146315694, 'learning_rate': 3.584837545126354e-05, 'epoch': 1.18}
対照学習損失: 2.1591018594335765e-05
対照学習損失: 0.0005446983268484473
{'loss': 0.0003, 'grad_norm': 0.027099598199129105, 'learning_rate': 3.5830324909747294e-05, 'epoch': 1.18}
対照学習損失: 9.835377568379045e-05
対照学習損失: 8.107715984806418e-05
{'loss': 0.0001, 'grad_norm': 0.004579393193125725, 'learning_rate': 3.5812274368231045e-05, 'epoch': 1.18}
対照学習損失: 7.511682633776218e-05
対照学習損失: 1.6062929717008956e-05
{'loss': 0.0, 'grad_norm': 0.0016720456769689918, 'learning_rate': 3.57942238267148e-05, 'epoch': 1.18}
対照学習損失: 0.0001238756813108921
対照学習損失: 1.7538259271532297e-05
{'loss': 0.0001, 'grad_norm': 0.003093891078606248, 'learning_rate': 3.577617328519855e-05, 'epoch': 1.18}
対照学習損失: 0.00035017088521271944
対照学習損失: 2.219447742390912e-05
{'loss': 0.0002, 'grad_norm': 0.013235846534371376, 'learning_rate': 3.575812274368231e-05, 'epoch': 1.18}
対照学習損失: 3.160360938636586e-05
対照学習損失: 0.00025576609186828136
{'loss': 0.0001, 'grad_norm': 0.01078120619058609, 'learning_rate': 3.574007220216607e-05, 'epoch': 1.18}
対照学習損失: 2.806444899761118e-05
対照学習損失: 0.04264220967888832
{'loss': 0.0213, 'grad_norm': 1.7005538940429688, 'learning_rate': 3.5722021660649824e-05, 'epoch': 1.18}
対照学習損失: 2.785673495964147e-05
対照学習損失: 0.0004950286820530891
{'loss': 0.0003, 'grad_norm': 0.024683458730578423, 'learning_rate': 3.570397111913358e-05, 'epoch': 1.19}
対照学習損失: 3.402281436137855e-05
対照学習損失: 7.395401189569384e-05
{'loss': 0.0001, 'grad_norm': 0.0019136065384373069, 'learning_rate': 3.568592057761733e-05, 'epoch': 1.19}
対照学習損失: 0.00025354683748446405
対照学習損失: 6.0484839195851237e-05
{'loss': 0.0002, 'grad_norm': 0.011599255725741386, 'learning_rate': 3.566787003610109e-05, 'epoch': 1.19}
対照学習損失: 3.6347770219435915e-05
対照学習損失: 3.723566987900995e-05
{'loss': 0.0, 'grad_norm': 0.0010832827538251877, 'learning_rate': 3.564981949458484e-05, 'epoch': 1.19}
対照学習損失: 1.776154022081755e-05
対照学習損失: 9.491930541116744e-06
{'loss': 0.0, 'grad_norm': 0.0003507602377794683, 'learning_rate': 3.5631768953068596e-05, 'epoch': 1.19}
対照学習損失: 9.291383321397007e-05
対照学習損失: 8.453342888969928e-05
{'loss': 0.0001, 'grad_norm': 0.003878213232383132, 'learning_rate': 3.561371841155235e-05, 'epoch': 1.19}
対照学習損失: 5.900050018681213e-05
対照学習損失: 1.4163320884108543e-05
{'loss': 0.0, 'grad_norm': 0.0012716708006337285, 'learning_rate': 3.5595667870036104e-05, 'epoch': 1.19}
対照学習損失: 1.3403366210695822e-05
対照学習損失: 1.7888400179799646e-05
{'loss': 0.0, 'grad_norm': 0.0005293520516715944, 'learning_rate': 3.5577617328519854e-05, 'epoch': 1.19}
対照学習損失: 3.440277214394882e-05
対照学習損失: 6.04397791903466e-05
{'loss': 0.0, 'grad_norm': 0.0024739601649343967, 'learning_rate': 3.555956678700361e-05, 'epoch': 1.19}
対照学習損失: 0.0002772880543489009
対照学習損失: 4.255105523043312e-05
{'loss': 0.0002, 'grad_norm': 0.0066053071059286594, 'learning_rate': 3.554151624548737e-05, 'epoch': 1.19}
対照学習損失: 0.0004379374731797725
対照学習損失: 0.00105990469455719
{'loss': 0.0007, 'grad_norm': 0.0864848718047142, 'learning_rate': 3.552346570397112e-05, 'epoch': 1.19}
対照学習損失: 0.0008263596682809293
対照学習損失: 7.752290548523888e-05
{'loss': 0.0005, 'grad_norm': 0.027473624795675278, 'learning_rate': 3.5505415162454877e-05, 'epoch': 1.2}
対照学習損失: 1.9646493456093594e-05
対照学習損失: 0.00039299105992540717
{'loss': 0.0002, 'grad_norm': 0.02607242576777935, 'learning_rate': 3.548736462093863e-05, 'epoch': 1.2}
対照学習損失: 0.00013609642337542027
対照学習損失: 3.3211024856427684e-05
{'loss': 0.0001, 'grad_norm': 0.00383889046497643, 'learning_rate': 3.5469314079422384e-05, 'epoch': 1.2}
対照学習損失: 4.078588972333819e-05
対照学習損失: 1.3239357940619811e-05
{'loss': 0.0, 'grad_norm': 0.002211707178503275, 'learning_rate': 3.5451263537906135e-05, 'epoch': 1.2}
対照学習損失: 0.00011744634684873745
対照学習損失: 3.4158470953116193e-05
{'loss': 0.0001, 'grad_norm': 0.0025586748961359262, 'learning_rate': 3.543321299638989e-05, 'epoch': 1.2}
対照学習損失: 6.97983123245649e-05
対照学習損失: 5.901994154555723e-05
{'loss': 0.0001, 'grad_norm': 0.005006437189877033, 'learning_rate': 3.541516245487365e-05, 'epoch': 1.2}
対照学習損失: 0.00032872805604711175
対照学習損失: 1.6942272850428708e-05
{'loss': 0.0002, 'grad_norm': 0.020943202078342438, 'learning_rate': 3.53971119133574e-05, 'epoch': 1.2}
対照学習損失: 2.3215361579786986e-05
対照学習損失: 2.918217251135502e-05
{'loss': 0.0, 'grad_norm': 0.0007265518070198596, 'learning_rate': 3.537906137184116e-05, 'epoch': 1.2}
対照学習損失: 0.0002673736016731709
対照学習損失: 0.00017661679885350168
{'loss': 0.0002, 'grad_norm': 0.01308474037796259, 'learning_rate': 3.536101083032491e-05, 'epoch': 1.2}
対照学習損失: 0.0006504787015728652
対照学習損失: 5.1856666686944664e-05
{'loss': 0.0004, 'grad_norm': 0.029159678146243095, 'learning_rate': 3.534296028880867e-05, 'epoch': 1.2}
対照学習損失: 2.629163645906374e-05
対照学習損失: 4.371187969809398e-05
{'loss': 0.0, 'grad_norm': 0.0015198952751234174, 'learning_rate': 3.532490974729242e-05, 'epoch': 1.2}
対照学習損失: 2.6359230105299503e-05
対照学習損失: 0.0001636123633943498
{'loss': 0.0001, 'grad_norm': 0.00488497456535697, 'learning_rate': 3.530685920577618e-05, 'epoch': 1.21}
対照学習損失: 3.29897302435711e-05
対照学習損失: 5.957603207207285e-05
{'loss': 0.0, 'grad_norm': 0.0017426797421649098, 'learning_rate': 3.528880866425993e-05, 'epoch': 1.21}
対照学習損失: 3.216151526430622e-05
対照学習損失: 0.0008286551455967128
{'loss': 0.0004, 'grad_norm': 0.02453101985156536, 'learning_rate': 3.5270758122743686e-05, 'epoch': 1.21}
対照学習損失: 0.006369308102875948
対照学習損失: 2.7126614440931007e-05
{'loss': 0.0032, 'grad_norm': 0.31119340658187866, 'learning_rate': 3.525270758122744e-05, 'epoch': 1.21}
対照学習損失: 1.8894090317189693e-05
対照学習損失: 4.244898445904255e-05
{'loss': 0.0, 'grad_norm': 0.0014632097445428371, 'learning_rate': 3.5234657039711194e-05, 'epoch': 1.21}
対照学習損失: 2.2067901227273978e-05
対照学習損失: 0.0002721944183576852
{'loss': 0.0001, 'grad_norm': 0.007144011091440916, 'learning_rate': 3.521660649819495e-05, 'epoch': 1.21}
対照学習損失: 7.38347716833232e-06
対照学習損失: 6.107064837124199e-05
{'loss': 0.0, 'grad_norm': 0.001597190392203629, 'learning_rate': 3.51985559566787e-05, 'epoch': 1.21}
対照学習損失: 3.1670628231950104e-05
対照学習損失: 2.308043258381076e-05
{'loss': 0.0, 'grad_norm': 0.001288236933760345, 'learning_rate': 3.518050541516246e-05, 'epoch': 1.21}
対照学習損失: 9.164314542431384e-05
対照学習損失: 2.8943404686287977e-05
{'loss': 0.0001, 'grad_norm': 0.0028106768149882555, 'learning_rate': 3.516245487364621e-05, 'epoch': 1.21}
対照学習損失: 0.002968932269141078
対照学習損失: 0.0012169589754194021
{'loss': 0.0021, 'grad_norm': 0.1267353743314743, 'learning_rate': 3.5144404332129966e-05, 'epoch': 1.21}
対照学習損失: 3.171465505147353e-05
対照学習損失: 0.00014816017937846482
{'loss': 0.0001, 'grad_norm': 0.00465404661372304, 'learning_rate': 3.512635379061372e-05, 'epoch': 1.22}
対照学習損失: 1.2807215171051212e-05
対照学習損失: 8.90332830749685e-06
{'loss': 0.0, 'grad_norm': 0.00031891700928099453, 'learning_rate': 3.5108303249097474e-05, 'epoch': 1.22}
対照学習損失: 3.423096495680511e-05
対照学習損失: 7.393549458356574e-05
{'loss': 0.0001, 'grad_norm': 0.0025746957398951054, 'learning_rate': 3.509025270758123e-05, 'epoch': 1.22}
対照学習損失: 2.579257488832809e-05
対照学習損失: 9.81851844699122e-05
{'loss': 0.0001, 'grad_norm': 0.006389631889760494, 'learning_rate': 3.507220216606498e-05, 'epoch': 1.22}
対照学習損失: 0.0001823542406782508
対照学習損失: 3.159271363983862e-05
{'loss': 0.0001, 'grad_norm': 0.004884432535618544, 'learning_rate': 3.505415162454874e-05, 'epoch': 1.22}
対照学習損失: 7.612729677930474e-05
対照学習損失: 0.00018557072326075286
{'loss': 0.0001, 'grad_norm': 0.005619835574179888, 'learning_rate': 3.503610108303249e-05, 'epoch': 1.22}
対照学習損失: 4.7099027142394334e-05
対照学習損失: 3.863577876472846e-05
{'loss': 0.0, 'grad_norm': 0.0013773665996268392, 'learning_rate': 3.5018050541516247e-05, 'epoch': 1.22}
対照学習損失: 6.474404653999954e-05
対照学習損失: 3.204860695404932e-05
{'loss': 0.0, 'grad_norm': 0.0020715591963380575, 'learning_rate': 3.5e-05, 'epoch': 1.22}
対照学習損失: 1.664433511905372e-05
対照学習損失: 4.402933700475842e-05
{'loss': 0.0, 'grad_norm': 0.0008657774887979031, 'learning_rate': 3.4981949458483754e-05, 'epoch': 1.22}
対照学習損失: 0.0014182820450514555
対照学習損失: 3.330908657517284e-05
{'loss': 0.0007, 'grad_norm': 0.03778019919991493, 'learning_rate': 3.496389891696751e-05, 'epoch': 1.22}
対照学習損失: 9.235923062078655e-05
対照学習損失: 0.0006980472826398909
{'loss': 0.0004, 'grad_norm': 0.02100037783384323, 'learning_rate': 3.494584837545127e-05, 'epoch': 1.22}
対照学習損失: 7.91761267464608e-05
対照学習損失: 0.0015093450201675296
{'loss': 0.0008, 'grad_norm': 0.03781509771943092, 'learning_rate': 3.492779783393502e-05, 'epoch': 1.23}
対照学習損失: 0.0009406658937223256
対照学習損失: 2.5137243937933818e-05
{'loss': 0.0005, 'grad_norm': 0.07380568236112595, 'learning_rate': 3.4909747292418776e-05, 'epoch': 1.23}
対照学習損失: 8.672399417264387e-06
対照学習損失: 0.00027065182803198695
{'loss': 0.0001, 'grad_norm': 0.0065537565387785435, 'learning_rate': 3.4891696750902533e-05, 'epoch': 1.23}
対照学習損失: 1.1190642908331938e-05
対照学習損失: 7.465659291483462e-05
{'loss': 0.0, 'grad_norm': 0.0020939819514751434, 'learning_rate': 3.4873646209386284e-05, 'epoch': 1.23}
対照学習損失: 0.00011660494783427566
対照学習損失: 0.0001984034024644643
{'loss': 0.0002, 'grad_norm': 0.004122219514101744, 'learning_rate': 3.485559566787004e-05, 'epoch': 1.23}
対照学習損失: 3.1714109354652464e-05
対照学習損失: 0.00011426377750467509
{'loss': 0.0001, 'grad_norm': 0.004663786850869656, 'learning_rate': 3.483754512635379e-05, 'epoch': 1.23}
対照学習損失: 7.852072303649038e-05
対照学習損失: 6.876805855426937e-05
{'loss': 0.0001, 'grad_norm': 0.0026036675553768873, 'learning_rate': 3.481949458483755e-05, 'epoch': 1.23}
対照学習損失: 6.348577153403312e-05
対照学習損失: 0.00012087009235983714
{'loss': 0.0001, 'grad_norm': 0.006359090097248554, 'learning_rate': 3.48014440433213e-05, 'epoch': 1.23}
対照学習損失: 0.0012586226221174002
対照学習損失: 9.415193198947236e-05
{'loss': 0.0007, 'grad_norm': 0.03888097032904625, 'learning_rate': 3.4783393501805056e-05, 'epoch': 1.23}
対照学習損失: 0.0003224449174012989
対照学習損失: 0.0002335041353944689
{'loss': 0.0003, 'grad_norm': 0.015023157000541687, 'learning_rate': 3.4765342960288813e-05, 'epoch': 1.23}
対照学習損失: 2.3729524400550872e-05
対照学習損失: 3.245819243602455e-05
{'loss': 0.0, 'grad_norm': 0.0018844027072191238, 'learning_rate': 3.4747292418772564e-05, 'epoch': 1.23}
対照学習損失: 3.0434015570790507e-05
対照学習損失: 0.0015694309258833528
{'loss': 0.0008, 'grad_norm': 0.04742114990949631, 'learning_rate': 3.472924187725632e-05, 'epoch': 1.24}
対照学習損失: 0.00029311294201761484
対照学習損失: 4.195306246401742e-05
{'loss': 0.0002, 'grad_norm': 0.014964395202696323, 'learning_rate': 3.471119133574007e-05, 'epoch': 1.24}
対照学習損失: 5.107696051709354e-05
対照学習損失: 0.00025413805269636214
{'loss': 0.0002, 'grad_norm': 0.009344636462628841, 'learning_rate': 3.469314079422383e-05, 'epoch': 1.24}
対照学習損失: 0.0004387479566503316
対照学習損失: 2.0302357370383106e-05
{'loss': 0.0002, 'grad_norm': 0.035931482911109924, 'learning_rate': 3.467509025270758e-05, 'epoch': 1.24}
対照学習損失: 2.406337807769887e-05
対照学習損失: 9.193925507133827e-06
{'loss': 0.0, 'grad_norm': 0.001138356514275074, 'learning_rate': 3.4657039711191336e-05, 'epoch': 1.24}
対照学習損失: 1.2568921192723792e-05
対照学習損失: 3.963678682339378e-05
{'loss': 0.0, 'grad_norm': 0.0021033668890595436, 'learning_rate': 3.463898916967509e-05, 'epoch': 1.24}
対照学習損失: 0.00014034849300514907
対照学習損失: 0.00027211190899834037
{'loss': 0.0002, 'grad_norm': 0.013406123965978622, 'learning_rate': 3.4620938628158844e-05, 'epoch': 1.24}
対照学習損失: 3.0501194487442262e-05
対照学習損失: 8.589762728661299e-05
{'loss': 0.0001, 'grad_norm': 0.0021291763987392187, 'learning_rate': 3.46028880866426e-05, 'epoch': 1.24}
対照学習損失: 7.266255852300674e-05
対照学習損失: 0.00024610364926047623
{'loss': 0.0002, 'grad_norm': 0.007373892702162266, 'learning_rate': 3.458483754512635e-05, 'epoch': 1.24}
対照学習損失: 1.7217906133737415e-05
対照学習損失: 1.5005241948529147e-05
{'loss': 0.0, 'grad_norm': 0.0004189610481262207, 'learning_rate': 3.4566787003610116e-05, 'epoch': 1.24}
対照学習損失: 4.948662535753101e-05
対照学習損失: 5.9540747315622866e-05
{'loss': 0.0001, 'grad_norm': 0.00231644744053483, 'learning_rate': 3.4548736462093866e-05, 'epoch': 1.24}
対照学習損失: 1.3492581274476834e-05
対照学習損失: 1.1145820280944463e-05
{'loss': 0.0, 'grad_norm': 0.0005317353643476963, 'learning_rate': 3.453068592057762e-05, 'epoch': 1.25}
対照学習損失: 3.02767803077586e-05
対照学習損失: 0.0012461997102946043
{'loss': 0.0006, 'grad_norm': 0.04764038696885109, 'learning_rate': 3.4512635379061374e-05, 'epoch': 1.25}
対照学習損失: 1.525852258055238e-05
対照学習損失: 3.34048418153543e-05
{'loss': 0.0, 'grad_norm': 0.0008184480830095708, 'learning_rate': 3.449458483754513e-05, 'epoch': 1.25}
対照学習損失: 3.1929463148117065e-05
対照学習損失: 0.0006620237254537642
{'loss': 0.0003, 'grad_norm': 0.020622489973902702, 'learning_rate': 3.447653429602888e-05, 'epoch': 1.25}
対照学習損失: 3.9134414691943675e-05
対照学習損失: 9.700502414489165e-05
{'loss': 0.0001, 'grad_norm': 0.0023037788923829794, 'learning_rate': 3.445848375451264e-05, 'epoch': 1.25}
対照学習損失: 0.0006295503117144108
対照学習損失: 0.0004410296387504786
{'loss': 0.0005, 'grad_norm': 0.03863971307873726, 'learning_rate': 3.4440433212996396e-05, 'epoch': 1.25}
対照学習損失: 4.5371583837550133e-05
対照学習損失: 2.8927732273587026e-05
{'loss': 0.0, 'grad_norm': 0.0009636799804866314, 'learning_rate': 3.4422382671480146e-05, 'epoch': 1.25}
対照学習損失: 0.00013510766439139843
対照学習損失: 4.2501102143432945e-05
{'loss': 0.0001, 'grad_norm': 0.006029435433447361, 'learning_rate': 3.4404332129963903e-05, 'epoch': 1.25}
対照学習損失: 1.4147981346468441e-05
対照学習損失: 3.115603976766579e-05
{'loss': 0.0, 'grad_norm': 0.0010545813711360097, 'learning_rate': 3.4386281588447654e-05, 'epoch': 1.25}
対照学習損失: 0.001089585362933576
対照学習損失: 9.967897494789213e-05
{'loss': 0.0006, 'grad_norm': 0.07080529630184174, 'learning_rate': 3.436823104693141e-05, 'epoch': 1.25}
対照学習損失: 0.00011051454930566251
対照学習損失: 0.0008358964114449918
{'loss': 0.0005, 'grad_norm': 0.02247825264930725, 'learning_rate': 3.435018050541516e-05, 'epoch': 1.25}
対照学習損失: 1.631646227906458e-05
対照学習損失: 0.0008569927304051816
{'loss': 0.0004, 'grad_norm': 0.01985297165811062, 'learning_rate': 3.433212996389892e-05, 'epoch': 1.26}
対照学習損失: 5.9316585975466296e-05
対照学習損失: 3.360566552146338e-05
{'loss': 0.0, 'grad_norm': 0.0015362181002274156, 'learning_rate': 3.431407942238267e-05, 'epoch': 1.26}
対照学習損失: 0.00019603411783464253
対照学習損失: 2.5300085326307453e-05
{'loss': 0.0001, 'grad_norm': 0.0073378365486860275, 'learning_rate': 3.4296028880866426e-05, 'epoch': 1.26}
対照学習損失: 0.00040285277646034956
対照学習損失: 3.415121682337485e-05
{'loss': 0.0002, 'grad_norm': 0.026888979598879814, 'learning_rate': 3.4277978339350183e-05, 'epoch': 1.26}
対照学習損失: 1.4401663975149859e-05
対照学習損失: 3.152839053655043e-05
{'loss': 0.0, 'grad_norm': 0.0006537770386785269, 'learning_rate': 3.4259927797833934e-05, 'epoch': 1.26}
対照学習損失: 0.0003444660687819123
対照学習損失: 0.00015393886133097112
{'loss': 0.0002, 'grad_norm': 0.016668761149048805, 'learning_rate': 3.424187725631769e-05, 'epoch': 1.26}
対照学習損失: 0.0001456117315683514
対照学習損失: 6.957451114431024e-05
{'loss': 0.0001, 'grad_norm': 0.008666236884891987, 'learning_rate': 3.422382671480144e-05, 'epoch': 1.26}
対照学習損失: 1.691245415713638e-05
対照学習損失: 2.1999785531079397e-05
{'loss': 0.0, 'grad_norm': 0.0006958022713661194, 'learning_rate': 3.42057761732852e-05, 'epoch': 1.26}
対照学習損失: 6.45545223960653e-05
対照学習損失: 7.345089397858828e-05
{'loss': 0.0001, 'grad_norm': 0.0028823516331613064, 'learning_rate': 3.418772563176895e-05, 'epoch': 1.26}
対照学習損失: 6.39115969534032e-05
対照学習損失: 0.00017093161295633763
{'loss': 0.0001, 'grad_norm': 0.008918527513742447, 'learning_rate': 3.416967509025271e-05, 'epoch': 1.26}
対照学習損失: 2.197708818130195e-05
対照学習損失: 0.0009719489607959986
{'loss': 0.0005, 'grad_norm': 0.05572528764605522, 'learning_rate': 3.4151624548736464e-05, 'epoch': 1.26}
対照学習損失: 2.332660551473964e-05
対照学習損失: 4.6972523705335334e-05
{'loss': 0.0, 'grad_norm': 0.0012359542306512594, 'learning_rate': 3.413357400722022e-05, 'epoch': 1.27}
対照学習損失: 0.0003991341218352318
対照学習損失: 1.4848618775431532e-05
{'loss': 0.0002, 'grad_norm': 0.015641294419765472, 'learning_rate': 3.411552346570397e-05, 'epoch': 1.27}
対照学習損失: 6.0341473727021366e-05
対照学習損失: 3.875929905916564e-05
{'loss': 0.0, 'grad_norm': 0.002201278228312731, 'learning_rate': 3.409747292418773e-05, 'epoch': 1.27}
対照学習損失: 3.4067365049850196e-05
対照学習損失: 7.792731776135042e-05
{'loss': 0.0001, 'grad_norm': 0.0052561345510184765, 'learning_rate': 3.4079422382671486e-05, 'epoch': 1.27}
対照学習損失: 5.172467353986576e-05
対照学習損失: 3.482225292827934e-05
{'loss': 0.0, 'grad_norm': 0.00204763887450099, 'learning_rate': 3.4061371841155236e-05, 'epoch': 1.27}
対照学習損失: 1.0833010492206085e-05
対照学習損失: 9.731778845889494e-05
{'loss': 0.0001, 'grad_norm': 0.005723506677895784, 'learning_rate': 3.404332129963899e-05, 'epoch': 1.27}
対照学習損失: 0.00010822954209288582
対照学習損失: 2.9143622668925673e-05
{'loss': 0.0001, 'grad_norm': 0.002060388447716832, 'learning_rate': 3.4025270758122744e-05, 'epoch': 1.27}
対照学習損失: 1.173443888546899e-05
対照学習損失: 2.598622813820839e-05
{'loss': 0.0, 'grad_norm': 0.0006257860222831368, 'learning_rate': 3.40072202166065e-05, 'epoch': 1.27}
対照学習損失: 0.0002895927173085511
対照学習損失: 5.044022145739291e-06
{'loss': 0.0001, 'grad_norm': 0.009820531122386456, 'learning_rate': 3.398916967509025e-05, 'epoch': 1.27}
対照学習損失: 3.80289857275784e-05
対照学習損失: 2.6760773835121654e-05
{'loss': 0.0, 'grad_norm': 0.0020351263228803873, 'learning_rate': 3.397111913357401e-05, 'epoch': 1.27}
対照学習損失: 7.7088363468647e-05
対照学習損失: 0.00016992143355309963
{'loss': 0.0001, 'grad_norm': 0.004233662970364094, 'learning_rate': 3.3953068592057766e-05, 'epoch': 1.27}
対照学習損失: 2.8422709874575958e-05
対照学習損失: 6.193999433889985e-05
{'loss': 0.0, 'grad_norm': 0.0012575534638017416, 'learning_rate': 3.3935018050541516e-05, 'epoch': 1.28}
対照学習損失: 0.0010035235900431871
対照学習損失: 0.00015566436923108995
{'loss': 0.0006, 'grad_norm': 0.06063494086265564, 'learning_rate': 3.3916967509025273e-05, 'epoch': 1.28}
対照学習損失: 0.00012052987585775554
対照学習損失: 6.491965905297548e-05
{'loss': 0.0001, 'grad_norm': 0.0037767859175801277, 'learning_rate': 3.3898916967509024e-05, 'epoch': 1.28}
対照学習損失: 3.517063305480406e-05
対照学習損失: 3.587677929317579e-05
{'loss': 0.0, 'grad_norm': 0.0015659650089219213, 'learning_rate': 3.388086642599278e-05, 'epoch': 1.28}
対照学習損失: 1.7962020137929358e-05
対照学習損失: 6.499707524199039e-05
{'loss': 0.0, 'grad_norm': 0.003263308899477124, 'learning_rate': 3.386281588447653e-05, 'epoch': 1.28}
対照学習損失: 3.905261110048741e-05
対照学習損失: 3.4291475458303466e-05
{'loss': 0.0, 'grad_norm': 0.0013220623368397355, 'learning_rate': 3.384476534296029e-05, 'epoch': 1.28}
対照学習損失: 3.855012255371548e-05
対照学習損失: 5.234095442574471e-05
{'loss': 0.0, 'grad_norm': 0.002316092373803258, 'learning_rate': 3.3826714801444046e-05, 'epoch': 1.28}
対照学習損失: 9.283321560360491e-06
対照学習損失: 0.00013291249342728406
{'loss': 0.0001, 'grad_norm': 0.003483317792415619, 'learning_rate': 3.3808664259927796e-05, 'epoch': 1.28}
対照学習損失: 6.869388926133979e-06
対照学習損失: 3.599120827857405e-05
{'loss': 0.0, 'grad_norm': 0.0010983286192640662, 'learning_rate': 3.3790613718411553e-05, 'epoch': 1.28}
対照学習損失: 0.0001918877533171326
対照学習損失: 2.822176247718744e-05
{'loss': 0.0001, 'grad_norm': 0.010259093716740608, 'learning_rate': 3.377256317689531e-05, 'epoch': 1.28}
対照学習損失: 5.4003947298042476e-05
対照学習損失: 4.2319396015955135e-05
{'loss': 0.0, 'grad_norm': 0.003402471076697111, 'learning_rate': 3.375451263537907e-05, 'epoch': 1.28}
対照学習損失: 0.0011603273451328278
対照学習損失: 2.42731548496522e-05
{'loss': 0.0006, 'grad_norm': 0.024618474766612053, 'learning_rate': 3.373646209386282e-05, 'epoch': 1.29}
対照学習損失: 2.2222802726901136e-05
対照学習損失: 2.2424885173677467e-05
{'loss': 0.0, 'grad_norm': 0.0008244214695878327, 'learning_rate': 3.3718411552346576e-05, 'epoch': 1.29}
対照学習損失: 1.141414850280853e-05
対照学習損失: 0.00012523829354904592
{'loss': 0.0001, 'grad_norm': 0.0035771075636148453, 'learning_rate': 3.3700361010830326e-05, 'epoch': 1.29}
対照学習損失: 2.168038918171078e-05
対照学習損失: 1.8573045963421464e-05
{'loss': 0.0, 'grad_norm': 0.0005364943644963205, 'learning_rate': 3.368231046931408e-05, 'epoch': 1.29}
対照学習損失: 6.786057201679796e-05
対照学習損失: 2.1598214516416192e-05
{'loss': 0.0, 'grad_norm': 0.0018722268287092447, 'learning_rate': 3.3664259927797834e-05, 'epoch': 1.29}
対照学習損失: 4.1264822357334197e-05
対照学習損失: 2.3572454665554687e-05
{'loss': 0.0, 'grad_norm': 0.0011656665010377765, 'learning_rate': 3.364620938628159e-05, 'epoch': 1.29}
対照学習損失: 1.5444775272044353e-05
対照学習損失: 0.00011443386756582186
{'loss': 0.0001, 'grad_norm': 0.0022819479927420616, 'learning_rate': 3.362815884476535e-05, 'epoch': 1.29}
対照学習損失: 2.9934937629150227e-05
対照学習損失: 1.5586392692057416e-05
{'loss': 0.0, 'grad_norm': 0.0007045489619486034, 'learning_rate': 3.36101083032491e-05, 'epoch': 1.29}
対照学習損失: 7.946143887238577e-05
対照学習損失: 2.6380814233561978e-05
{'loss': 0.0001, 'grad_norm': 0.0020487315487116575, 'learning_rate': 3.3592057761732856e-05, 'epoch': 1.29}
対照学習損失: 6.364531873259693e-05
対照学習損失: 0.00023732881527394056
{'loss': 0.0002, 'grad_norm': 0.010572945699095726, 'learning_rate': 3.3574007220216606e-05, 'epoch': 1.29}
対照学習損失: 5.7294255384476855e-06
対照学習損失: 6.713390757795423e-05
{'loss': 0.0, 'grad_norm': 0.0011558703845366836, 'learning_rate': 3.355595667870036e-05, 'epoch': 1.29}
対照学習損失: 0.0001573331974213943
対照学習損失: 1.319447073910851e-05
{'loss': 0.0001, 'grad_norm': 0.004203311167657375, 'learning_rate': 3.3537906137184114e-05, 'epoch': 1.3}
対照学習損失: 2.9173874281696044e-05
対照学習損失: 3.718770312843844e-05
{'loss': 0.0, 'grad_norm': 0.0014220545999705791, 'learning_rate': 3.351985559566787e-05, 'epoch': 1.3}
対照学習損失: 8.743161015445367e-05
対照学習損失: 1.0684033441066276e-05
{'loss': 0.0, 'grad_norm': 0.0023129251785576344, 'learning_rate': 3.350180505415163e-05, 'epoch': 1.3}
対照学習損失: 0.00026178339612670243
対照学習損失: 8.491090557072312e-05
{'loss': 0.0002, 'grad_norm': 0.01010625809431076, 'learning_rate': 3.348375451263538e-05, 'epoch': 1.3}
対照学習損失: 0.0002119029959430918
対照学習損失: 1.6420792235294357e-05
{'loss': 0.0001, 'grad_norm': 0.012078335508704185, 'learning_rate': 3.3465703971119136e-05, 'epoch': 1.3}
対照学習損失: 2.4317165298270993e-05
対照学習損失: 0.003557480638846755
{'loss': 0.0018, 'grad_norm': 0.10214639455080032, 'learning_rate': 3.3447653429602886e-05, 'epoch': 1.3}
対照学習損失: 3.9081838622223586e-05
対照学習損失: 3.996024315711111e-05
{'loss': 0.0, 'grad_norm': 0.0015985548961907625, 'learning_rate': 3.3429602888086643e-05, 'epoch': 1.3}
対照学習損失: 2.54189180850517e-05
対照学習損失: 2.6604142476571724e-05
{'loss': 0.0, 'grad_norm': 0.0009999183239415288, 'learning_rate': 3.3411552346570394e-05, 'epoch': 1.3}
対照学習損失: 3.216905315639451e-05
対照学習損失: 3.435946928220801e-05
{'loss': 0.0, 'grad_norm': 0.0010583807015791535, 'learning_rate': 3.339350180505416e-05, 'epoch': 1.3}
対照学習損失: 0.0007581462850794196
対照学習損失: 7.8888340794947e-05
{'loss': 0.0004, 'grad_norm': 0.027671074494719505, 'learning_rate': 3.337545126353791e-05, 'epoch': 1.3}
対照学習損失: 2.6207544578937814e-05
対照学習損失: 0.00013527445844374597
{'loss': 0.0001, 'grad_norm': 0.005018650088459253, 'learning_rate': 3.3357400722021665e-05, 'epoch': 1.3}
対照学習損失: 3.763077256735414e-05
対照学習損失: 3.8632930227322504e-05
{'loss': 0.0, 'grad_norm': 0.0017256110440939665, 'learning_rate': 3.3339350180505416e-05, 'epoch': 1.31}
対照学習損失: 7.087147969286889e-05
対照学習損失: 0.0003086984797846526
{'loss': 0.0002, 'grad_norm': 0.0072534456849098206, 'learning_rate': 3.332129963898917e-05, 'epoch': 1.31}
対照学習損失: 1.8357524822931737e-05
対照学習損失: 6.685459084110335e-05
{'loss': 0.0, 'grad_norm': 0.0025447718799114227, 'learning_rate': 3.330324909747293e-05, 'epoch': 1.31}
対照学習損失: 0.0001985585258807987
対照学習損失: 0.00044170182081870735
{'loss': 0.0003, 'grad_norm': 0.013769187964498997, 'learning_rate': 3.328519855595668e-05, 'epoch': 1.31}
対照学習損失: 0.0001884465164039284
対照学習損失: 1.0907429896178655e-05
{'loss': 0.0001, 'grad_norm': 0.005230453796684742, 'learning_rate': 3.326714801444044e-05, 'epoch': 1.31}
対照学習損失: 1.9504981537465937e-05
対照学習損失: 2.9644214009749703e-05
{'loss': 0.0, 'grad_norm': 0.0008190549560822546, 'learning_rate': 3.324909747292419e-05, 'epoch': 1.31}
対照学習損失: 2.4220586055889726e-05
対照学習損失: 5.7212560932384804e-05
{'loss': 0.0, 'grad_norm': 0.001716572092846036, 'learning_rate': 3.3231046931407946e-05, 'epoch': 1.31}
対照学習損失: 5.7458961236989126e-05
対照学習損失: 2.5822111638262868e-05
{'loss': 0.0, 'grad_norm': 0.0013030733680352569, 'learning_rate': 3.3212996389891696e-05, 'epoch': 1.31}
対照学習損失: 0.00041426223469898105
対照学習損失: 0.0014080419205129147
{'loss': 0.0009, 'grad_norm': 0.07663889974355698, 'learning_rate': 3.319494584837545e-05, 'epoch': 1.31}
対照学習損失: 0.00015003189037088305
対照学習損失: 3.663089592009783e-05
{'loss': 0.0001, 'grad_norm': 0.00354992737993598, 'learning_rate': 3.317689530685921e-05, 'epoch': 1.31}
対照学習損失: 2.826637137332e-05
対照学習損失: 3.5544078855309635e-05
{'loss': 0.0, 'grad_norm': 0.0010673439828678966, 'learning_rate': 3.315884476534296e-05, 'epoch': 1.31}
対照学習損失: 1.6159909137059003e-05
対照学習損失: 7.36032598069869e-05
{'loss': 0.0, 'grad_norm': 0.002641006838530302, 'learning_rate': 3.314079422382672e-05, 'epoch': 1.32}
対照学習損失: 7.504117820644751e-05
対照学習損失: 3.881805969285779e-05
{'loss': 0.0001, 'grad_norm': 0.003327748039737344, 'learning_rate': 3.312274368231047e-05, 'epoch': 1.32}
対照学習損失: 0.0001421090855728835
対照学習損失: 0.00010044407827081159
{'loss': 0.0001, 'grad_norm': 0.0073929536156356335, 'learning_rate': 3.3104693140794226e-05, 'epoch': 1.32}
対照学習損失: 0.0006964309141039848
対照学習損失: 0.0006186552345752716
{'loss': 0.0007, 'grad_norm': 0.048288874328136444, 'learning_rate': 3.3086642599277976e-05, 'epoch': 1.32}
対照学習損失: 3.28746136801783e-05
対照学習損失: 0.0002933179493993521
{'loss': 0.0002, 'grad_norm': 0.009376676753163338, 'learning_rate': 3.306859205776173e-05, 'epoch': 1.32}
対照学習損失: 6.526666766148992e-06
対照学習損失: 6.236067747522611e-06
{'loss': 0.0, 'grad_norm': 0.00017610163195058703, 'learning_rate': 3.3050541516245484e-05, 'epoch': 1.32}
対照学習損失: 0.00030555101693607867
対照学習損失: 2.6679532311391085e-05
{'loss': 0.0002, 'grad_norm': 0.01904795877635479, 'learning_rate': 3.303249097472924e-05, 'epoch': 1.32}
対照学習損失: 1.0743611710495315e-05
対照学習損失: 1.8141679902328178e-05
{'loss': 0.0, 'grad_norm': 0.0003416274848859757, 'learning_rate': 3.3014440433213e-05, 'epoch': 1.32}
対照学習損失: 1.1525695299496874e-05
対照学習損失: 7.090305734891444e-05
{'loss': 0.0, 'grad_norm': 0.0012972448021173477, 'learning_rate': 3.2996389891696755e-05, 'epoch': 1.32}
対照学習損失: 9.26715656532906e-05
対照学習損失: 0.0002257324813399464
{'loss': 0.0002, 'grad_norm': 0.006018938962370157, 'learning_rate': 3.297833935018051e-05, 'epoch': 1.32}
対照学習損失: 4.715583781944588e-05
対照学習損失: 3.455399928498082e-05
{'loss': 0.0, 'grad_norm': 0.0014196248957887292, 'learning_rate': 3.296028880866426e-05, 'epoch': 1.33}
対照学習損失: 1.330629220319679e-05
対照学習損失: 0.00021164627105463296
{'loss': 0.0001, 'grad_norm': 0.015714464709162712, 'learning_rate': 3.294223826714802e-05, 'epoch': 1.33}
対照学習損失: 1.5042232917039655e-05
対照学習損失: 0.000392870424548164
{'loss': 0.0002, 'grad_norm': 0.024931568652391434, 'learning_rate': 3.292418772563177e-05, 'epoch': 1.33}
対照学習損失: 0.00018664372328203171
対照学習損失: 0.004438686650246382
{'loss': 0.0023, 'grad_norm': 0.2558976113796234, 'learning_rate': 3.290613718411553e-05, 'epoch': 1.33}
対照学習損失: 2.2358457499649376e-05
対照学習損失: 2.9055501727270894e-05
{'loss': 0.0, 'grad_norm': 0.0010441950289532542, 'learning_rate': 3.288808664259928e-05, 'epoch': 1.33}
対照学習損失: 0.0006310271564871073
対照学習損失: 5.7170229410985485e-05
{'loss': 0.0003, 'grad_norm': 0.048713721334934235, 'learning_rate': 3.2870036101083035e-05, 'epoch': 1.33}
対照学習損失: 2.12927334359847e-05
対照学習損失: 0.0008114437805488706
{'loss': 0.0004, 'grad_norm': 0.03607035428285599, 'learning_rate': 3.2851985559566786e-05, 'epoch': 1.33}
対照学習損失: 2.6478115614736453e-05
対照学習損失: 2.7580008463701233e-05
{'loss': 0.0, 'grad_norm': 0.0008726160740479827, 'learning_rate': 3.283393501805054e-05, 'epoch': 1.33}
対照学習損失: 1.5847042959649116e-05
対照学習損失: 2.4354161723749712e-05
{'loss': 0.0, 'grad_norm': 0.0006132860435172915, 'learning_rate': 3.28158844765343e-05, 'epoch': 1.33}
対照学習損失: 6.5266522142337635e-06
対照学習損失: 6.338853563647717e-05
{'loss': 0.0, 'grad_norm': 0.0018219905905425549, 'learning_rate': 3.279783393501805e-05, 'epoch': 1.33}
対照学習損失: 4.893997174804099e-05
対照学習損失: 2.351118018850684e-05
{'loss': 0.0, 'grad_norm': 0.0025296376552432775, 'learning_rate': 3.277978339350181e-05, 'epoch': 1.33}
対照学習損失: 5.990234058117494e-06
対照学習損失: 1.187587076856289e-05
{'loss': 0.0, 'grad_norm': 0.0002731196000240743, 'learning_rate': 3.276173285198556e-05, 'epoch': 1.34}
対照学習損失: 3.424737951718271e-05
対照学習損失: 0.0007290341891348362
{'loss': 0.0004, 'grad_norm': 0.03583730757236481, 'learning_rate': 3.2743682310469316e-05, 'epoch': 1.34}
対照学習損失: 5.2243543905206025e-05
対照学習損失: 0.0001867267128545791
{'loss': 0.0001, 'grad_norm': 0.01581430993974209, 'learning_rate': 3.2725631768953066e-05, 'epoch': 1.34}
対照学習損失: 1.225590858666692e-05
対照学習損失: 0.0007993524195626378
{'loss': 0.0004, 'grad_norm': 0.08677490055561066, 'learning_rate': 3.270758122743682e-05, 'epoch': 1.34}
対照学習損失: 3.6816858482779935e-05
対照学習損失: 2.0666666387114674e-05
{'loss': 0.0, 'grad_norm': 0.0013811327517032623, 'learning_rate': 3.268953068592058e-05, 'epoch': 1.34}
対照学習損失: 8.031604920688551e-06
対照学習損失: 0.012897967360913754
{'loss': 0.0065, 'grad_norm': 0.6279799938201904, 'learning_rate': 3.267148014440433e-05, 'epoch': 1.34}
対照学習損失: 3.416635809116997e-05
対照学習損失: 3.958417073590681e-05
{'loss': 0.0, 'grad_norm': 0.0015644690720364451, 'learning_rate': 3.265342960288809e-05, 'epoch': 1.34}
対照学習損失: 8.356064063264057e-05
対照学習損失: 0.0007807452930137515
{'loss': 0.0004, 'grad_norm': 0.04081563651561737, 'learning_rate': 3.263537906137184e-05, 'epoch': 1.34}
対照学習損失: 0.0002154268731828779
対照学習損失: 0.00011247115617152303
{'loss': 0.0002, 'grad_norm': 0.0072529129683971405, 'learning_rate': 3.2617328519855596e-05, 'epoch': 1.34}
対照学習損失: 0.0004780387389473617
対照学習損失: 2.9316337531781755e-05
{'loss': 0.0003, 'grad_norm': 0.012082681059837341, 'learning_rate': 3.259927797833935e-05, 'epoch': 1.34}
対照学習損失: 1.3023458450334147e-05
対照学習損失: 1.3589617083198391e-05
{'loss': 0.0, 'grad_norm': 0.0005003802361898124, 'learning_rate': 3.258122743682311e-05, 'epoch': 1.34}
対照学習損失: 0.0012246114201843739
対照学習損失: 4.105269181309268e-05
{'loss': 0.0006, 'grad_norm': 0.02915022149682045, 'learning_rate': 3.256317689530686e-05, 'epoch': 1.35}
対照学習損失: 2.8063368517905474e-05
対照学習損失: 0.0001676930405665189
{'loss': 0.0001, 'grad_norm': 0.004329941235482693, 'learning_rate': 3.254512635379062e-05, 'epoch': 1.35}
対照学習損失: 9.918950672727078e-05
対照学習損失: 1.3231864613771904e-05
{'loss': 0.0001, 'grad_norm': 0.0036831209436059, 'learning_rate': 3.252707581227437e-05, 'epoch': 1.35}
対照学習損失: 6.928984930709703e-06
対照学習損失: 2.8571597795234993e-05
{'loss': 0.0, 'grad_norm': 0.001069606514647603, 'learning_rate': 3.2509025270758125e-05, 'epoch': 1.35}
対照学習損失: 2.415205744910054e-05
対照学習損失: 0.0004686993779614568
{'loss': 0.0002, 'grad_norm': 0.016809538006782532, 'learning_rate': 3.249097472924188e-05, 'epoch': 1.35}
対照学習損失: 9.325108112534508e-05
対照学習損失: 0.0006530790124088526
{'loss': 0.0004, 'grad_norm': 0.02104990929365158, 'learning_rate': 3.247292418772563e-05, 'epoch': 1.35}
対照学習損失: 0.0012401980347931385
対照学習損失: 4.7291003284044564e-05
{'loss': 0.0006, 'grad_norm': 0.08974843472242355, 'learning_rate': 3.245487364620939e-05, 'epoch': 1.35}
対照学習損失: 0.0005404488183557987
対照学習損失: 6.698660581605509e-05
{'loss': 0.0003, 'grad_norm': 0.05065053328871727, 'learning_rate': 3.243682310469314e-05, 'epoch': 1.35}
対照学習損失: 0.0010081633226945996
対照学習損失: 6.959230813663453e-05
{'loss': 0.0005, 'grad_norm': 0.08100245893001556, 'learning_rate': 3.24187725631769e-05, 'epoch': 1.35}
対照学習損失: 0.0007107407436706126
対照学習損失: 0.014895739033818245
{'loss': 0.0078, 'grad_norm': 0.40047910809516907, 'learning_rate': 3.240072202166065e-05, 'epoch': 1.35}
対照学習損失: 0.0017790686106309295
対照学習損失: 4.267261101631448e-05
{'loss': 0.0009, 'grad_norm': 0.14977534115314484, 'learning_rate': 3.2382671480144405e-05, 'epoch': 1.35}
対照学習損失: 0.00119405472651124
対照学習損失: 0.0006562916678376496
{'loss': 0.0009, 'grad_norm': 0.07408694922924042, 'learning_rate': 3.236462093862816e-05, 'epoch': 1.36}
対照学習損失: 1.9847835574182682e-05
対照学習損失: 1.9423285266384482e-05
{'loss': 0.0, 'grad_norm': 0.0006026140181347728, 'learning_rate': 3.234657039711191e-05, 'epoch': 1.36}
対照学習損失: 9.803519060369581e-05
対照学習損失: 0.0004362001782283187
{'loss': 0.0003, 'grad_norm': 0.009385920129716396, 'learning_rate': 3.232851985559567e-05, 'epoch': 1.36}
対照学習損失: 0.00029224081663414836
対照学習損失: 0.0005355286994017661
{'loss': 0.0004, 'grad_norm': 0.04197124391794205, 'learning_rate': 3.231046931407942e-05, 'epoch': 1.36}
対照学習損失: 1.361944669042714e-05
対照学習損失: 8.235807035816833e-05
{'loss': 0.0, 'grad_norm': 0.00253188400529325, 'learning_rate': 3.229241877256318e-05, 'epoch': 1.36}
対照学習損失: 0.0002269165124744177
対照学習損失: 0.00025419198209419847
{'loss': 0.0002, 'grad_norm': 0.02522333711385727, 'learning_rate': 3.227436823104693e-05, 'epoch': 1.36}
対照学習損失: 2.699172000575345e-05
対照学習損失: 7.59207978262566e-06
{'loss': 0.0, 'grad_norm': 0.00048817237257026136, 'learning_rate': 3.2256317689530686e-05, 'epoch': 1.36}
対照学習損失: 2.1970165107632056e-05
対照学習損失: 5.175775004317984e-05
{'loss': 0.0, 'grad_norm': 0.001258531934581697, 'learning_rate': 3.223826714801444e-05, 'epoch': 1.36}
対照学習損失: 4.313624958740547e-05
対照学習損失: 4.138334043091163e-05
{'loss': 0.0, 'grad_norm': 0.0020362739451229572, 'learning_rate': 3.22202166064982e-05, 'epoch': 1.36}
対照学習損失: 1.8178978280047886e-05
対照学習損失: 0.0001161702093668282
{'loss': 0.0001, 'grad_norm': 0.003754111472517252, 'learning_rate': 3.220216606498195e-05, 'epoch': 1.36}
対照学習損失: 0.00016033182328101248
対照学習損失: 0.003006173297762871
{'loss': 0.0016, 'grad_norm': 0.13251365721225739, 'learning_rate': 3.218411552346571e-05, 'epoch': 1.36}
対照学習損失: 1.2270922525203787e-05
対照学習損失: 9.865165338851511e-05
{'loss': 0.0001, 'grad_norm': 0.00316001009196043, 'learning_rate': 3.2166064981949465e-05, 'epoch': 1.37}
対照学習損失: 0.00027000883710570633
対照学習損失: 5.069050894235261e-05
{'loss': 0.0002, 'grad_norm': 0.011102946475148201, 'learning_rate': 3.2148014440433215e-05, 'epoch': 1.37}
対照学習損失: 7.778316103213001e-06
対照学習損失: 2.1442199795274064e-05
{'loss': 0.0, 'grad_norm': 0.0007994677289389074, 'learning_rate': 3.212996389891697e-05, 'epoch': 1.37}
対照学習損失: 1.7031557945301756e-05
対照学習損失: 2.8564505555550568e-05
{'loss': 0.0, 'grad_norm': 0.0005855445051565766, 'learning_rate': 3.211191335740072e-05, 'epoch': 1.37}
対照学習損失: 0.0019068499095737934
対照学習損失: 2.273084464832209e-05
{'loss': 0.001, 'grad_norm': 0.05611211806535721, 'learning_rate': 3.209386281588448e-05, 'epoch': 1.37}
対照学習損失: 1.812685513868928e-05
対照学習損失: 2.1404573999461718e-05
{'loss': 0.0, 'grad_norm': 0.00044663544395007193, 'learning_rate': 3.207581227436823e-05, 'epoch': 1.37}
対照学習損失: 0.013133802451193333
対照学習損失: 2.5501656637061387e-05
{'loss': 0.0066, 'grad_norm': 0.3550415635108948, 'learning_rate': 3.205776173285199e-05, 'epoch': 1.37}
対照学習損失: 0.0028054476715624332
対照学習損失: 2.3907909053377807e-05
{'loss': 0.0014, 'grad_norm': 0.08209219574928284, 'learning_rate': 3.2039711191335745e-05, 'epoch': 1.37}
対照学習損失: 6.307861622190103e-05
対照学習損失: 7.487775292247534e-06
{'loss': 0.0, 'grad_norm': 0.004381247796118259, 'learning_rate': 3.2021660649819495e-05, 'epoch': 1.37}
対照学習損失: 0.0035800111945718527
対照学習損失: 1.7410175132681616e-05
{'loss': 0.0018, 'grad_norm': 0.32105305790901184, 'learning_rate': 3.200361010830325e-05, 'epoch': 1.37}
対照学習損失: 1.276985858567059e-05
対照学習損失: 2.794582542264834e-05
{'loss': 0.0, 'grad_norm': 0.0005217092111706734, 'learning_rate': 3.1985559566787e-05, 'epoch': 1.37}
対照学習損失: 0.0002147747145500034
対照学習損失: 9.231314470525831e-05
{'loss': 0.0002, 'grad_norm': 0.006673726718872786, 'learning_rate': 3.196750902527076e-05, 'epoch': 1.38}
対照学習損失: 0.005351248662918806
対照学習損失: 2.077137105516158e-05
{'loss': 0.0027, 'grad_norm': 0.1439739465713501, 'learning_rate': 3.194945848375451e-05, 'epoch': 1.38}
対照学習損失: 0.0001537241623736918
対照学習損失: 1.1481191904749721e-05
{'loss': 0.0001, 'grad_norm': 0.003727234899997711, 'learning_rate': 3.193140794223827e-05, 'epoch': 1.38}
対照学習損失: 3.0081933800829574e-05
対照学習損失: 0.00021255420870147645
{'loss': 0.0001, 'grad_norm': 0.005725590977817774, 'learning_rate': 3.191335740072202e-05, 'epoch': 1.38}
対照学習損失: 1.9154680558131076e-05
対照学習損失: 7.397292938549072e-05
{'loss': 0.0, 'grad_norm': 0.0017982561839744449, 'learning_rate': 3.1895306859205775e-05, 'epoch': 1.38}
対照学習損失: 1.4275028661359102e-05
対照学習損失: 8.808603161014616e-05
{'loss': 0.0001, 'grad_norm': 0.002219622489064932, 'learning_rate': 3.187725631768953e-05, 'epoch': 1.38}
対照学習損失: 2.6224670364172198e-05
対照学習損失: 0.0032891137525439262
{'loss': 0.0017, 'grad_norm': 0.07503755390644073, 'learning_rate': 3.185920577617328e-05, 'epoch': 1.38}
対照学習損失: 1.6658983440720476e-05
対照学習損失: 8.992760740511585e-06
{'loss': 0.0, 'grad_norm': 0.00034568304545246065, 'learning_rate': 3.184115523465704e-05, 'epoch': 1.38}
対照学習損失: 3.468737850198522e-05
対照学習損失: 4.947160960000474e-06
{'loss': 0.0, 'grad_norm': 0.0006445649196393788, 'learning_rate': 3.18231046931408e-05, 'epoch': 1.38}
対照学習損失: 1.3798173313261941e-05
対照学習損失: 0.00029389900737442076
{'loss': 0.0002, 'grad_norm': 0.010339179076254368, 'learning_rate': 3.1805054151624555e-05, 'epoch': 1.38}
対照学習損失: 2.8370159270707518e-05
対照学習損失: 5.992281512590125e-05
{'loss': 0.0, 'grad_norm': 0.002052758354693651, 'learning_rate': 3.1787003610108305e-05, 'epoch': 1.38}
対照学習損失: 5.683311610482633e-05
対照学習損失: 9.427557961316779e-05
{'loss': 0.0001, 'grad_norm': 0.007993679493665695, 'learning_rate': 3.176895306859206e-05, 'epoch': 1.39}
対照学習損失: 6.0810125432908535e-05
対照学習損失: 1.387273096042918e-05
{'loss': 0.0, 'grad_norm': 0.0013565643457695842, 'learning_rate': 3.175090252707581e-05, 'epoch': 1.39}
対照学習損失: 3.390459096408449e-05
対照学習損失: 4.344107583165169e-05
{'loss': 0.0, 'grad_norm': 0.001358647015877068, 'learning_rate': 3.173285198555957e-05, 'epoch': 1.39}
対照学習損失: 3.1601914088241756e-05
対照学習損失: 2.7573731131269597e-05
{'loss': 0.0, 'grad_norm': 0.002380096586421132, 'learning_rate': 3.171480144404333e-05, 'epoch': 1.39}
対照学習損失: 1.3373539331951179e-05
対照学習損失: 6.08280606684275e-05
{'loss': 0.0, 'grad_norm': 0.0010824872879311442, 'learning_rate': 3.169675090252708e-05, 'epoch': 1.39}
対照学習損失: 1.5474186511710286e-05
対照学習損失: 2.0815650714212097e-05
{'loss': 0.0, 'grad_norm': 0.0007245726301334798, 'learning_rate': 3.1678700361010835e-05, 'epoch': 1.39}
対照学習損失: 0.0002206703502452001
対照学習損失: 2.0294226487749256e-05
{'loss': 0.0001, 'grad_norm': 0.011320899240672588, 'learning_rate': 3.1660649819494585e-05, 'epoch': 1.39}
対照学習損失: 3.3070260542444885e-05
対照学習損失: 0.003401084803044796
{'loss': 0.0017, 'grad_norm': 0.22420333325862885, 'learning_rate': 3.164259927797834e-05, 'epoch': 1.39}
対照学習損失: 8.46478360472247e-05
対照学習損失: 0.0001038871196215041
{'loss': 0.0001, 'grad_norm': 0.007635063491761684, 'learning_rate': 3.162454873646209e-05, 'epoch': 1.39}
対照学習損失: 9.027335909195244e-05
対照学習損失: 8.093784708762541e-05
{'loss': 0.0001, 'grad_norm': 0.005508871749043465, 'learning_rate': 3.160649819494585e-05, 'epoch': 1.39}
対照学習損失: 1.2539157978608273e-05
対照学習損失: 0.00011590731446631253
{'loss': 0.0001, 'grad_norm': 0.003437114180997014, 'learning_rate': 3.15884476534296e-05, 'epoch': 1.39}
対照学習損失: 2.7178142772754654e-05
対照学習損失: 0.00026294030249118805
{'loss': 0.0001, 'grad_norm': 0.008080629631876945, 'learning_rate': 3.157039711191336e-05, 'epoch': 1.4}
対照学習損失: 2.711921843001619e-05
対照学習損失: 1.1801308573922142e-05
{'loss': 0.0, 'grad_norm': 0.0004096474149264395, 'learning_rate': 3.1552346570397115e-05, 'epoch': 1.4}
対照学習損失: 0.00011294762225588784
対照学習損失: 0.00014397491759154946
{'loss': 0.0001, 'grad_norm': 0.004272310994565487, 'learning_rate': 3.1534296028880865e-05, 'epoch': 1.4}
対照学習損失: 2.017537917708978e-05
対照学習損失: 1.211448307003593e-05
{'loss': 0.0, 'grad_norm': 0.0004559776571113616, 'learning_rate': 3.151624548736462e-05, 'epoch': 1.4}
対照学習損失: 0.0005005437415093184
対照学習損失: 4.937093035550788e-05
{'loss': 0.0003, 'grad_norm': 0.01373789831995964, 'learning_rate': 3.149819494584837e-05, 'epoch': 1.4}
対照学習損失: 2.0033943656017072e-05
対照学習損失: 1.8014819943346083e-05
{'loss': 0.0, 'grad_norm': 0.0005477591184899211, 'learning_rate': 3.148014440433213e-05, 'epoch': 1.4}
対照学習損失: 2.8369859137455933e-05
対照学習損失: 1.4423917491512839e-05
{'loss': 0.0, 'grad_norm': 0.0005759031046181917, 'learning_rate': 3.146209386281588e-05, 'epoch': 1.4}
対照学習損失: 0.00028790434589609504
対照学習損失: 7.71271443227306e-05
{'loss': 0.0002, 'grad_norm': 0.00807639118283987, 'learning_rate': 3.1444043321299645e-05, 'epoch': 1.4}
対照学習損失: 2.1523730538319796e-05
対照学習損失: 8.637719292892143e-05
{'loss': 0.0001, 'grad_norm': 0.004057610407471657, 'learning_rate': 3.1425992779783395e-05, 'epoch': 1.4}
対照学習損失: 3.43669526046142e-05
対照学習損失: 0.00013071842840872705
{'loss': 0.0001, 'grad_norm': 0.0027999153826385736, 'learning_rate': 3.140794223826715e-05, 'epoch': 1.4}
対照学習損失: 3.88725457014516e-05
対照学習損失: 2.9002572773606516e-05
{'loss': 0.0, 'grad_norm': 0.00165441888384521, 'learning_rate': 3.13898916967509e-05, 'epoch': 1.4}
対照学習損失: 0.0002995928516611457
対照学習損失: 8.419076948484872e-06
{'loss': 0.0002, 'grad_norm': 0.007650808431208134, 'learning_rate': 3.137184115523466e-05, 'epoch': 1.41}
対照学習損失: 4.810398240806535e-05
対照学習損失: 0.00016654162027407438
{'loss': 0.0001, 'grad_norm': 0.006135079078376293, 'learning_rate': 3.135379061371842e-05, 'epoch': 1.41}
対照学習損失: 1.801497273845598e-05
対照学習損失: 7.972386083565652e-05
{'loss': 0.0, 'grad_norm': 0.0030065434984862804, 'learning_rate': 3.133574007220217e-05, 'epoch': 1.41}
対照学習損失: 6.411629874492064e-05
対照学習損失: 0.0002683859202079475
{'loss': 0.0002, 'grad_norm': 0.006726694758981466, 'learning_rate': 3.1317689530685925e-05, 'epoch': 1.41}
対照学習損失: 0.0038502775132656097
対照学習損失: 1.6599344235146418e-05
{'loss': 0.0019, 'grad_norm': 0.11347334086894989, 'learning_rate': 3.1299638989169675e-05, 'epoch': 1.41}
対照学習損失: 2.0801320715690963e-05
対照学習損失: 2.0450319425435737e-05
{'loss': 0.0, 'grad_norm': 0.0007502626394852996, 'learning_rate': 3.128158844765343e-05, 'epoch': 1.41}
対照学習損失: 1.922908631968312e-05
対照学習損失: 2.8951115382369608e-05
{'loss': 0.0, 'grad_norm': 0.0007849234389141202, 'learning_rate': 3.126353790613718e-05, 'epoch': 1.41}
対照学習損失: 0.0006032289820723236
対照学習損失: 2.1330444724299014e-05
{'loss': 0.0003, 'grad_norm': 0.013844450004398823, 'learning_rate': 3.124548736462094e-05, 'epoch': 1.41}
対照学習損失: 0.0004624244465958327
対照学習損失: 3.20694744004868e-05
{'loss': 0.0002, 'grad_norm': 0.015091637149453163, 'learning_rate': 3.12274368231047e-05, 'epoch': 1.41}
対照学習損失: 1.1048915439459961e-05
対照学習損失: 7.510076102335006e-06
{'loss': 0.0, 'grad_norm': 0.00035830060369335115, 'learning_rate': 3.120938628158845e-05, 'epoch': 1.41}
対照学習損失: 4.429741238709539e-05
対照学習損失: 1.7135926100309007e-05
{'loss': 0.0, 'grad_norm': 0.0007310114451684058, 'learning_rate': 3.1191335740072205e-05, 'epoch': 1.41}
対照学習損失: 7.515624747611582e-05
対照学習損失: 8.806488949630875e-06
{'loss': 0.0, 'grad_norm': 0.006212686654180288, 'learning_rate': 3.1173285198555955e-05, 'epoch': 1.42}
対照学習損失: 0.0005400817608460784
対照学習損失: 6.400795246008784e-05
{'loss': 0.0003, 'grad_norm': 0.0264198686927557, 'learning_rate': 3.115523465703971e-05, 'epoch': 1.42}
対照学習損失: 4.536504275165498e-05
対照学習損失: 3.911045496352017e-05
{'loss': 0.0, 'grad_norm': 0.0019504011142998934, 'learning_rate': 3.113718411552346e-05, 'epoch': 1.42}
対照学習損失: 0.00017790270794648677
対照学習損失: 2.674597635632381e-05
{'loss': 0.0001, 'grad_norm': 0.015129080042243004, 'learning_rate': 3.111913357400722e-05, 'epoch': 1.42}
対照学習損失: 3.222215309506282e-05
対照学習損失: 0.00011090207408415154
{'loss': 0.0001, 'grad_norm': 0.0033552006352692842, 'learning_rate': 3.110108303249098e-05, 'epoch': 1.42}
対照学習損失: 1.24273401524988e-05
対照学習損失: 2.0011364540550858e-05
{'loss': 0.0, 'grad_norm': 0.0007598305819556117, 'learning_rate': 3.108303249097473e-05, 'epoch': 1.42}
対照学習損失: 5.101191345602274e-05
対照学習損失: 7.94222614786122e-06
{'loss': 0.0, 'grad_norm': 0.0015266683185473084, 'learning_rate': 3.1064981949458485e-05, 'epoch': 1.42}
対照学習損失: 7.79847614467144e-05
対照学習損失: 0.0001782175386324525
{'loss': 0.0001, 'grad_norm': 0.005716443061828613, 'learning_rate': 3.104693140794224e-05, 'epoch': 1.42}
対照学習損失: 9.576295269653201e-05
対照学習損失: 1.1034178896807134e-05
{'loss': 0.0001, 'grad_norm': 0.002451530424878001, 'learning_rate': 3.1028880866426e-05, 'epoch': 1.42}
対照学習損失: 2.0956877051503398e-05
対照学習損失: 0.0002904442953877151
{'loss': 0.0002, 'grad_norm': 0.005486387759447098, 'learning_rate': 3.101083032490975e-05, 'epoch': 1.42}
対照学習損失: 7.757517596473917e-05
対照学習損失: 2.556806430220604e-05
{'loss': 0.0001, 'grad_norm': 0.0031537250615656376, 'learning_rate': 3.099277978339351e-05, 'epoch': 1.43}
対照学習損失: 7.635194924660027e-05
対照学習損失: 2.395286355749704e-05
{'loss': 0.0001, 'grad_norm': 0.004478243179619312, 'learning_rate': 3.097472924187726e-05, 'epoch': 1.43}
対照学習損失: 1.8134287529392168e-05
対照学習損失: 1.5236113540595397e-05
{'loss': 0.0, 'grad_norm': 0.0004392066039144993, 'learning_rate': 3.0956678700361015e-05, 'epoch': 1.43}
対照学習損失: 3.699640365084633e-05
対照学習損失: 1.5034916032163892e-05
{'loss': 0.0, 'grad_norm': 0.0012805568985641003, 'learning_rate': 3.0938628158844765e-05, 'epoch': 1.43}
対照学習損失: 1.1600423022173345e-05
対照学習損失: 9.026440238812938e-05
{'loss': 0.0001, 'grad_norm': 0.003945610485970974, 'learning_rate': 3.092057761732852e-05, 'epoch': 1.43}
対照学習損失: 3.339868999319151e-05
対照学習損失: 3.8350124668795615e-05
{'loss': 0.0, 'grad_norm': 0.0011012284085154533, 'learning_rate': 3.090252707581228e-05, 'epoch': 1.43}
対照学習損失: 1.5087098290678114e-05
対照学習損失: 1.2524191333795898e-05
{'loss': 0.0, 'grad_norm': 0.0002988688356708735, 'learning_rate': 3.088447653429603e-05, 'epoch': 1.43}
対照学習損失: 0.00017420563381165266
対照学習損失: 4.4272401282796636e-05
{'loss': 0.0001, 'grad_norm': 0.004121433012187481, 'learning_rate': 3.086642599277979e-05, 'epoch': 1.43}
対照学習損失: 4.228429315844551e-05
対照学習損失: 3.467065835138783e-05
{'loss': 0.0, 'grad_norm': 0.0024885486345738173, 'learning_rate': 3.084837545126354e-05, 'epoch': 1.43}
対照学習損失: 5.833778686792357e-06
対照学習損失: 9.186378520098515e-06
{'loss': 0.0, 'grad_norm': 0.00020041567040607333, 'learning_rate': 3.0830324909747295e-05, 'epoch': 1.43}
対照学習損失: 2.1255247702356428e-05
対照学習損失: 1.3932174624642357e-05
{'loss': 0.0, 'grad_norm': 0.0008996090036816895, 'learning_rate': 3.0812274368231045e-05, 'epoch': 1.43}
対照学習損失: 2.2737778635928407e-05
対照学習損失: 2.1560286768362857e-05
{'loss': 0.0, 'grad_norm': 0.0008158897398971021, 'learning_rate': 3.07942238267148e-05, 'epoch': 1.44}
対照学習損失: 2.1084302716189995e-05
対照学習損失: 1.0177373042097315e-05
{'loss': 0.0, 'grad_norm': 0.0006740678218193352, 'learning_rate': 3.077617328519856e-05, 'epoch': 1.44}
対照学習損失: 0.00020201830193400383
対照学習損失: 1.9557175619411282e-05
{'loss': 0.0001, 'grad_norm': 0.0070647625252604485, 'learning_rate': 3.075812274368231e-05, 'epoch': 1.44}
対照学習損失: 7.584593731735367e-06
対照学習損失: 3.9195358112920076e-05
{'loss': 0.0, 'grad_norm': 0.0008003688417375088, 'learning_rate': 3.074007220216607e-05, 'epoch': 1.44}
対照学習損失: 3.1169191061053425e-05
対照学習損失: 3.964309871662408e-05
{'loss': 0.0, 'grad_norm': 0.001764885033480823, 'learning_rate': 3.072202166064982e-05, 'epoch': 1.44}
対照学習損失: 4.053101292811334e-06
対照学習損失: 2.026424408541061e-05
{'loss': 0.0, 'grad_norm': 0.0004903609515167773, 'learning_rate': 3.0703971119133575e-05, 'epoch': 1.44}
対照学習損失: 0.0002650353708304465
対照学習損失: 1.9534867533366196e-05
{'loss': 0.0001, 'grad_norm': 0.014649187214672565, 'learning_rate': 3.0685920577617325e-05, 'epoch': 1.44}
対照学習損失: 2.8705613658530638e-05
対照学習損失: 1.4714520148118027e-05
{'loss': 0.0, 'grad_norm': 0.0006563100032508373, 'learning_rate': 3.066787003610108e-05, 'epoch': 1.44}
対照学習損失: 8.683699707034975e-05
対照学習損失: 5.9120367950527e-05
{'loss': 0.0001, 'grad_norm': 0.004885198548436165, 'learning_rate': 3.064981949458484e-05, 'epoch': 1.44}
対照学習損失: 0.0011785774258896708
対照学習損失: 1.785861240932718e-05
{'loss': 0.0006, 'grad_norm': 0.034554991871118546, 'learning_rate': 3.06317689530686e-05, 'epoch': 1.44}
対照学習損失: 0.00010796789138112217
対照学習損失: 1.3887549357605167e-05
{'loss': 0.0001, 'grad_norm': 0.0038333924021571875, 'learning_rate': 3.061371841155235e-05, 'epoch': 1.44}
対照学習損失: 0.00040077726589515805
対照学習損失: 9.953847438737284e-06
{'loss': 0.0002, 'grad_norm': 0.01599164493381977, 'learning_rate': 3.0595667870036104e-05, 'epoch': 1.45}
対照学習損失: 2.756582762231119e-05
対照学習損失: 1.0654197467374615e-05
{'loss': 0.0, 'grad_norm': 0.0004854365542996675, 'learning_rate': 3.057761732851986e-05, 'epoch': 1.45}
対照学習損失: 2.5062152417376637e-05
対照学習損失: 3.344212382216938e-05
{'loss': 0.0, 'grad_norm': 0.0007396761793643236, 'learning_rate': 3.055956678700361e-05, 'epoch': 1.45}
対照学習損失: 7.376009307336062e-06
対照学習損失: 2.3214452085085213e-05
{'loss': 0.0, 'grad_norm': 0.0008823266834951937, 'learning_rate': 3.054151624548737e-05, 'epoch': 1.45}
対照学習損失: 3.560099139576778e-05
対照学習損失: 1.221858929056907e-05
{'loss': 0.0, 'grad_norm': 0.0019499297486618161, 'learning_rate': 3.052346570397112e-05, 'epoch': 1.45}
対照学習損失: 1.1660008567559998e-05
対照学習損失: 0.0001244685990968719
{'loss': 0.0001, 'grad_norm': 0.0030711854342371225, 'learning_rate': 3.0505415162454877e-05, 'epoch': 1.45}
対照学習損失: 2.410165689070709e-05
対照学習損失: 2.0331564883235842e-05
{'loss': 0.0, 'grad_norm': 0.0012089223600924015, 'learning_rate': 3.048736462093863e-05, 'epoch': 1.45}
対照学習損失: 1.574233829160221e-05
対照学習損失: 0.0001537904900033027
{'loss': 0.0001, 'grad_norm': 0.005524867679923773, 'learning_rate': 3.0469314079422385e-05, 'epoch': 1.45}
対照学習損失: 0.00018694742175284773
対照学習損失: 0.00015969635569490492
{'loss': 0.0002, 'grad_norm': 0.013938471674919128, 'learning_rate': 3.045126353790614e-05, 'epoch': 1.45}
対照学習損失: 7.549554720753804e-05
対照学習損失: 0.0001074773317668587
{'loss': 0.0001, 'grad_norm': 0.0038023795932531357, 'learning_rate': 3.0433212996389892e-05, 'epoch': 1.45}
対照学習損失: 4.000678381999023e-05
対照学習損失: 2.4705070245545357e-05
{'loss': 0.0, 'grad_norm': 0.0010259770788252354, 'learning_rate': 3.0415162454873646e-05, 'epoch': 1.45}
対照学習損失: 2.461572330503259e-05
対照学習損失: 4.5144792238716036e-05
{'loss': 0.0, 'grad_norm': 0.0017940518446266651, 'learning_rate': 3.03971119133574e-05, 'epoch': 1.46}
対照学習損失: 4.440527845872566e-06
対照学習損失: 2.6119967515114695e-05
{'loss': 0.0, 'grad_norm': 0.0004789657541550696, 'learning_rate': 3.0379061371841157e-05, 'epoch': 1.46}
対照学習損失: 3.926434146706015e-06
対照学習損失: 1.8506818378227763e-05
{'loss': 0.0, 'grad_norm': 0.0002806438715197146, 'learning_rate': 3.036101083032491e-05, 'epoch': 1.46}
対照学習損失: 1.0669050425349269e-05
対照学習損失: 3.135722363367677e-05
{'loss': 0.0, 'grad_norm': 0.0009694314794614911, 'learning_rate': 3.0342960288808665e-05, 'epoch': 1.46}
対照学習損失: 3.341884075780399e-05
対照学習損失: 4.2095634853467345e-06
{'loss': 0.0, 'grad_norm': 0.0011705553624778986, 'learning_rate': 3.032490974729242e-05, 'epoch': 1.46}
対照学習損失: 0.02138988859951496
対照学習損失: 8.029276068555191e-05
{'loss': 0.0107, 'grad_norm': 0.4878118634223938, 'learning_rate': 3.0306859205776172e-05, 'epoch': 1.46}
対照学習損失: 8.180631994036958e-06
対照学習損失: 1.653992512729019e-05
{'loss': 0.0, 'grad_norm': 0.00032928347354754806, 'learning_rate': 3.0288808664259926e-05, 'epoch': 1.46}
対照学習損失: 0.0001362292969133705
対照学習損失: 0.00011147355689899996
{'loss': 0.0001, 'grad_norm': 0.0051081497222185135, 'learning_rate': 3.0270758122743687e-05, 'epoch': 1.46}
対照学習損失: 0.005540653131902218
対照学習損失: 2.069690526695922e-05
{'loss': 0.0028, 'grad_norm': 0.255707323551178, 'learning_rate': 3.025270758122744e-05, 'epoch': 1.46}
対照学習損失: 0.00011368838750058785
対照学習損失: 7.897558134573046e-06
{'loss': 0.0001, 'grad_norm': 0.00922228954732418, 'learning_rate': 3.0234657039711194e-05, 'epoch': 1.46}
対照学習損失: 0.0003303259436506778
対照学習損失: 1.4230328815756366e-05
{'loss': 0.0002, 'grad_norm': 0.015290556475520134, 'learning_rate': 3.0216606498194948e-05, 'epoch': 1.46}
対照学習損失: 0.00011691108375089243
対照学習損失: 3.251960879424587e-05
{'loss': 0.0001, 'grad_norm': 0.0039942143484950066, 'learning_rate': 3.0198555956678702e-05, 'epoch': 1.47}
対照学習損失: 1.0423154890304431e-05
対照学習損失: 6.219819624675438e-05
{'loss': 0.0, 'grad_norm': 0.0023232377134263515, 'learning_rate': 3.018050541516246e-05, 'epoch': 1.47}
対照学習損失: 0.000660889083519578
対照学習損失: 5.808559581055306e-05
{'loss': 0.0004, 'grad_norm': 0.017648059874773026, 'learning_rate': 3.0162454873646213e-05, 'epoch': 1.47}
対照学習損失: 0.0003491779207251966
対照学習損失: 9.244003740604967e-05
{'loss': 0.0002, 'grad_norm': 0.011856141500175, 'learning_rate': 3.0144404332129967e-05, 'epoch': 1.47}
対照学習損失: 5.7295968872494996e-05
対照学習損失: 0.0001758873404469341
{'loss': 0.0001, 'grad_norm': 0.0063059222884476185, 'learning_rate': 3.012635379061372e-05, 'epoch': 1.47}
対照学習損失: 3.7187674024607986e-05
対照学習損失: 2.1076744815218262e-05
{'loss': 0.0, 'grad_norm': 0.0009467605850659311, 'learning_rate': 3.0108303249097474e-05, 'epoch': 1.47}
対照学習損失: 0.00012424039596226066
対照学習損失: 0.00030397638329304755
{'loss': 0.0002, 'grad_norm': 0.008359010331332684, 'learning_rate': 3.0090252707581228e-05, 'epoch': 1.47}
対照学習損失: 3.4679007512750104e-05
対照学習損失: 4.150633321842179e-05
{'loss': 0.0, 'grad_norm': 0.0022698380053043365, 'learning_rate': 3.0072202166064982e-05, 'epoch': 1.47}
対照学習損失: 6.521743489429355e-05
対照学習損失: 0.00014656095299869776
{'loss': 0.0001, 'grad_norm': 0.004393361043184996, 'learning_rate': 3.0054151624548736e-05, 'epoch': 1.47}
対照学習損失: 3.453888712101616e-05
対照学習損失: 5.82805514568463e-05
{'loss': 0.0, 'grad_norm': 0.0014481174293905497, 'learning_rate': 3.0036101083032493e-05, 'epoch': 1.47}
対照学習損失: 5.792245428892784e-05
対照学習損失: 7.970795559231192e-05
{'loss': 0.0001, 'grad_norm': 0.0044059776701033115, 'learning_rate': 3.0018050541516247e-05, 'epoch': 1.47}
対照学習損失: 1.767227877280675e-05
対照学習損失: 6.818031397415325e-05
{'loss': 0.0, 'grad_norm': 0.00346636981703341, 'learning_rate': 3e-05, 'epoch': 1.48}
対照学習損失: 1.066163349605631e-05
対照学習損失: 3.543656202964485e-05
{'loss': 0.0, 'grad_norm': 0.001057870569638908, 'learning_rate': 2.9981949458483755e-05, 'epoch': 1.48}
対照学習損失: 0.004006484989076853
対照学習損失: 0.0057783895172178745
{'loss': 0.0049, 'grad_norm': 0.20430557429790497, 'learning_rate': 2.996389891696751e-05, 'epoch': 1.48}
対照学習損失: 0.00012724328553304076
対照学習損失: 7.621892109455075e-06
{'loss': 0.0001, 'grad_norm': 0.003060178365558386, 'learning_rate': 2.9945848375451262e-05, 'epoch': 1.48}
対照学習損失: 1.161530235549435e-05
対照学習損失: 9.98363975668326e-06
{'loss': 0.0, 'grad_norm': 0.00024267159460578114, 'learning_rate': 2.9927797833935016e-05, 'epoch': 1.48}
対照学習損失: 0.00018531364912632853
対照学習損失: 6.362719886965351e-06
{'loss': 0.0001, 'grad_norm': 0.004075626842677593, 'learning_rate': 2.9909747292418773e-05, 'epoch': 1.48}
対照学習損失: 9.01501698535867e-06
対照学習損失: 0.06869726628065109
{'loss': 0.0344, 'grad_norm': 1.1594785451889038, 'learning_rate': 2.9891696750902527e-05, 'epoch': 1.48}
対照学習損失: 2.532990401959978e-05
対照学習損失: 6.999737524893135e-05
{'loss': 0.0, 'grad_norm': 0.004072388634085655, 'learning_rate': 2.9873646209386284e-05, 'epoch': 1.48}
対照学習損失: 1.8760045350063592e-05
対照学習損失: 4.9711507017491385e-05
{'loss': 0.0, 'grad_norm': 0.0021101906895637512, 'learning_rate': 2.985559566787004e-05, 'epoch': 1.48}
対照学習損失: 8.602043089922518e-05
対照学習損失: 2.2790118237026036e-05
{'loss': 0.0001, 'grad_norm': 0.0025772720109671354, 'learning_rate': 2.9837545126353795e-05, 'epoch': 1.48}
対照学習損失: 7.458920299541205e-05
対照学習損失: 3.1949464755598456e-05
{'loss': 0.0001, 'grad_norm': 0.004907191265374422, 'learning_rate': 2.981949458483755e-05, 'epoch': 1.48}
対照学習損失: 1.4297444067778997e-05
対照学習損失: 3.138626561849378e-05
{'loss': 0.0, 'grad_norm': 0.0008311105193570256, 'learning_rate': 2.9801444043321303e-05, 'epoch': 1.49}
対照学習損失: 3.2570962503086776e-05
対照学習損失: 4.148107473156415e-05
{'loss': 0.0, 'grad_norm': 0.0015826913295313716, 'learning_rate': 2.9783393501805057e-05, 'epoch': 1.49}
対照学習損失: 0.0002126546751242131
対照学習損失: 1.4468487279373221e-05
{'loss': 0.0001, 'grad_norm': 0.009910962544381618, 'learning_rate': 2.976534296028881e-05, 'epoch': 1.49}
対照学習損失: 0.0001589626626810059
対照学習損失: 3.332249980303459e-05
{'loss': 0.0001, 'grad_norm': 0.0047617447562515736, 'learning_rate': 2.9747292418772564e-05, 'epoch': 1.49}
対照学習損失: 2.2402993636205792e-05
対照学習損失: 5.94412958889734e-05
{'loss': 0.0, 'grad_norm': 0.0019467375241219997, 'learning_rate': 2.9729241877256318e-05, 'epoch': 1.49}
対照学習損失: 1.273290217795875e-05
対照学習損失: 9.140859765466303e-05
{'loss': 0.0001, 'grad_norm': 0.003354498650878668, 'learning_rate': 2.9711191335740075e-05, 'epoch': 1.49}
対照学習損失: 0.0002079172118101269
対照学習損失: 3.2166673918254673e-05
{'loss': 0.0001, 'grad_norm': 0.008785882033407688, 'learning_rate': 2.969314079422383e-05, 'epoch': 1.49}
対照学習損失: 4.680371057474986e-05
対照学習損失: 4.6515870053553954e-05
{'loss': 0.0, 'grad_norm': 0.0017523179994896054, 'learning_rate': 2.9675090252707583e-05, 'epoch': 1.49}
対照学習損失: 2.7429483452579007e-05
対照学習損失: 2.8533826480270363e-05
{'loss': 0.0, 'grad_norm': 0.0021623375359922647, 'learning_rate': 2.9657039711191337e-05, 'epoch': 1.49}
対照学習損失: 7.129539153538644e-05
対照学習損失: 5.62308996450156e-05
{'loss': 0.0001, 'grad_norm': 0.0025100780185312033, 'learning_rate': 2.963898916967509e-05, 'epoch': 1.49}
対照学習損失: 0.00011205117334611714
対照学習損失: 0.00011232090037083253
{'loss': 0.0001, 'grad_norm': 0.004524374380707741, 'learning_rate': 2.9620938628158844e-05, 'epoch': 1.49}
対照学習損失: 9.807386959437281e-05
対照学習損失: 1.4006900528329425e-05
{'loss': 0.0001, 'grad_norm': 0.0016694512451067567, 'learning_rate': 2.9602888086642598e-05, 'epoch': 1.5}
対照学習損失: 1.361197246296797e-05
対照学習損失: 0.000894776254426688
{'loss': 0.0005, 'grad_norm': 0.05272649973630905, 'learning_rate': 2.9584837545126352e-05, 'epoch': 1.5}
対照学習損失: 2.6812613214133307e-05
対照学習損失: 9.501635213382542e-05
{'loss': 0.0001, 'grad_norm': 0.004090293776243925, 'learning_rate': 2.956678700361011e-05, 'epoch': 1.5}
対照学習損失: 0.00015211918798740953
対照学習損失: 4.3095969886053354e-05
{'loss': 0.0001, 'grad_norm': 0.00470005301758647, 'learning_rate': 2.9548736462093863e-05, 'epoch': 1.5}
対照学習損失: 6.697513890685514e-05
対照学習損失: 0.0031351905781775713
{'loss': 0.0016, 'grad_norm': 0.09155945479869843, 'learning_rate': 2.9530685920577617e-05, 'epoch': 1.5}
対照学習損失: 3.494654447422363e-05
対照学習損失: 0.00020929178572259843
{'loss': 0.0001, 'grad_norm': 0.006224602926522493, 'learning_rate': 2.951263537906137e-05, 'epoch': 1.5}
対照学習損失: 9.1920854174532e-05
対照学習損失: 2.3132886781240813e-05
{'loss': 0.0001, 'grad_norm': 0.0025594208855181932, 'learning_rate': 2.9494584837545125e-05, 'epoch': 1.5}
対照学習損失: 2.5829767764662392e-05
対照学習損失: 3.0253484510467388e-05
{'loss': 0.0, 'grad_norm': 0.0008432320319116116, 'learning_rate': 2.9476534296028885e-05, 'epoch': 1.5}
対照学習損失: 0.0001461907959310338
対照学習損失: 0.0006003596936352551
{'loss': 0.0004, 'grad_norm': 0.020631520077586174, 'learning_rate': 2.945848375451264e-05, 'epoch': 1.5}
対照学習損失: 0.0002960016718134284
対照学習損失: 3.6714183806907386e-05
{'loss': 0.0002, 'grad_norm': 0.009916724637150764, 'learning_rate': 2.9440433212996393e-05, 'epoch': 1.5}
対照学習損失: 2.9711591196246445e-05
対照学習損失: 0.0001493441959610209
{'loss': 0.0001, 'grad_norm': 0.004706762731075287, 'learning_rate': 2.9422382671480147e-05, 'epoch': 1.5}
対照学習損失: 0.00010062765795737505
対照学習損失: 2.947232496808283e-05
{'loss': 0.0001, 'grad_norm': 0.0030025714077055454, 'learning_rate': 2.94043321299639e-05, 'epoch': 1.51}
対照学習損失: 0.00018553869449533522
対照学習損失: 0.00013513359590433538
{'loss': 0.0002, 'grad_norm': 0.008124031126499176, 'learning_rate': 2.9386281588447658e-05, 'epoch': 1.51}
対照学習損失: 0.00027011852944269776
対照学習損失: 5.5843822337919846e-05
{'loss': 0.0002, 'grad_norm': 0.007951212115585804, 'learning_rate': 2.936823104693141e-05, 'epoch': 1.51}
対照学習損失: 0.0001472638250561431
対照学習損失: 9.149111065198667e-06
{'loss': 0.0001, 'grad_norm': 0.003501142142340541, 'learning_rate': 2.9350180505415165e-05, 'epoch': 1.51}
対照学習損失: 1.6562222299398854e-05
対照学習損失: 8.176184201147407e-05
{'loss': 0.0, 'grad_norm': 0.0026545326691120863, 'learning_rate': 2.933212996389892e-05, 'epoch': 1.51}
対照学習損失: 2.2403297407436185e-05
対照学習損失: 0.0008272145641967654
{'loss': 0.0004, 'grad_norm': 0.04594368487596512, 'learning_rate': 2.9314079422382673e-05, 'epoch': 1.51}
対照学習損失: 8.516502566635609e-05
対照学習損失: 3.888642822857946e-05
{'loss': 0.0001, 'grad_norm': 0.0018566602375358343, 'learning_rate': 2.9296028880866427e-05, 'epoch': 1.51}
対照学習損失: 0.00048254188732244074
対照学習損失: 2.284236325067468e-05
{'loss': 0.0003, 'grad_norm': 0.029775431379675865, 'learning_rate': 2.927797833935018e-05, 'epoch': 1.51}
対照学習損失: 4.4352334953146055e-05
対照学習損失: 3.3314874599454924e-05
{'loss': 0.0, 'grad_norm': 0.00248958938755095, 'learning_rate': 2.9259927797833934e-05, 'epoch': 1.51}
対照学習損失: 3.692108657560311e-05
対照学習損失: 5.0061509682564065e-05
{'loss': 0.0, 'grad_norm': 0.002261391142383218, 'learning_rate': 2.924187725631769e-05, 'epoch': 1.51}
対照学習損失: 3.683861723402515e-05
対照学習損失: 1.5265906768036075e-05
{'loss': 0.0, 'grad_norm': 0.0010370949748903513, 'learning_rate': 2.9223826714801445e-05, 'epoch': 1.51}
対照学習損失: 3.9916027162689716e-05
対照学習損失: 0.00017664094048086554
{'loss': 0.0001, 'grad_norm': 0.005300764460116625, 'learning_rate': 2.92057761732852e-05, 'epoch': 1.52}
対照学習損失: 0.000180309361894615
対照学習損失: 7.293108501471579e-05
{'loss': 0.0001, 'grad_norm': 0.016054555773735046, 'learning_rate': 2.9187725631768953e-05, 'epoch': 1.52}
対照学習損失: 8.687278750585392e-06
対照学習損失: 2.7848574973177165e-05
{'loss': 0.0, 'grad_norm': 0.0005346881807781756, 'learning_rate': 2.9169675090252707e-05, 'epoch': 1.52}
対照学習損失: 3.820433630608022e-05
対照学習損失: 1.742650056257844e-05
{'loss': 0.0, 'grad_norm': 0.0006920886808075011, 'learning_rate': 2.915162454873646e-05, 'epoch': 1.52}
対照学習損失: 7.689851918257773e-05
対照学習損失: 1.339588561677374e-05
{'loss': 0.0, 'grad_norm': 0.0017786583630368114, 'learning_rate': 2.9133574007220214e-05, 'epoch': 1.52}
対照学習損失: 2.7543475880520418e-05
対照学習損失: 1.2993536984140519e-05
{'loss': 0.0, 'grad_norm': 0.0007588820881210268, 'learning_rate': 2.9115523465703968e-05, 'epoch': 1.52}
対照学習損失: 4.744095713249408e-05
対照学習損失: 5.74803925701417e-05
{'loss': 0.0001, 'grad_norm': 0.0017697596922516823, 'learning_rate': 2.909747292418773e-05, 'epoch': 1.52}
対照学習損失: 9.022531230584718e-06
対照学習損失: 2.0994950318709016e-05
{'loss': 0.0, 'grad_norm': 0.000779341091401875, 'learning_rate': 2.9079422382671483e-05, 'epoch': 1.52}
対照学習損失: 6.976688018767163e-05
対照学習損失: 8.739461918594316e-06
{'loss': 0.0, 'grad_norm': 0.0022822394967079163, 'learning_rate': 2.906137184115524e-05, 'epoch': 1.52}
対照学習損失: 1.941578375408426e-05
対照学習損失: 2.3364005755865946e-05
{'loss': 0.0, 'grad_norm': 0.0009480143780820072, 'learning_rate': 2.9043321299638994e-05, 'epoch': 1.52}
対照学習損失: 4.0377111872658134e-05
対照学習損失: 0.00011413616448407993
{'loss': 0.0001, 'grad_norm': 0.0026456136256456375, 'learning_rate': 2.9025270758122748e-05, 'epoch': 1.52}
対照学習損失: 5.508620961336419e-05
対照学習損失: 0.00018040453142020851
{'loss': 0.0001, 'grad_norm': 0.005390964914113283, 'learning_rate': 2.90072202166065e-05, 'epoch': 1.53}
対照学習損失: 4.454247755347751e-05
対照学習損失: 0.0005830185255035758
{'loss': 0.0003, 'grad_norm': 0.03790561482310295, 'learning_rate': 2.8989169675090255e-05, 'epoch': 1.53}
対照学習損失: 0.0002828720025718212
対照学習損失: 8.835986227495596e-05
{'loss': 0.0002, 'grad_norm': 0.007259564008563757, 'learning_rate': 2.897111913357401e-05, 'epoch': 1.53}
対照学習損失: 0.0005087031750008464
対照学習損失: 3.2226853363681585e-05
{'loss': 0.0003, 'grad_norm': 0.02249332144856453, 'learning_rate': 2.8953068592057763e-05, 'epoch': 1.53}
対照学習損失: 5.5304422858171165e-05
対照学習損失: 0.004506263416260481
{'loss': 0.0023, 'grad_norm': 0.13229842483997345, 'learning_rate': 2.8935018050541517e-05, 'epoch': 1.53}
対照学習損失: 1.589143357705325e-05
対照学習損失: 0.0001285490143345669
{'loss': 0.0001, 'grad_norm': 0.0024406916927546263, 'learning_rate': 2.8916967509025274e-05, 'epoch': 1.53}
対照学習損失: 8.448854714515619e-06
対照学習損失: 7.432504207827151e-05
{'loss': 0.0, 'grad_norm': 0.0012573400745168328, 'learning_rate': 2.8898916967509028e-05, 'epoch': 1.53}
対照学習損失: 0.0003385496383998543
対照学習損失: 0.000253399892244488
{'loss': 0.0003, 'grad_norm': 0.019248878583312035, 'learning_rate': 2.888086642599278e-05, 'epoch': 1.53}
対照学習損失: 3.0537074053427204e-05
対照学習損失: 2.4249417037935928e-05
{'loss': 0.0, 'grad_norm': 0.0024607505183666945, 'learning_rate': 2.8862815884476535e-05, 'epoch': 1.53}
対照学習損失: 5.7303539506392553e-05
対照学習損失: 0.0001154553538071923
{'loss': 0.0001, 'grad_norm': 0.002919071353971958, 'learning_rate': 2.884476534296029e-05, 'epoch': 1.53}
対照学習損失: 2.404912083875388e-05
対照学習損失: 7.286695472430438e-05
{'loss': 0.0, 'grad_norm': 0.0020151252392679453, 'learning_rate': 2.8826714801444043e-05, 'epoch': 1.54}
対照学習損失: 8.443190745310858e-05
対照学習損失: 1.8364999050390907e-05
{'loss': 0.0001, 'grad_norm': 0.0027929991483688354, 'learning_rate': 2.8808664259927797e-05, 'epoch': 1.54}
対照学習損失: 0.00014771154383197427
対照学習損失: 1.6219524695770815e-05
{'loss': 0.0001, 'grad_norm': 0.006959176156669855, 'learning_rate': 2.879061371841155e-05, 'epoch': 1.54}
対照学習損失: 5.6339129514526576e-05
対照学習損失: 3.771591582335532e-05
{'loss': 0.0, 'grad_norm': 0.002097050193697214, 'learning_rate': 2.8772563176895308e-05, 'epoch': 1.54}
対照学習損失: 3.6311917938292027e-05
対照学習損失: 0.0010544985998421907
{'loss': 0.0005, 'grad_norm': 0.02014920860528946, 'learning_rate': 2.875451263537906e-05, 'epoch': 1.54}
対照学習損失: 1.4371908946486656e-05
対照学習損失: 1.2568903002829757e-05
{'loss': 0.0, 'grad_norm': 0.000370258028851822, 'learning_rate': 2.8736462093862815e-05, 'epoch': 1.54}
対照学習損失: 2.9271785024320707e-05
対照学習損失: 3.692831887747161e-05
{'loss': 0.0, 'grad_norm': 0.002068666974082589, 'learning_rate': 2.871841155234657e-05, 'epoch': 1.54}
対照学習損失: 1.1518366591189988e-05
対照学習損失: 8.94806144060567e-05
{'loss': 0.0001, 'grad_norm': 0.002609318122267723, 'learning_rate': 2.870036101083033e-05, 'epoch': 1.54}
対照学習損失: 3.0887236789567396e-05
対照学習損失: 5.778802005806938e-05
{'loss': 0.0, 'grad_norm': 0.0018277167109772563, 'learning_rate': 2.8682310469314084e-05, 'epoch': 1.54}
対照学習損失: 2.8391939849825576e-05
対照学習損失: 0.00021884577290620655
{'loss': 0.0001, 'grad_norm': 0.004525439348071814, 'learning_rate': 2.8664259927797837e-05, 'epoch': 1.54}
対照学習損失: 8.948049980972428e-06
対照学習損失: 9.625949132896494e-06
{'loss': 0.0, 'grad_norm': 0.0002633377443999052, 'learning_rate': 2.864620938628159e-05, 'epoch': 1.54}
対照学習損失: 6.429789664252894e-06
対照学習損失: 0.0003928128571715206
{'loss': 0.0002, 'grad_norm': 0.012432445771992207, 'learning_rate': 2.8628158844765345e-05, 'epoch': 1.55}
対照学習損失: 1.7835936887422577e-05
対照学習損失: 0.00011003947292920202
{'loss': 0.0001, 'grad_norm': 0.003627280006185174, 'learning_rate': 2.86101083032491e-05, 'epoch': 1.55}
対照学習損失: 7.815546268830076e-06
対照学習損失: 1.830525798141025e-05
{'loss': 0.0, 'grad_norm': 0.0003504089545458555, 'learning_rate': 2.8592057761732856e-05, 'epoch': 1.55}
対照学習損失: 0.0003477832651697099
対照学習損失: 2.5666458896012045e-05
{'loss': 0.0002, 'grad_norm': 0.009978679940104485, 'learning_rate': 2.857400722021661e-05, 'epoch': 1.55}
対照学習損失: 0.00010361193562857807
対照学習損失: 2.7528536520549096e-05
{'loss': 0.0001, 'grad_norm': 0.0025442473124712706, 'learning_rate': 2.8555956678700364e-05, 'epoch': 1.55}
対照学習損失: 3.088888479396701e-05
対照学習損失: 5.714544386137277e-06
{'loss': 0.0, 'grad_norm': 0.000619145983364433, 'learning_rate': 2.8537906137184118e-05, 'epoch': 1.55}
対照学習損失: 1.5839194020372815e-05
対照学習損失: 8.380850340472534e-05
{'loss': 0.0, 'grad_norm': 0.0027959749568253756, 'learning_rate': 2.851985559566787e-05, 'epoch': 1.55}
対照学習損失: 9.715404303278774e-06
対照学習損失: 0.00013678325922228396
{'loss': 0.0001, 'grad_norm': 0.008801154792308807, 'learning_rate': 2.8501805054151625e-05, 'epoch': 1.55}
対照学習損失: 3.501313767628744e-05
対照学習損失: 0.00019822288595605642
{'loss': 0.0001, 'grad_norm': 0.005508840549737215, 'learning_rate': 2.848375451263538e-05, 'epoch': 1.55}
対照学習損失: 0.00015926006017252803
対照学習損失: 4.1122097172774374e-05
{'loss': 0.0001, 'grad_norm': 0.0094016557559371, 'learning_rate': 2.8465703971119133e-05, 'epoch': 1.55}
対照学習損失: 8.031633115024306e-06
対照学習損失: 0.00018933966930489987
{'loss': 0.0001, 'grad_norm': 0.017462266609072685, 'learning_rate': 2.844765342960289e-05, 'epoch': 1.55}
対照学習損失: 5.0636772357393056e-05
対照学習損失: 0.0004404202045407146
{'loss': 0.0002, 'grad_norm': 0.027041763067245483, 'learning_rate': 2.8429602888086644e-05, 'epoch': 1.56}
対照学習損失: 1.2516788956418168e-05
対照学習損失: 2.4973400286398828e-05
{'loss': 0.0, 'grad_norm': 0.0006509635713882744, 'learning_rate': 2.8411552346570398e-05, 'epoch': 1.56}
対照学習損失: 6.146696250652894e-06
対照学習損失: 3.854526948998682e-05
{'loss': 0.0, 'grad_norm': 0.0008515574154444039, 'learning_rate': 2.839350180505415e-05, 'epoch': 1.56}
対照学習損失: 9.305501589551568e-05
対照学習損失: 3.6953129892935976e-05
{'loss': 0.0001, 'grad_norm': 0.0022185728885233402, 'learning_rate': 2.8375451263537905e-05, 'epoch': 1.56}
対照学習損失: 5.238704034127295e-05
対照学習損失: 3.666180418804288e-05
{'loss': 0.0, 'grad_norm': 0.0011310214176774025, 'learning_rate': 2.835740072202166e-05, 'epoch': 1.56}
対照学習損失: 1.9616736608440988e-05
対照学習損失: 0.00027267917175777256
{'loss': 0.0001, 'grad_norm': 0.00674606766551733, 'learning_rate': 2.8339350180505413e-05, 'epoch': 1.56}
対照学習損失: 0.004759545903652906
対照学習損失: 1.4841182746749837e-05
{'loss': 0.0024, 'grad_norm': 0.14363393187522888, 'learning_rate': 2.8321299638989167e-05, 'epoch': 1.56}
対照学習損失: 4.179692405159585e-05
対照学習損失: 6.400000529538374e-06
{'loss': 0.0, 'grad_norm': 0.0015119012678042054, 'learning_rate': 2.8303249097472927e-05, 'epoch': 1.56}
対照学習損失: 0.00034144040546379983
対照学習損失: 2.036888326983899e-05
{'loss': 0.0002, 'grad_norm': 0.010510867461562157, 'learning_rate': 2.828519855595668e-05, 'epoch': 1.56}
対照学習損失: 1.1339652701281011e-05
対照学習損失: 3.41194863722194e-05
{'loss': 0.0, 'grad_norm': 0.0010570281883701682, 'learning_rate': 2.8267148014440435e-05, 'epoch': 1.56}
対照学習損失: 3.689313598442823e-05
対照学習損失: 7.107801138772629e-06
{'loss': 0.0, 'grad_norm': 0.0007167293806560338, 'learning_rate': 2.8249097472924192e-05, 'epoch': 1.56}
対照学習損失: 0.0003730011230800301
対照学習損失: 1.8051803635898978e-05
{'loss': 0.0002, 'grad_norm': 0.012108356691896915, 'learning_rate': 2.8231046931407946e-05, 'epoch': 1.57}
対照学習損失: 0.0002991349610965699
対照学習損失: 0.00012746939319185913
{'loss': 0.0002, 'grad_norm': 0.008056429214775562, 'learning_rate': 2.82129963898917e-05, 'epoch': 1.57}
対照学習損失: 6.474516794696683e-06
対照学習損失: 0.00013480300549417734
{'loss': 0.0001, 'grad_norm': 0.00272301584482193, 'learning_rate': 2.8194945848375454e-05, 'epoch': 1.57}
対照学習損失: 8.910741598811e-06
対照学習損失: 0.0004859681357629597
{'loss': 0.0002, 'grad_norm': 0.010289261117577553, 'learning_rate': 2.8176895306859207e-05, 'epoch': 1.57}
対照学習損失: 6.556463176821126e-06
対照学習損失: 1.6033211068133824e-05
{'loss': 0.0, 'grad_norm': 0.00048460334073752165, 'learning_rate': 2.815884476534296e-05, 'epoch': 1.57}
対照学習損失: 8.693399286130443e-05
対照学習損失: 1.9407496438361704e-05
{'loss': 0.0001, 'grad_norm': 0.005762633867561817, 'learning_rate': 2.8140794223826715e-05, 'epoch': 1.57}
対照学習損失: 2.5144523533526808e-05
対照学習損失: 0.003501007566228509
{'loss': 0.0018, 'grad_norm': 0.12674866616725922, 'learning_rate': 2.8122743682310472e-05, 'epoch': 1.57}
対照学習損失: 9.909803338814527e-05
対照学習損失: 4.3134881707374007e-05
{'loss': 0.0001, 'grad_norm': 0.002679927507415414, 'learning_rate': 2.8104693140794226e-05, 'epoch': 1.57}
対照学習損失: 1.2650796634261496e-05
対照学習損失: 0.00010498765186639503
{'loss': 0.0001, 'grad_norm': 0.002745724515989423, 'learning_rate': 2.808664259927798e-05, 'epoch': 1.57}
対照学習損失: 0.00027644968940876424
対照学習損失: 9.22368599276524e-06
{'loss': 0.0001, 'grad_norm': 0.005516672506928444, 'learning_rate': 2.8068592057761734e-05, 'epoch': 1.57}
対照学習損失: 2.7178539312444627e-05
対照学習損失: 6.057451537344605e-05
{'loss': 0.0, 'grad_norm': 0.0011395062319934368, 'learning_rate': 2.8050541516245488e-05, 'epoch': 1.57}
対照学習損失: 8.57554368849378e-06
対照学習損失: 2.268611206091009e-05
{'loss': 0.0, 'grad_norm': 0.0003821078280452639, 'learning_rate': 2.803249097472924e-05, 'epoch': 1.58}
対照学習損失: 7.836028817109764e-05
対照学習損失: 4.437028110260144e-05
{'loss': 0.0001, 'grad_norm': 0.0028785220347344875, 'learning_rate': 2.8014440433212995e-05, 'epoch': 1.58}
対照学習損失: 2.0196986952214502e-05
対照学習損失: 2.4673459847690538e-05
{'loss': 0.0, 'grad_norm': 0.0013002741616219282, 'learning_rate': 2.799638989169675e-05, 'epoch': 1.58}
対照学習損失: 2.9151315175113268e-05
対照学習損失: 0.00041743903420865536
{'loss': 0.0002, 'grad_norm': 0.010919624008238316, 'learning_rate': 2.7978339350180506e-05, 'epoch': 1.58}
対照学習損失: 0.0003391131176613271
対照学習損失: 4.5456981752067804e-05
{'loss': 0.0002, 'grad_norm': 0.010208530351519585, 'learning_rate': 2.796028880866426e-05, 'epoch': 1.58}
対照学習損失: 3.9982325688470155e-05
対照学習損失: 1.2755076568282675e-05
{'loss': 0.0, 'grad_norm': 0.001159443985670805, 'learning_rate': 2.7942238267148014e-05, 'epoch': 1.58}
対照学習損失: 4.184149292996153e-05
対照学習損失: 5.6028093240456656e-06
{'loss': 0.0, 'grad_norm': 0.0029777518939226866, 'learning_rate': 2.7924187725631774e-05, 'epoch': 1.58}
対照学習損失: 2.4235581804532558e-05
対照学習損失: 5.3274947276804596e-05
{'loss': 0.0, 'grad_norm': 0.001384004601277411, 'learning_rate': 2.7906137184115528e-05, 'epoch': 1.58}
対照学習損失: 1.1019243174814619e-05
対照学習損失: 2.47855805355357e-05
{'loss': 0.0, 'grad_norm': 0.000711103668436408, 'learning_rate': 2.7888086642599282e-05, 'epoch': 1.58}
対照学習損失: 1.4505983926937915e-05
対照学習損失: 9.253518328478094e-06
{'loss': 0.0, 'grad_norm': 0.00029783861828036606, 'learning_rate': 2.7870036101083036e-05, 'epoch': 1.58}
対照学習損失: 3.700725574162789e-05
対照学習損失: 4.46625126642175e-05
{'loss': 0.0, 'grad_norm': 0.001627377001568675, 'learning_rate': 2.785198555956679e-05, 'epoch': 1.58}
対照学習損失: 1.2673222045123111e-05
対照学習損失: 3.538547025527805e-05
{'loss': 0.0, 'grad_norm': 0.0007547455024905503, 'learning_rate': 2.7833935018050544e-05, 'epoch': 1.59}
対照学習損失: 2.6440551664563827e-05
対照学習損失: 0.00410911999642849
{'loss': 0.0021, 'grad_norm': 0.1342182606458664, 'learning_rate': 2.7815884476534297e-05, 'epoch': 1.59}
対照学習損失: 4.991498281015083e-05
対照学習損失: 0.000581851985771209
{'loss': 0.0003, 'grad_norm': 0.01755383051931858, 'learning_rate': 2.779783393501805e-05, 'epoch': 1.59}
対照学習損失: 1.0221989214187488e-05
対照学習損失: 2.7689773560268804e-05
{'loss': 0.0, 'grad_norm': 0.0008087154128588736, 'learning_rate': 2.777978339350181e-05, 'epoch': 1.59}
対照学習損失: 1.8185481167165563e-05
対照学習損失: 1.224116567755118e-05
{'loss': 0.0, 'grad_norm': 0.000588655355386436, 'learning_rate': 2.7761732851985562e-05, 'epoch': 1.59}
対照学習損失: 4.477774837141624e-06
対照学習損失: 2.966496322187595e-05
{'loss': 0.0, 'grad_norm': 0.0017120810225605965, 'learning_rate': 2.7743682310469316e-05, 'epoch': 1.59}
対照学習損失: 6.335011858027428e-05
対照学習損失: 0.00018471202929504216
{'loss': 0.0001, 'grad_norm': 0.006161470431834459, 'learning_rate': 2.772563176895307e-05, 'epoch': 1.59}
対照学習損失: 0.0001431635464541614
対照学習損失: 8.582997907069512e-06
{'loss': 0.0001, 'grad_norm': 0.003264597849920392, 'learning_rate': 2.7707581227436824e-05, 'epoch': 1.59}
対照学習損失: 2.1494146494660527e-05
対照学習損失: 0.0005450918106362224
{'loss': 0.0003, 'grad_norm': 0.019327720627188683, 'learning_rate': 2.7689530685920577e-05, 'epoch': 1.59}
対照学習損失: 5.4294709116220474e-05
対照学習損失: 0.09427297115325928
{'loss': 0.0472, 'grad_norm': 4.6557745933532715, 'learning_rate': 2.767148014440433e-05, 'epoch': 1.59}
対照学習損失: 3.744390414794907e-05
対照学習損失: 5.103624062030576e-06
{'loss': 0.0, 'grad_norm': 0.000846345501486212, 'learning_rate': 2.765342960288809e-05, 'epoch': 1.59}
対照学習損失: 5.5632946896366775e-05
対照学習損失: 9.026045154314488e-05
{'loss': 0.0001, 'grad_norm': 0.003196375211700797, 'learning_rate': 2.7635379061371842e-05, 'epoch': 1.6}
対照学習損失: 4.8875466745812446e-05
対照学習損失: 1.3418080015981104e-05
{'loss': 0.0, 'grad_norm': 0.000920710212085396, 'learning_rate': 2.7617328519855596e-05, 'epoch': 1.6}
対照学習損失: 6.629554263781756e-05
対照学習損失: 0.004132152069360018
{'loss': 0.0021, 'grad_norm': 0.17944365739822388, 'learning_rate': 2.759927797833935e-05, 'epoch': 1.6}
対照学習損失: 0.00023343632346950471
対照学習損失: 1.4714639291923959e-05
{'loss': 0.0001, 'grad_norm': 0.011694155633449554, 'learning_rate': 2.7581227436823104e-05, 'epoch': 1.6}
対照学習損失: 3.424638998694718e-05
対照学習損失: 5.733885336667299e-05
{'loss': 0.0, 'grad_norm': 0.0038500407245010138, 'learning_rate': 2.7563176895306858e-05, 'epoch': 1.6}
対照学習損失: 6.020032742526382e-06
対照学習損失: 0.00010037209722213447
{'loss': 0.0001, 'grad_norm': 0.003445429727435112, 'learning_rate': 2.754512635379061e-05, 'epoch': 1.6}
対照学習損失: 9.630973363528028e-05
対照学習損失: 0.00010402235784567893
{'loss': 0.0001, 'grad_norm': 0.0062302490696311, 'learning_rate': 2.7527075812274372e-05, 'epoch': 1.6}
対照学習損失: 0.00018084263138007373
対照学習損失: 0.0011233808472752571
{'loss': 0.0007, 'grad_norm': 0.07043330371379852, 'learning_rate': 2.7509025270758126e-05, 'epoch': 1.6}
対照学習損失: 0.0002903797721955925
対照学習損失: 3.5262277378933504e-05
{'loss': 0.0002, 'grad_norm': 0.02283053658902645, 'learning_rate': 2.749097472924188e-05, 'epoch': 1.6}
対照学習損失: 3.380080670467578e-05
対照学習損失: 0.0008816951303742826
{'loss': 0.0005, 'grad_norm': 0.059172749519348145, 'learning_rate': 2.7472924187725633e-05, 'epoch': 1.6}
対照学習損失: 0.0056175896897912025
対照学習損失: 4.102667298866436e-05
{'loss': 0.0028, 'grad_norm': 0.09460964798927307, 'learning_rate': 2.745487364620939e-05, 'epoch': 1.6}
対照学習損失: 0.00037118318141438067
対照学習損失: 0.0031772968359291553
{'loss': 0.0018, 'grad_norm': 0.22823236882686615, 'learning_rate': 2.7436823104693144e-05, 'epoch': 1.61}
対照学習損失: 0.0032728277146816254
対照学習損失: 1.9214525309507735e-05
{'loss': 0.0016, 'grad_norm': 0.1241454929113388, 'learning_rate': 2.7418772563176898e-05, 'epoch': 1.61}
対照学習損失: 0.0003836210526060313
対照学習損失: 0.00010062734509119764
{'loss': 0.0002, 'grad_norm': 0.02570810541510582, 'learning_rate': 2.7400722021660652e-05, 'epoch': 1.61}
対照学習損失: 0.0003210534923709929
対照学習損失: 0.0005549248307943344
{'loss': 0.0004, 'grad_norm': 0.024453865364193916, 'learning_rate': 2.7382671480144406e-05, 'epoch': 1.61}
対照学習損失: 0.0007476169848814607
対照学習損失: 1.3298841622599866e-05
{'loss': 0.0004, 'grad_norm': 0.03214230760931969, 'learning_rate': 2.736462093862816e-05, 'epoch': 1.61}
対照学習損失: 1.0095434845425189e-05
対照学習損失: 6.093183037592098e-05
{'loss': 0.0, 'grad_norm': 0.002956958720460534, 'learning_rate': 2.7346570397111914e-05, 'epoch': 1.61}
対照学習損失: 1.5057315977173857e-05
対照学習損失: 0.0006884054746478796
{'loss': 0.0004, 'grad_norm': 0.06268921494483948, 'learning_rate': 2.7328519855595667e-05, 'epoch': 1.61}
対照学習損失: 2.030214454862289e-05
対照学習損失: 8.085179433692247e-05
{'loss': 0.0001, 'grad_norm': 0.003675108542665839, 'learning_rate': 2.7310469314079425e-05, 'epoch': 1.61}
対照学習損失: 1.962410897249356e-05
対照学習損失: 0.0002103775623254478
{'loss': 0.0001, 'grad_norm': 0.006379379890859127, 'learning_rate': 2.729241877256318e-05, 'epoch': 1.61}
対照学習損失: 7.090665894793347e-05
対照学習損失: 0.00039063827716745436
{'loss': 0.0002, 'grad_norm': 0.021664900705218315, 'learning_rate': 2.7274368231046932e-05, 'epoch': 1.61}
対照学習損失: 0.0001396356092300266
対照学習損失: 0.00016301109280902892
{'loss': 0.0002, 'grad_norm': 0.016593825072050095, 'learning_rate': 2.7256317689530686e-05, 'epoch': 1.61}
対照学習損失: 2.9554867069236934e-05
対照学習損失: 6.643599044764414e-05
{'loss': 0.0, 'grad_norm': 0.002052903641015291, 'learning_rate': 2.723826714801444e-05, 'epoch': 1.62}
対照学習損失: 0.00010306319745723158
対照学習損失: 5.127209806232713e-05
{'loss': 0.0001, 'grad_norm': 0.0032653980888426304, 'learning_rate': 2.7220216606498194e-05, 'epoch': 1.62}
対照学習損失: 0.0024862245190888643
対照学習損失: 0.00012615042214747518
{'loss': 0.0013, 'grad_norm': 0.0729893296957016, 'learning_rate': 2.7202166064981947e-05, 'epoch': 1.62}
対照学習損失: 2.5182058379868977e-05
対照学習損失: 2.855694037862122e-05
{'loss': 0.0, 'grad_norm': 0.000981001416221261, 'learning_rate': 2.7184115523465705e-05, 'epoch': 1.62}
対照学習損失: 4.507958874455653e-05
対照学習損失: 0.00021661292703356594
{'loss': 0.0001, 'grad_norm': 0.005662846844643354, 'learning_rate': 2.716606498194946e-05, 'epoch': 1.62}
対照学習損失: 3.158068284392357e-05
対照学習損失: 0.00023115219664759934
{'loss': 0.0001, 'grad_norm': 0.005449702963232994, 'learning_rate': 2.7148014440433216e-05, 'epoch': 1.62}
対照学習損失: 3.299630043329671e-05
対照学習損失: 4.2122617742279544e-05
{'loss': 0.0, 'grad_norm': 0.0018826257437467575, 'learning_rate': 2.7129963898916973e-05, 'epoch': 1.62}
対照学習損失: 0.0001851077686296776
対照学習損失: 0.001652156701311469
{'loss': 0.0009, 'grad_norm': 0.08377689868211746, 'learning_rate': 2.7111913357400727e-05, 'epoch': 1.62}
対照学習損失: 5.4764044762123376e-05
対照学習損失: 1.9281582353869453e-05
{'loss': 0.0, 'grad_norm': 0.0010509646963328123, 'learning_rate': 2.709386281588448e-05, 'epoch': 1.62}
対照学習損失: 0.00043968483805656433
対照学習損失: 0.00029914715560153127
{'loss': 0.0004, 'grad_norm': 0.02963978424668312, 'learning_rate': 2.7075812274368234e-05, 'epoch': 1.62}
対照学習損失: 4.2658954043872654e-05
対照学習損失: 1.37386450660415e-05
{'loss': 0.0, 'grad_norm': 0.0007381809409707785, 'learning_rate': 2.7057761732851988e-05, 'epoch': 1.62}
対照学習損失: 4.017744868178852e-05
対照学習損失: 0.00026731938123703003
{'loss': 0.0002, 'grad_norm': 0.011412534862756729, 'learning_rate': 2.7039711191335742e-05, 'epoch': 1.63}
対照学習損失: 2.5472127163084224e-05
対照学習損失: 2.1434509108075872e-05
{'loss': 0.0, 'grad_norm': 0.0006295000202953815, 'learning_rate': 2.7021660649819496e-05, 'epoch': 1.63}
対照学習損失: 5.416545263869921e-06
対照学習損失: 4.311975499149412e-05
{'loss': 0.0, 'grad_norm': 0.002147509716451168, 'learning_rate': 2.700361010830325e-05, 'epoch': 1.63}
対照学習損失: 8.154565875884145e-05
対照学習損失: 0.0024438186082988977
{'loss': 0.0013, 'grad_norm': 0.050439126789569855, 'learning_rate': 2.6985559566787007e-05, 'epoch': 1.63}
対照学習損失: 0.00016255024820566177
対照学習損失: 4.865207301918417e-06
{'loss': 0.0001, 'grad_norm': 0.004983528517186642, 'learning_rate': 2.696750902527076e-05, 'epoch': 1.63}
対照学習損失: 0.0001866227830760181
対照学習損失: 4.138853546464816e-05
{'loss': 0.0001, 'grad_norm': 0.005777101963758469, 'learning_rate': 2.6949458483754514e-05, 'epoch': 1.63}
対照学習損失: 7.602924597449601e-05
対照学習損失: 1.712790981400758e-05
{'loss': 0.0, 'grad_norm': 0.001622422132641077, 'learning_rate': 2.6931407942238268e-05, 'epoch': 1.63}
対照学習損失: 1.4520989680022467e-05
対照学習損失: 1.2054850230924785e-05
{'loss': 0.0, 'grad_norm': 0.0002793396997731179, 'learning_rate': 2.6913357400722022e-05, 'epoch': 1.63}
対照学習損失: 4.839113535126671e-05
対照学習損失: 3.976506195613183e-05
{'loss': 0.0, 'grad_norm': 0.004253052640706301, 'learning_rate': 2.6895306859205776e-05, 'epoch': 1.63}
対照学習損失: 0.00046187255065888166
対照学習損失: 5.938077265454922e-06
{'loss': 0.0002, 'grad_norm': 0.014355598017573357, 'learning_rate': 2.687725631768953e-05, 'epoch': 1.63}
対照学習損失: 4.6206583647290245e-05
対照学習損失: 0.00014379093772731721
{'loss': 0.0001, 'grad_norm': 0.006576103158295155, 'learning_rate': 2.6859205776173284e-05, 'epoch': 1.64}
対照学習損失: 7.169843593146652e-05
対照学習損失: 2.1723628378822468e-05
{'loss': 0.0, 'grad_norm': 0.002223243238404393, 'learning_rate': 2.684115523465704e-05, 'epoch': 1.64}
対照学習損失: 8.27752137411153e-06
対照学習損失: 4.480460484046489e-05
{'loss': 0.0, 'grad_norm': 0.0007283050217665732, 'learning_rate': 2.6823104693140795e-05, 'epoch': 1.64}
対照学習損失: 1.5101821190910414e-05
対照学習損失: 4.498961061472073e-05
{'loss': 0.0, 'grad_norm': 0.0013900669291615486, 'learning_rate': 2.680505415162455e-05, 'epoch': 1.64}
対照学習損失: 0.00021666059910785407
対照学習損失: 3.295175338280387e-05
{'loss': 0.0001, 'grad_norm': 0.010372289456427097, 'learning_rate': 2.6787003610108302e-05, 'epoch': 1.64}
対照学習損失: 3.552249108906835e-05
対照学習損失: 3.7115387385711074e-05
{'loss': 0.0, 'grad_norm': 0.001251266454346478, 'learning_rate': 2.6768953068592056e-05, 'epoch': 1.64}
対照学習損失: 6.228648999240249e-06
対照学習損失: 9.19238809728995e-05
{'loss': 0.0, 'grad_norm': 0.0020216312259435654, 'learning_rate': 2.6750902527075817e-05, 'epoch': 1.64}
対照学習損失: 0.00010998283687513322
対照学習損失: 8.087548485491425e-05
{'loss': 0.0001, 'grad_norm': 0.007058954797685146, 'learning_rate': 2.673285198555957e-05, 'epoch': 1.64}
対照学習損失: 5.330103886080906e-05
対照学習損失: 1.8156319129047915e-05
{'loss': 0.0, 'grad_norm': 0.0012565414654091, 'learning_rate': 2.6714801444043324e-05, 'epoch': 1.64}
対照学習損失: 1.9385444829822518e-05
対照学習損失: 0.00012795535440091044
{'loss': 0.0001, 'grad_norm': 0.0022784459870308638, 'learning_rate': 2.6696750902527078e-05, 'epoch': 1.64}
対照学習損失: 7.553401519544423e-05
対照学習損失: 3.5504890547599643e-05
{'loss': 0.0001, 'grad_norm': 0.0018167992820963264, 'learning_rate': 2.6678700361010832e-05, 'epoch': 1.64}
対照学習損失: 0.0003582813951652497
対照学習損失: 1.6659239918226376e-05
{'loss': 0.0002, 'grad_norm': 0.010381062515079975, 'learning_rate': 2.666064981949459e-05, 'epoch': 1.65}
対照学習損失: 2.1717911295127124e-05
対照学習損失: 8.80645529832691e-06
{'loss': 0.0, 'grad_norm': 0.00040116687887348235, 'learning_rate': 2.6642599277978343e-05, 'epoch': 1.65}
対照学習損失: 3.4587203117553145e-05
対照学習損失: 0.00012068555224686861
{'loss': 0.0001, 'grad_norm': 0.0032872140873223543, 'learning_rate': 2.6624548736462097e-05, 'epoch': 1.65}
対照学習損失: 5.119715933687985e-05
対照学習損失: 1.5444755263160914e-05
{'loss': 0.0, 'grad_norm': 0.002533442573621869, 'learning_rate': 2.660649819494585e-05, 'epoch': 1.65}
対照学習損失: 2.803432653308846e-05
対照学習損失: 3.525974534568377e-05
{'loss': 0.0, 'grad_norm': 0.0025214022025465965, 'learning_rate': 2.6588447653429604e-05, 'epoch': 1.65}
対照学習損失: 2.5658691811258905e-05
対照学習損失: 8.732002243050374e-06
{'loss': 0.0, 'grad_norm': 0.0010834165150299668, 'learning_rate': 2.6570397111913358e-05, 'epoch': 1.65}
対照学習損失: 6.1690252550761215e-06
対照学習損失: 4.462878678168636e-06
{'loss': 0.0, 'grad_norm': 0.00011441706010373309, 'learning_rate': 2.6552346570397112e-05, 'epoch': 1.65}
対照学習損失: 7.476521568605676e-05
対照学習損失: 1.686733594397083e-05
{'loss': 0.0, 'grad_norm': 0.001322553725913167, 'learning_rate': 2.6534296028880866e-05, 'epoch': 1.65}
対照学習損失: 5.006763785786461e-06
対照学習損失: 2.2172180251800455e-05
{'loss': 0.0, 'grad_norm': 0.0003519575111567974, 'learning_rate': 2.6516245487364623e-05, 'epoch': 1.65}
対照学習損失: 3.262453537900001e-05
対照学習損失: 1.9802550013992004e-05
{'loss': 0.0, 'grad_norm': 0.0009248445858247578, 'learning_rate': 2.6498194945848377e-05, 'epoch': 1.65}
対照学習損失: 1.0490224667591974e-05
対照学習損失: 0.00018810597248375416
{'loss': 0.0001, 'grad_norm': 0.009064634330570698, 'learning_rate': 2.648014440433213e-05, 'epoch': 1.65}
対照学習損失: 7.912902947282419e-05
対照学習損失: 1.1093754437752068e-05
{'loss': 0.0, 'grad_norm': 0.005017376970499754, 'learning_rate': 2.6462093862815884e-05, 'epoch': 1.66}
対照学習損失: 0.0009388734470121562
対照学習損失: 0.0011772370198741555
{'loss': 0.0011, 'grad_norm': 0.05087120085954666, 'learning_rate': 2.6444043321299638e-05, 'epoch': 1.66}
対照学習損失: 8.284981049655471e-06
対照学習損失: 0.00037944313953630626
{'loss': 0.0002, 'grad_norm': 0.008742749691009521, 'learning_rate': 2.6425992779783392e-05, 'epoch': 1.66}
対照学習損失: 6.794881483074278e-05
対照学習損失: 0.015945177525281906
{'loss': 0.008, 'grad_norm': 0.46513456106185913, 'learning_rate': 2.6407942238267146e-05, 'epoch': 1.66}
対照学習損失: 0.00012557055742945522
対照学習損失: 5.6251469686685596e-06
{'loss': 0.0001, 'grad_norm': 0.007254940923303366, 'learning_rate': 2.63898916967509e-05, 'epoch': 1.66}
対照学習損失: 0.0002617674181237817
対照学習損失: 8.085626905085519e-05
{'loss': 0.0002, 'grad_norm': 0.007267503533512354, 'learning_rate': 2.6371841155234657e-05, 'epoch': 1.66}
対照学習損失: 1.7739359464030713e-05
対照学習損失: 9.494228288531303e-05
{'loss': 0.0001, 'grad_norm': 0.0023215056862682104, 'learning_rate': 2.6353790613718414e-05, 'epoch': 1.66}
対照学習損失: 3.5900771763408557e-05
対照学習損失: 0.00022042982163839042
{'loss': 0.0001, 'grad_norm': 0.00636955676600337, 'learning_rate': 2.633574007220217e-05, 'epoch': 1.66}
対照学習損失: 0.0001351350947516039
対照学習損失: 1.2077145584044047e-05
{'loss': 0.0001, 'grad_norm': 0.003546834457665682, 'learning_rate': 2.6317689530685925e-05, 'epoch': 1.66}
対照学習損失: 0.0007477988256141543
対照学習損失: 6.219622446224093e-05
{'loss': 0.0004, 'grad_norm': 0.026474880054593086, 'learning_rate': 2.629963898916968e-05, 'epoch': 1.66}
対照学習損失: 1.1734427062037867e-05
対照学習損失: 1.1786612958530895e-05
{'loss': 0.0, 'grad_norm': 0.00035027891863137484, 'learning_rate': 2.6281588447653433e-05, 'epoch': 1.66}
対照学習損失: 3.694479528348893e-05
対照学習損失: 0.0002032741904258728
{'loss': 0.0001, 'grad_norm': 0.0052210260182619095, 'learning_rate': 2.6263537906137187e-05, 'epoch': 1.67}
対照学習損失: 3.7889782106503844e-05
対照学習損失: 0.00019175557827111334
{'loss': 0.0001, 'grad_norm': 0.008180810138583183, 'learning_rate': 2.624548736462094e-05, 'epoch': 1.67}
対照学習損失: 1.648038778512273e-05
対照学習損失: 9.58130658545997e-06
{'loss': 0.0, 'grad_norm': 0.00031160018988884985, 'learning_rate': 2.6227436823104694e-05, 'epoch': 1.67}
対照学習損失: 2.4943450625869446e-05
対照学習損失: 7.092901796568185e-06
{'loss': 0.0, 'grad_norm': 0.0004801527538802475, 'learning_rate': 2.6209386281588448e-05, 'epoch': 1.67}
対照学習損失: 0.0030378031078726053
対照学習損失: 8.499217801727355e-05
{'loss': 0.0016, 'grad_norm': 0.08179918676614761, 'learning_rate': 2.6191335740072205e-05, 'epoch': 1.67}
対照学習損失: 0.005484782159328461
対照学習損失: 8.426784916082397e-05
{'loss': 0.0028, 'grad_norm': 0.23490045964717865, 'learning_rate': 2.617328519855596e-05, 'epoch': 1.67}
対照学習損失: 9.458108979742974e-05
対照学習損失: 1.1995221939287148e-05
{'loss': 0.0001, 'grad_norm': 0.002697878750041127, 'learning_rate': 2.6155234657039713e-05, 'epoch': 1.67}
対照学習損失: 5.394771142164245e-05
対照学習損失: 4.607410664903e-05
{'loss': 0.0001, 'grad_norm': 0.0030202162452042103, 'learning_rate': 2.6137184115523467e-05, 'epoch': 1.67}
対照学習損失: 2.6053570763906464e-05
対照学習損失: 1.1712105333572254e-05
{'loss': 0.0, 'grad_norm': 0.00044170551700517535, 'learning_rate': 2.611913357400722e-05, 'epoch': 1.67}
対照学習損失: 1.6830501408549026e-05
対照学習損失: 2.1374766220105812e-05
{'loss': 0.0, 'grad_norm': 0.0007355026318691671, 'learning_rate': 2.6101083032490974e-05, 'epoch': 1.67}
対照学習損失: 1.7799042325350456e-05
対照学習損失: 4.3712887418223545e-05
{'loss': 0.0, 'grad_norm': 0.0011216390412300825, 'learning_rate': 2.6083032490974728e-05, 'epoch': 1.67}
対照学習損失: 7.837949851818848e-06
対照学習損失: 0.00015936394629534334
{'loss': 0.0001, 'grad_norm': 0.0033148087095469236, 'learning_rate': 2.6064981949458482e-05, 'epoch': 1.68}
対照学習損失: 0.003014508867636323
対照学習損失: 2.9629245545947924e-05
{'loss': 0.0015, 'grad_norm': 0.059076208621263504, 'learning_rate': 2.604693140794224e-05, 'epoch': 1.68}
対照学習損失: 8.091215931926854e-06
対照学習損失: 0.0007804788183420897
{'loss': 0.0004, 'grad_norm': 0.051311515271663666, 'learning_rate': 2.6028880866425993e-05, 'epoch': 1.68}
対照学習損失: 4.05572573072277e-05
対照学習損失: 5.8884837926598266e-05
{'loss': 0.0, 'grad_norm': 0.0022968074772506952, 'learning_rate': 2.6010830324909747e-05, 'epoch': 1.68}
対照学習損失: 2.697031231946312e-05
対照学習損失: 7.967834244482219e-05
{'loss': 0.0001, 'grad_norm': 0.0021705462131649256, 'learning_rate': 2.59927797833935e-05, 'epoch': 1.68}
対照学習損失: 3.8783586205681786e-05
対照学習損失: 1.9973755115643144e-05
{'loss': 0.0, 'grad_norm': 0.0012593483552336693, 'learning_rate': 2.597472924187726e-05, 'epoch': 1.68}
対照学習損失: 7.466757961083204e-05
対照学習損失: 5.527512257685885e-05
{'loss': 0.0001, 'grad_norm': 0.0024307884741574526, 'learning_rate': 2.5956678700361015e-05, 'epoch': 1.68}
対照学習損失: 4.773273030878045e-05
対照学習損失: 5.486808004206978e-05
{'loss': 0.0001, 'grad_norm': 0.0025288898032158613, 'learning_rate': 2.593862815884477e-05, 'epoch': 1.68}
対照学習損失: 0.0004662620776798576
対照学習損失: 4.558946966426447e-05
{'loss': 0.0003, 'grad_norm': 0.015078919008374214, 'learning_rate': 2.5920577617328523e-05, 'epoch': 1.68}
対照学習損失: 2.0115990992053412e-05
対照学習損失: 3.6856399674434215e-05
{'loss': 0.0, 'grad_norm': 0.000772718049120158, 'learning_rate': 2.5902527075812276e-05, 'epoch': 1.68}
対照学習損失: 2.1359966922318563e-05
対照学習損失: 0.00021738902432844043
{'loss': 0.0001, 'grad_norm': 0.005014551803469658, 'learning_rate': 2.588447653429603e-05, 'epoch': 1.68}
対照学習損失: 0.00011469107266748324
対照学習損失: 4.935971446684562e-05
{'loss': 0.0001, 'grad_norm': 0.0035791443660855293, 'learning_rate': 2.5866425992779787e-05, 'epoch': 1.69}
対照学習損失: 3.5259687138022855e-05
対照学習損失: 0.00030385801801458
{'loss': 0.0002, 'grad_norm': 0.013198484666645527, 'learning_rate': 2.584837545126354e-05, 'epoch': 1.69}
対照学習損失: 1.1928112144232728e-05
対照学習損失: 3.26121698890347e-05
{'loss': 0.0, 'grad_norm': 0.0009828450856730342, 'learning_rate': 2.5830324909747295e-05, 'epoch': 1.69}
対照学習損失: 8.64486355567351e-05
対照学習損失: 4.94560954393819e-05
{'loss': 0.0001, 'grad_norm': 0.0026953155174851418, 'learning_rate': 2.581227436823105e-05, 'epoch': 1.69}
対照学習損失: 1.7121104974648915e-05
対照学習損失: 0.00042963240412063897
{'loss': 0.0002, 'grad_norm': 0.009213205426931381, 'learning_rate': 2.5794223826714803e-05, 'epoch': 1.69}
対照学習損失: 0.0008521642885170877
対照学習損失: 1.4252552318794187e-05
{'loss': 0.0004, 'grad_norm': 0.053098663687705994, 'learning_rate': 2.5776173285198557e-05, 'epoch': 1.69}
対照学習損失: 3.526508226059377e-05
対照学習損失: 9.626024620956741e-06
{'loss': 0.0, 'grad_norm': 0.0008455011993646622, 'learning_rate': 2.575812274368231e-05, 'epoch': 1.69}
対照学習損失: 0.0002479999093338847
対照学習損失: 2.2835511117591523e-05
{'loss': 0.0001, 'grad_norm': 0.019619354978203773, 'learning_rate': 2.5740072202166064e-05, 'epoch': 1.69}
対照学習損失: 1.8199893020209856e-05
対照学習損失: 1.5429623090312816e-05
{'loss': 0.0, 'grad_norm': 0.0008255496504716575, 'learning_rate': 2.572202166064982e-05, 'epoch': 1.69}
対照学習損失: 5.810835864394903e-05
対照学習損失: 2.1397456293925643e-05
{'loss': 0.0, 'grad_norm': 0.001183096319437027, 'learning_rate': 2.5703971119133575e-05, 'epoch': 1.69}
対照学習損失: 2.8571141228894703e-05
対照学習損失: 4.302932575228624e-05
{'loss': 0.0, 'grad_norm': 0.002881312044337392, 'learning_rate': 2.568592057761733e-05, 'epoch': 1.69}
対照学習損失: 0.00016439113824162632
対照学習損失: 3.775719233090058e-05
{'loss': 0.0001, 'grad_norm': 0.006076249293982983, 'learning_rate': 2.5667870036101083e-05, 'epoch': 1.7}
対照学習損失: 7.420710971928202e-06
対照学習損失: 0.0004106582491658628
{'loss': 0.0002, 'grad_norm': 0.012303904630243778, 'learning_rate': 2.5649819494584837e-05, 'epoch': 1.7}
対照学習損失: 1.180148683488369e-05
対照学習損失: 0.00014001125236973166
{'loss': 0.0001, 'grad_norm': 0.004344646353274584, 'learning_rate': 2.563176895306859e-05, 'epoch': 1.7}
対照学習損失: 5.07525764987804e-05
対照学習損失: 0.0009901392040774226
{'loss': 0.0005, 'grad_norm': 0.030081313103437424, 'learning_rate': 2.5613718411552344e-05, 'epoch': 1.7}
対照学習損失: 8.851199709170032e-06
対照学習損失: 3.3778633223846555e-05
{'loss': 0.0, 'grad_norm': 0.0007590032182633877, 'learning_rate': 2.5595667870036098e-05, 'epoch': 1.7}
対照学習損失: 6.653946911683306e-05
対照学習損失: 6.305683928076178e-05
{'loss': 0.0001, 'grad_norm': 0.003248056164011359, 'learning_rate': 2.557761732851986e-05, 'epoch': 1.7}
対照学習損失: 1.2315580534050241e-05
対照学習損失: 0.0001682183356024325
{'loss': 0.0001, 'grad_norm': 0.004766910802572966, 'learning_rate': 2.5559566787003613e-05, 'epoch': 1.7}
対照学習損失: 0.00034220307134091854
対照学習損失: 3.128275784547441e-05
{'loss': 0.0002, 'grad_norm': 0.013359778560698032, 'learning_rate': 2.5541516245487366e-05, 'epoch': 1.7}
対照学習損失: 0.00018636856111697853
対照学習損失: 4.2813815525732934e-05
{'loss': 0.0001, 'grad_norm': 0.004882548935711384, 'learning_rate': 2.5523465703971124e-05, 'epoch': 1.7}
対照学習損失: 0.000445120211225003
対照学習損失: 3.1439110898645595e-05
{'loss': 0.0002, 'grad_norm': 0.028776202350854874, 'learning_rate': 2.5505415162454877e-05, 'epoch': 1.7}
対照学習損失: 0.00027290606522001326
対照学習損失: 0.0001755153207341209
{'loss': 0.0002, 'grad_norm': 0.00985409040004015, 'learning_rate': 2.548736462093863e-05, 'epoch': 1.7}
対照学習損失: 2.9255903427838348e-05
対照学習損失: 0.00028343286248855293
{'loss': 0.0002, 'grad_norm': 0.008242093026638031, 'learning_rate': 2.5469314079422385e-05, 'epoch': 1.71}
対照学習損失: 6.611412391066551e-05
対照学習損失: 1.350767706753686e-05
{'loss': 0.0, 'grad_norm': 0.001307108672335744, 'learning_rate': 2.545126353790614e-05, 'epoch': 1.71}
対照学習損失: 6.363228021655232e-05
対照学習損失: 1.633884312468581e-05
{'loss': 0.0, 'grad_norm': 0.0016853598644956946, 'learning_rate': 2.5433212996389893e-05, 'epoch': 1.71}
対照学習損失: 1.1577953046071343e-05
対照学習損失: 2.486156517989002e-05
{'loss': 0.0, 'grad_norm': 0.000655476818792522, 'learning_rate': 2.5415162454873646e-05, 'epoch': 1.71}
対照学習損失: 0.0010295734973624349
対照学習損失: 2.6892950700130314e-05
{'loss': 0.0005, 'grad_norm': 0.031329378485679626, 'learning_rate': 2.5397111913357404e-05, 'epoch': 1.71}
対照学習損失: 5.2467603381955996e-05
対照学習損失: 0.00022781021834816784
{'loss': 0.0001, 'grad_norm': 0.012399774976074696, 'learning_rate': 2.5379061371841157e-05, 'epoch': 1.71}
対照学習損失: 2.8830720111727715e-05
対照学習損失: 1.1227850336581469e-05
{'loss': 0.0, 'grad_norm': 0.0013749352656304836, 'learning_rate': 2.536101083032491e-05, 'epoch': 1.71}
対照学習損失: 2.7260510250926018e-05
対照学習損失: 0.0004918770864605904
{'loss': 0.0003, 'grad_norm': 0.011420954950153828, 'learning_rate': 2.5342960288808665e-05, 'epoch': 1.71}
対照学習損失: 5.6929493439383805e-05
対照学習損失: 0.0003277267678640783
{'loss': 0.0002, 'grad_norm': 0.010205930098891258, 'learning_rate': 2.532490974729242e-05, 'epoch': 1.71}
対照学習損失: 0.00015110793174244463
対照学習損失: 1.2151193004683591e-05
{'loss': 0.0001, 'grad_norm': 0.005440485663712025, 'learning_rate': 2.5306859205776173e-05, 'epoch': 1.71}
対照学習損失: 2.7171477995580062e-05
対照学習損失: 2.1956211639917456e-05
{'loss': 0.0, 'grad_norm': 0.0008808975690044463, 'learning_rate': 2.5288808664259927e-05, 'epoch': 1.71}
対照学習損失: 0.00039354292675852776
対照学習損失: 0.007976838387548923
{'loss': 0.0042, 'grad_norm': 0.2350624054670334, 'learning_rate': 2.527075812274368e-05, 'epoch': 1.72}
対照学習損失: 7.429423567373306e-05
対照学習損失: 4.334825280238874e-05
{'loss': 0.0001, 'grad_norm': 0.0029428494162857533, 'learning_rate': 2.5252707581227438e-05, 'epoch': 1.72}
対照学習損失: 4.767820792039856e-05
対照学習損失: 4.862383138970472e-05
{'loss': 0.0, 'grad_norm': 0.0025408018846064806, 'learning_rate': 2.523465703971119e-05, 'epoch': 1.72}
対照学習損失: 2.8399383154464886e-05
対照学習損失: 7.297855336219072e-05
{'loss': 0.0001, 'grad_norm': 0.002944936975836754, 'learning_rate': 2.5216606498194945e-05, 'epoch': 1.72}
対照学習損失: 6.8470299083855934e-06
対照学習損失: 4.55412118753884e-05
{'loss': 0.0, 'grad_norm': 0.0010817470028996468, 'learning_rate': 2.51985559566787e-05, 'epoch': 1.72}
対照学習損失: 1.876749229268171e-05
対照学習損失: 3.7703623092966154e-05
{'loss': 0.0, 'grad_norm': 0.001059497008100152, 'learning_rate': 2.518050541516246e-05, 'epoch': 1.72}
対照学習損失: 8.538998372387141e-05
対照学習損失: 0.000966711260844022
{'loss': 0.0005, 'grad_norm': 0.1016688421368599, 'learning_rate': 2.5162454873646213e-05, 'epoch': 1.72}
対照学習損失: 7.877559983171523e-05
対照学習損失: 0.0004403938364703208
{'loss': 0.0003, 'grad_norm': 0.019193649291992188, 'learning_rate': 2.5144404332129967e-05, 'epoch': 1.72}
対照学習損失: 2.4533293981221505e-05
対照学習損失: 8.866109055816196e-06
{'loss': 0.0, 'grad_norm': 0.0005395711632445455, 'learning_rate': 2.512635379061372e-05, 'epoch': 1.72}
対照学習損失: 0.0002601597807370126
対照学習損失: 7.789245137246326e-05
{'loss': 0.0002, 'grad_norm': 0.010081460699439049, 'learning_rate': 2.5108303249097475e-05, 'epoch': 1.72}
対照学習損失: 0.00019668694585561752
対照学習損失: 0.0003907312930095941
{'loss': 0.0003, 'grad_norm': 0.02902977727353573, 'learning_rate': 2.509025270758123e-05, 'epoch': 1.72}
対照学習損失: 0.0005851141759194434
対照学習損失: 0.0004838005406782031
{'loss': 0.0005, 'grad_norm': 0.022425519302487373, 'learning_rate': 2.5072202166064983e-05, 'epoch': 1.73}
対照学習損失: 5.2513787522912025e-05
対照学習損失: 4.542053648037836e-05
{'loss': 0.0, 'grad_norm': 0.0022103586234152317, 'learning_rate': 2.505415162454874e-05, 'epoch': 1.73}
対照学習損失: 2.483166645106394e-05
対照学習損失: 0.0003397034015506506
{'loss': 0.0002, 'grad_norm': 0.01329986285418272, 'learning_rate': 2.5036101083032494e-05, 'epoch': 1.73}
対照学習損失: 0.00019381222955416888
対照学習損失: 2.4220784325734712e-05
{'loss': 0.0001, 'grad_norm': 0.005882733967155218, 'learning_rate': 2.5018050541516247e-05, 'epoch': 1.73}
対照学習損失: 0.00030432650237344205
対照学習損失: 3.830150672001764e-05
{'loss': 0.0002, 'grad_norm': 0.007351978216320276, 'learning_rate': 2.5e-05, 'epoch': 1.73}
対照学習損失: 1.7292028132942505e-05
対照学習損失: 2.5874625862343237e-05
{'loss': 0.0, 'grad_norm': 0.0005853975308127701, 'learning_rate': 2.4981949458483755e-05, 'epoch': 1.73}
対照学習損失: 1.1578061275940854e-05
対照学習損失: 4.550184166873805e-05
{'loss': 0.0, 'grad_norm': 0.0010041503701359034, 'learning_rate': 2.496389891696751e-05, 'epoch': 1.73}
対照学習損失: 2.655161551956553e-05
対照学習損失: 0.016790036112070084
{'loss': 0.0084, 'grad_norm': 0.6370701193809509, 'learning_rate': 2.4945848375451263e-05, 'epoch': 1.73}
対照学習損失: 0.00012520031305029988
対照学習損失: 0.00010804759222082794
{'loss': 0.0001, 'grad_norm': 0.006180198863148689, 'learning_rate': 2.492779783393502e-05, 'epoch': 1.73}
対照学習損失: 2.0645045879064128e-05
対照学習損失: 0.03176721930503845
{'loss': 0.0159, 'grad_norm': 1.0927749872207642, 'learning_rate': 2.4909747292418774e-05, 'epoch': 1.73}
対照学習損失: 1.5034524039947428e-05
対照学習損失: 5.4816722695250064e-05
{'loss': 0.0, 'grad_norm': 0.002960664452984929, 'learning_rate': 2.489169675090253e-05, 'epoch': 1.73}
対照学習損失: 1.022204196488019e-05
対照学習損失: 0.0015813092468306422
{'loss': 0.0008, 'grad_norm': 0.0493805930018425, 'learning_rate': 2.4873646209386285e-05, 'epoch': 1.74}
対照学習損失: 1.8342963812756352e-05
対照学習損失: 0.00020809263514820486
{'loss': 0.0001, 'grad_norm': 0.007762654218822718, 'learning_rate': 2.485559566787004e-05, 'epoch': 1.74}
対照学習損失: 2.0838702766923234e-05
対照学習損失: 1.595136200194247e-05
{'loss': 0.0, 'grad_norm': 0.0004133960173930973, 'learning_rate': 2.4837545126353792e-05, 'epoch': 1.74}
対照学習損失: 9.82292476692237e-05
対照学習損失: 3.875236143358052e-05
{'loss': 0.0001, 'grad_norm': 0.003614152316004038, 'learning_rate': 2.4819494584837546e-05, 'epoch': 1.74}
対照学習損失: 0.0001103649556171149
対照学習損失: 5.927080201217905e-05
{'loss': 0.0001, 'grad_norm': 0.003118072170764208, 'learning_rate': 2.48014440433213e-05, 'epoch': 1.74}
対照学習損失: 6.417410622816533e-05
対照学習損失: 0.0004902633372694254
{'loss': 0.0003, 'grad_norm': 0.013158515095710754, 'learning_rate': 2.4783393501805054e-05, 'epoch': 1.74}
対照学習損失: 0.0001897297624964267
対照学習損失: 3.495798591757193e-05
{'loss': 0.0001, 'grad_norm': 0.004442029632627964, 'learning_rate': 2.476534296028881e-05, 'epoch': 1.74}
対照学習損失: 0.0004550019802991301
対照学習損失: 0.000692456029355526
{'loss': 0.0006, 'grad_norm': 0.04886827617883682, 'learning_rate': 2.4747292418772565e-05, 'epoch': 1.74}
対照学習損失: 0.0003418337437324226
対照学習損失: 6.755534559488297e-05
{'loss': 0.0002, 'grad_norm': 0.02950643002986908, 'learning_rate': 2.472924187725632e-05, 'epoch': 1.74}
対照学習損失: 0.00015710206935182214
対照学習損失: 2.8094640583731234e-05
{'loss': 0.0001, 'grad_norm': 0.010262597352266312, 'learning_rate': 2.4711191335740072e-05, 'epoch': 1.74}
対照学習損失: 1.4841055417491589e-05
対照学習損失: 2.2306321625364944e-05
{'loss': 0.0, 'grad_norm': 0.000671333575155586, 'learning_rate': 2.469314079422383e-05, 'epoch': 1.75}
対照学習損失: 3.472980097285472e-05
対照学習損失: 6.276895146584138e-05
{'loss': 0.0, 'grad_norm': 0.002138532930985093, 'learning_rate': 2.4675090252707583e-05, 'epoch': 1.75}
対照学習損失: 0.00016192023758776486
対照学習損失: 5.0986189307877794e-05
{'loss': 0.0001, 'grad_norm': 0.0052945553325116634, 'learning_rate': 2.4657039711191337e-05, 'epoch': 1.75}
対照学習損失: 3.982918133260682e-05
対照学習損失: 6.281193054746836e-05
{'loss': 0.0001, 'grad_norm': 0.0018540030578151345, 'learning_rate': 2.463898916967509e-05, 'epoch': 1.75}
対照学習損失: 1.4811404071224388e-05
対照学習損失: 1.0624373317114078e-05
{'loss': 0.0, 'grad_norm': 0.0002726740203797817, 'learning_rate': 2.4620938628158845e-05, 'epoch': 1.75}
対照学習損失: 3.037992792087607e-05
対照学習損失: 9.685642908152658e-06
{'loss': 0.0, 'grad_norm': 0.0008791601867415011, 'learning_rate': 2.46028880866426e-05, 'epoch': 1.75}
対照学習損失: 0.0005532142822630703
対照学習損失: 2.57555930147646e-05
{'loss': 0.0003, 'grad_norm': 0.01984938606619835, 'learning_rate': 2.4584837545126356e-05, 'epoch': 1.75}
対照学習損失: 7.384828495560214e-05
対照学習損失: 3.0805029382463545e-05
{'loss': 0.0001, 'grad_norm': 0.0018214719602838159, 'learning_rate': 2.456678700361011e-05, 'epoch': 1.75}
対照学習損失: 0.00016431916446890682
対照学習損失: 0.0002309434930793941
{'loss': 0.0002, 'grad_norm': 0.00938482116907835, 'learning_rate': 2.4548736462093864e-05, 'epoch': 1.75}
対照学習損失: 0.0006221230141818523
対照学習損失: 6.541574293805752e-06
{'loss': 0.0003, 'grad_norm': 0.08606994152069092, 'learning_rate': 2.4530685920577617e-05, 'epoch': 1.75}
対照学習損失: 7.116385677363724e-05
対照学習損失: 1.2412495379976463e-05
{'loss': 0.0, 'grad_norm': 0.0013048207620158792, 'learning_rate': 2.451263537906137e-05, 'epoch': 1.75}
対照学習損失: 0.00011321669444441795
対照学習損失: 0.00010606146679492667
{'loss': 0.0001, 'grad_norm': 0.006742974743247032, 'learning_rate': 2.449458483754513e-05, 'epoch': 1.76}
対照学習損失: 0.00017376065079588443
対照学習損失: 5.280546611174941e-05
{'loss': 0.0001, 'grad_norm': 0.010461347177624702, 'learning_rate': 2.4476534296028882e-05, 'epoch': 1.76}
対照学習損失: 3.285965067334473e-05
対照学習損失: 5.613815665128641e-05
{'loss': 0.0, 'grad_norm': 0.0032738731242716312, 'learning_rate': 2.4458483754512636e-05, 'epoch': 1.76}
対照学習損失: 4.1204693843610585e-05
対照学習損失: 0.003172313328832388
{'loss': 0.0016, 'grad_norm': 0.13756586611270905, 'learning_rate': 2.444043321299639e-05, 'epoch': 1.76}
対照学習損失: 0.00019749373313970864
対照学習損失: 7.018396900093649e-06
{'loss': 0.0001, 'grad_norm': 0.0052836500108242035, 'learning_rate': 2.4422382671480147e-05, 'epoch': 1.76}
対照学習損失: 8.210474334191531e-06
対照学習損失: 6.945917994016781e-05
{'loss': 0.0, 'grad_norm': 0.0020858836360275745, 'learning_rate': 2.44043321299639e-05, 'epoch': 1.76}
対照学習損失: 3.129038668703288e-05
対照学習損失: 8.196558337658644e-05
{'loss': 0.0001, 'grad_norm': 0.002197264228016138, 'learning_rate': 2.4386281588447655e-05, 'epoch': 1.76}
対照学習損失: 0.00012802091077901423
対照学習損失: 8.866046300681774e-06
{'loss': 0.0001, 'grad_norm': 0.0115776052698493, 'learning_rate': 2.436823104693141e-05, 'epoch': 1.76}
対照学習損失: 2.80116728390567e-05
対照学習損失: 1.3515042155631818e-05
{'loss': 0.0, 'grad_norm': 0.0012812197674065828, 'learning_rate': 2.4350180505415162e-05, 'epoch': 1.76}
対照学習損失: 0.0014939577085897326
対照学習損失: 1.2613679245987441e-05
{'loss': 0.0008, 'grad_norm': 0.060874443501234055, 'learning_rate': 2.4332129963898916e-05, 'epoch': 1.76}
対照学習損失: 7.604451093357056e-05
対照学習損失: 3.101513721048832e-05
{'loss': 0.0001, 'grad_norm': 0.00198006140999496, 'learning_rate': 2.4314079422382673e-05, 'epoch': 1.76}
対照学習損失: 1.228577002621023e-05
対照学習損失: 2.174764813389629e-05
{'loss': 0.0, 'grad_norm': 0.00039951075450517237, 'learning_rate': 2.4296028880866427e-05, 'epoch': 1.77}
対照学習損失: 0.0005018789670430124
対照学習損失: 6.209015555214137e-05
{'loss': 0.0003, 'grad_norm': 0.041433192789554596, 'learning_rate': 2.427797833935018e-05, 'epoch': 1.77}
対照学習損失: 1.8566337530501187e-05
対照学習損失: 7.077993359416723e-06
{'loss': 0.0, 'grad_norm': 0.00031649196171201766, 'learning_rate': 2.4259927797833938e-05, 'epoch': 1.77}
対照学習損失: 2.2603575416724198e-05
対照学習損失: 2.8690294129773974e-05
{'loss': 0.0, 'grad_norm': 0.0008823376847431064, 'learning_rate': 2.4241877256317692e-05, 'epoch': 1.77}
対照学習損失: 2.1188461687415838e-05
対照学習損失: 0.00022858503507450223
{'loss': 0.0001, 'grad_norm': 0.006036943756043911, 'learning_rate': 2.4223826714801446e-05, 'epoch': 1.77}
対照学習損失: 4.194781286059879e-05
対照学習損失: 0.0006485829944722354
{'loss': 0.0003, 'grad_norm': 0.041829727590084076, 'learning_rate': 2.42057761732852e-05, 'epoch': 1.77}
対照学習損失: 0.0001439539628336206
対照学習損失: 3.2325562642654404e-05
{'loss': 0.0001, 'grad_norm': 0.003035277593880892, 'learning_rate': 2.4187725631768953e-05, 'epoch': 1.77}
対照学習損失: 0.00015444160089828074
対照学習損失: 3.6012017517350614e-05
{'loss': 0.0001, 'grad_norm': 0.0061638085171580315, 'learning_rate': 2.4169675090252707e-05, 'epoch': 1.77}
対照学習損失: 1.7664580809650943e-05
対照学習損失: 2.5673292839201167e-05
{'loss': 0.0, 'grad_norm': 0.0006278004148043692, 'learning_rate': 2.415162454873646e-05, 'epoch': 1.77}
対照学習損失: 1.5124147466849536e-05
対照学習損失: 5.40396649739705e-05
{'loss': 0.0, 'grad_norm': 0.0011909108143299818, 'learning_rate': 2.4133574007220215e-05, 'epoch': 1.77}
対照学習損失: 2.5807330530369654e-05
対照学習損失: 0.00014050469326321036
{'loss': 0.0001, 'grad_norm': 0.004396796692162752, 'learning_rate': 2.4115523465703972e-05, 'epoch': 1.77}
対照学習損失: 9.409296762896702e-05
対照学習損失: 2.3863076421548612e-05
{'loss': 0.0001, 'grad_norm': 0.0027835199143737555, 'learning_rate': 2.409747292418773e-05, 'epoch': 1.78}
対照学習損失: 0.0002767676196526736
対照学習損失: 1.5899193385848776e-05
{'loss': 0.0001, 'grad_norm': 0.02680603787302971, 'learning_rate': 2.4079422382671483e-05, 'epoch': 1.78}
対照学習損失: 2.8295773518038914e-05
対照学習損失: 1.0862813724088483e-05
{'loss': 0.0, 'grad_norm': 0.0010234755463898182, 'learning_rate': 2.4061371841155237e-05, 'epoch': 1.78}
対照学習損失: 2.2484997316496447e-05
対照学習損失: 5.0016740715363994e-05
{'loss': 0.0, 'grad_norm': 0.0015296157216653228, 'learning_rate': 2.404332129963899e-05, 'epoch': 1.78}
対照学習損失: 7.82301231083693e-06
対照学習損失: 0.00011465148418210447
{'loss': 0.0001, 'grad_norm': 0.002399853663519025, 'learning_rate': 2.4025270758122745e-05, 'epoch': 1.78}
対照学習損失: 2.9949900635983795e-05
対照学習損失: 6.246170232770965e-05
{'loss': 0.0, 'grad_norm': 0.0012594074942171574, 'learning_rate': 2.40072202166065e-05, 'epoch': 1.78}
対照学習損失: 4.806894503417425e-05
対照学習損失: 5.215431883698329e-05
{'loss': 0.0001, 'grad_norm': 0.001461861771531403, 'learning_rate': 2.3989169675090252e-05, 'epoch': 1.78}
対照学習損失: 1.252418223884888e-05
対照学習損失: 6.429788300010841e-06
{'loss': 0.0, 'grad_norm': 0.00027011169004254043, 'learning_rate': 2.3971119133574006e-05, 'epoch': 1.78}
対照学習損失: 8.277498636743985e-06
対照学習損失: 3.3591335522942245e-05
{'loss': 0.0, 'grad_norm': 0.0007976859342306852, 'learning_rate': 2.3953068592057763e-05, 'epoch': 1.78}
対照学習損失: 2.2215974240680225e-05
対照学習損失: 4.5471166231436655e-05
{'loss': 0.0, 'grad_norm': 0.00239196908660233, 'learning_rate': 2.3935018050541517e-05, 'epoch': 1.78}
対照学習損失: 7.450536031683441e-06
対照学習損失: 2.1792362531414255e-05
{'loss': 0.0, 'grad_norm': 0.0003614278102759272, 'learning_rate': 2.3916967509025274e-05, 'epoch': 1.78}
対照学習損失: 7.40945033612661e-05
対照学習損失: 9.348314051749185e-05
{'loss': 0.0001, 'grad_norm': 0.002901485189795494, 'learning_rate': 2.3898916967509028e-05, 'epoch': 1.79}
対照学習損失: 2.924837826867588e-05
対照学習損失: 0.0003522672632243484
{'loss': 0.0002, 'grad_norm': 0.011507862247526646, 'learning_rate': 2.3880866425992782e-05, 'epoch': 1.79}
対照学習損失: 5.508898902917281e-05
対照学習損失: 0.0013113121967762709
{'loss': 0.0007, 'grad_norm': 0.19598011672496796, 'learning_rate': 2.3862815884476536e-05, 'epoch': 1.79}
対照学習損失: 9.126822078542318e-06
対照学習損失: 0.0001571473985677585
{'loss': 0.0001, 'grad_norm': 0.009845887310802937, 'learning_rate': 2.384476534296029e-05, 'epoch': 1.79}
対照学習損失: 1.2434555173967965e-05
対照学習損失: 1.262845762539655e-05
{'loss': 0.0, 'grad_norm': 0.0003480132145341486, 'learning_rate': 2.3826714801444043e-05, 'epoch': 1.79}
対照学習損失: 0.0014696205034852028
対照学習損失: 6.48097920930013e-05
{'loss': 0.0008, 'grad_norm': 0.04322803020477295, 'learning_rate': 2.3808664259927797e-05, 'epoch': 1.79}
対照学習損失: 6.71291218168335e-06
対照学習損失: 1.5742520190542564e-05
{'loss': 0.0, 'grad_norm': 0.00044484998215921223, 'learning_rate': 2.3790613718411554e-05, 'epoch': 1.79}
対照学習損失: 7.301344885490835e-05
対照学習損失: 6.675320764770731e-05
{'loss': 0.0001, 'grad_norm': 0.003390904515981674, 'learning_rate': 2.3772563176895308e-05, 'epoch': 1.79}
対照学習損失: 0.001228910987265408
対照学習損失: 0.00014436976925935596
{'loss': 0.0007, 'grad_norm': 0.06177247688174248, 'learning_rate': 2.3754512635379062e-05, 'epoch': 1.79}
対照学習損失: 2.2529215129907243e-05
対照学習損失: 7.08244479028508e-05
{'loss': 0.0, 'grad_norm': 0.0014661907916888595, 'learning_rate': 2.3736462093862816e-05, 'epoch': 1.79}
対照学習損失: 2.7447027605376206e-05
対照学習損失: 0.0004649996990337968
{'loss': 0.0002, 'grad_norm': 0.009126320481300354, 'learning_rate': 2.3718411552346573e-05, 'epoch': 1.79}
対照学習損失: 0.00011633880785666406
対照学習損失: 1.8826805899152532e-05
{'loss': 0.0001, 'grad_norm': 0.002479221671819687, 'learning_rate': 2.3700361010830327e-05, 'epoch': 1.8}
対照学習損失: 0.00014943523274268955
対照学習損失: 3.066522185690701e-05
{'loss': 0.0001, 'grad_norm': 0.004091307986527681, 'learning_rate': 2.368231046931408e-05, 'epoch': 1.8}
対照学習損失: 2.1955718693789095e-05
対照学習損失: 1.3820427739119623e-05
{'loss': 0.0, 'grad_norm': 0.0009262703824788332, 'learning_rate': 2.3664259927797834e-05, 'epoch': 1.8}
対照学習損失: 8.018613880267367e-05
対照学習損失: 7.927325896162074e-06
{'loss': 0.0, 'grad_norm': 0.0022885615471750498, 'learning_rate': 2.3646209386281588e-05, 'epoch': 1.8}
対照学習損失: 7.975773041835055e-05
対照学習損失: 1.287442137254402e-05
{'loss': 0.0, 'grad_norm': 0.0029607394244521856, 'learning_rate': 2.3628158844765346e-05, 'epoch': 1.8}
対照学習損失: 0.0025325093884021044
対照学習損失: 2.3766193407936953e-05
{'loss': 0.0013, 'grad_norm': 0.0629723072052002, 'learning_rate': 2.36101083032491e-05, 'epoch': 1.8}
対照学習損失: 0.0002216710417997092
対照学習損失: 4.814163548871875e-05
{'loss': 0.0001, 'grad_norm': 0.013162857852876186, 'learning_rate': 2.3592057761732853e-05, 'epoch': 1.8}
対照学習損失: 4.524722680798732e-05
対照学習損失: 0.0010275858221575618
{'loss': 0.0005, 'grad_norm': 0.08002395927906036, 'learning_rate': 2.3574007220216607e-05, 'epoch': 1.8}
対照学習損失: 0.00010250638297293335
対照学習損失: 3.74410483345855e-05
{'loss': 0.0001, 'grad_norm': 0.0034193084575235844, 'learning_rate': 2.355595667870036e-05, 'epoch': 1.8}
対照学習損失: 0.00032323613413609564
対照学習損失: 2.843730544555001e-05
{'loss': 0.0002, 'grad_norm': 0.013503738678991795, 'learning_rate': 2.3537906137184115e-05, 'epoch': 1.8}
対照学習損失: 2.9092348995618522e-05
対照学習損失: 0.0013873283751308918
{'loss': 0.0007, 'grad_norm': 0.06379351019859314, 'learning_rate': 2.3519855595667872e-05, 'epoch': 1.8}
対照学習損失: 1.1607786291278899e-05
対照学習損失: 1.5988201994332485e-05
{'loss': 0.0, 'grad_norm': 0.0003410762583371252, 'learning_rate': 2.3501805054151626e-05, 'epoch': 1.81}
対照学習損失: 1.454300399927888e-05
対照学習損失: 3.5685505281435326e-05
{'loss': 0.0, 'grad_norm': 0.0009209990967065096, 'learning_rate': 2.348375451263538e-05, 'epoch': 1.81}
対照学習損失: 2.2506712411995977e-05
対照学習損失: 6.214276072569191e-05
{'loss': 0.0, 'grad_norm': 0.001222126418724656, 'learning_rate': 2.3465703971119137e-05, 'epoch': 1.81}
対照学習損失: 5.998551569064148e-05
対照学習損失: 0.00037460701423697174
{'loss': 0.0002, 'grad_norm': 0.026954971253871918, 'learning_rate': 2.344765342960289e-05, 'epoch': 1.81}
対照学習損失: 8.59040483192075e-06
対照学習損失: 0.005809989292174578
{'loss': 0.0029, 'grad_norm': 0.14825797080993652, 'learning_rate': 2.3429602888086644e-05, 'epoch': 1.81}
対照学習損失: 8.215282286982983e-05
対照学習損失: 1.4528333849739283e-05
{'loss': 0.0, 'grad_norm': 0.005469403695315123, 'learning_rate': 2.3411552346570398e-05, 'epoch': 1.81}
対照学習損失: 3.142899367958307e-05
対照学習損失: 1.832696761994157e-05
{'loss': 0.0, 'grad_norm': 0.0030207070522010326, 'learning_rate': 2.3393501805054152e-05, 'epoch': 1.81}
対照学習損失: 1.3939818927610759e-05
対照学習損失: 1.2159009202150628e-05
{'loss': 0.0, 'grad_norm': 0.0004537258355412632, 'learning_rate': 2.3375451263537906e-05, 'epoch': 1.81}
対照学習損失: 1.136926675826544e-05
対照学習損失: 4.5552151277661324e-05
{'loss': 0.0, 'grad_norm': 0.0011328068794682622, 'learning_rate': 2.335740072202166e-05, 'epoch': 1.81}
対照学習損失: 5.060276816948317e-05
対照学習損失: 1.3738470443058759e-05
{'loss': 0.0, 'grad_norm': 0.0009817664977163076, 'learning_rate': 2.3339350180505417e-05, 'epoch': 1.81}
対照学習損失: 2.9537312002503313e-05
対照学習損失: 3.467229544185102e-05
{'loss': 0.0, 'grad_norm': 0.0021906017791479826, 'learning_rate': 2.332129963898917e-05, 'epoch': 1.81}
対照学習損失: 6.274109182413667e-05
対照学習損失: 2.8853968615294434e-05
{'loss': 0.0, 'grad_norm': 0.003204299369826913, 'learning_rate': 2.3303249097472928e-05, 'epoch': 1.82}
対照学習損失: 4.103340324945748e-05
対照学習損失: 8.4939572843723e-05
{'loss': 0.0001, 'grad_norm': 0.003039018251001835, 'learning_rate': 2.328519855595668e-05, 'epoch': 1.82}
対照学習損失: 1.4729421309311874e-05
対照学習損失: 4.610839459928684e-05
{'loss': 0.0, 'grad_norm': 0.0022979394998401403, 'learning_rate': 2.3267148014440435e-05, 'epoch': 1.82}
対照学習損失: 0.00013646429579239339
対照学習損失: 0.0006928264629095793
{'loss': 0.0004, 'grad_norm': 0.042463552206754684, 'learning_rate': 2.324909747292419e-05, 'epoch': 1.82}
対照学習損失: 3.898452268913388e-05
対照学習損失: 5.96060162934009e-05
{'loss': 0.0, 'grad_norm': 0.005664995405822992, 'learning_rate': 2.3231046931407943e-05, 'epoch': 1.82}
対照学習損失: 0.00017158706032205373
対照学習損失: 1.9117553165415302e-05
{'loss': 0.0001, 'grad_norm': 0.0029902360402047634, 'learning_rate': 2.3212996389891697e-05, 'epoch': 1.82}
対照学習損失: 0.000147315178764984
対照学習損失: 1.9102611986454576e-05
{'loss': 0.0001, 'grad_norm': 0.0035140947438776493, 'learning_rate': 2.319494584837545e-05, 'epoch': 1.82}
対照学習損失: 1.2442269508028403e-05
対照学習損失: 7.227945752674714e-05
{'loss': 0.0, 'grad_norm': 0.001947646727785468, 'learning_rate': 2.3176895306859204e-05, 'epoch': 1.82}
対照学習損失: 5.081274139229208e-06
対照学習損失: 1.2852010513597634e-05
{'loss': 0.0, 'grad_norm': 0.00041725343908183277, 'learning_rate': 2.315884476534296e-05, 'epoch': 1.82}
対照学習損失: 2.5261104383389466e-05
対照学習損失: 3.327944432385266e-05
{'loss': 0.0, 'grad_norm': 0.001332792453467846, 'learning_rate': 2.314079422382672e-05, 'epoch': 1.82}
対照学習損失: 4.670919224736281e-05
対照学習損失: 1.9266357412561774e-05
{'loss': 0.0, 'grad_norm': 0.0009535571443848312, 'learning_rate': 2.3122743682310473e-05, 'epoch': 1.82}
対照学習損失: 2.016045618802309e-05
対照学習損失: 4.054284727317281e-05
{'loss': 0.0, 'grad_norm': 0.0007559615187346935, 'learning_rate': 2.3104693140794227e-05, 'epoch': 1.83}
対照学習損失: 0.00010679210390662774
対照学習損失: 6.601636414416134e-05
{'loss': 0.0001, 'grad_norm': 0.0035616769455373287, 'learning_rate': 2.308664259927798e-05, 'epoch': 1.83}
対照学習損失: 2.093557850457728e-05
対照学習損失: 4.873293801210821e-05
{'loss': 0.0, 'grad_norm': 0.0009606403182260692, 'learning_rate': 2.3068592057761734e-05, 'epoch': 1.83}
対照学習損失: 4.379065285320394e-05
対照学習損失: 9.47280423133634e-05
{'loss': 0.0001, 'grad_norm': 0.0049529545940458775, 'learning_rate': 2.3050541516245488e-05, 'epoch': 1.83}
対照学習損失: 5.841210622747894e-06
対照学習損失: 3.5861859942087904e-05
{'loss': 0.0, 'grad_norm': 0.0015371645567938685, 'learning_rate': 2.3032490974729242e-05, 'epoch': 1.83}
対照学習損失: 1.7805956304073334e-05
対照学習損失: 8.180599252227694e-06
{'loss': 0.0, 'grad_norm': 0.00036555854603648186, 'learning_rate': 2.3014440433212996e-05, 'epoch': 1.83}
対照学習損失: 1.5802321286173537e-05
対照学習損失: 6.015772669343278e-05
{'loss': 0.0, 'grad_norm': 0.0016482342034578323, 'learning_rate': 2.2996389891696753e-05, 'epoch': 1.83}
対照学習損失: 7.484914385713637e-05
対照学習損失: 9.955843415809795e-05
{'loss': 0.0001, 'grad_norm': 0.0026119647081941366, 'learning_rate': 2.2978339350180507e-05, 'epoch': 1.83}
対照学習損失: 1.546710882394109e-05
対照学習損失: 4.36004156654235e-05
{'loss': 0.0, 'grad_norm': 0.0010340900626033545, 'learning_rate': 2.296028880866426e-05, 'epoch': 1.83}
対照学習損失: 0.0001732455421006307
対照学習損失: 2.34231301874388e-05
{'loss': 0.0001, 'grad_norm': 0.00486247381195426, 'learning_rate': 2.2942238267148018e-05, 'epoch': 1.83}
対照学習損失: 1.4662343346572015e-05
対照学習損失: 6.452141860791016e-06
{'loss': 0.0, 'grad_norm': 0.00035036623012274504, 'learning_rate': 2.292418772563177e-05, 'epoch': 1.83}
対照学習損失: 2.4241946448455565e-05
対照学習損失: 1.2613649232662283e-05
{'loss': 0.0, 'grad_norm': 0.0009800611296668649, 'learning_rate': 2.2906137184115525e-05, 'epoch': 1.84}
対照学習損失: 9.442090959055349e-05
対照学習損失: 0.004754344932734966
{'loss': 0.0024, 'grad_norm': 0.21736600995063782, 'learning_rate': 2.288808664259928e-05, 'epoch': 1.84}
対照学習損失: 0.00015868032642174512
対照学習損失: 1.2211266039230395e-05
{'loss': 0.0001, 'grad_norm': 0.007016070652753115, 'learning_rate': 2.2870036101083033e-05, 'epoch': 1.84}
対照学習損失: 2.6790477932081558e-05
対照学習損失: 9.53656854107976e-06
{'loss': 0.0, 'grad_norm': 0.0005333704175427556, 'learning_rate': 2.2851985559566787e-05, 'epoch': 1.84}
対照学習損失: 2.9480273951776326e-05
対照学習損失: 3.7203273677732795e-05
{'loss': 0.0, 'grad_norm': 0.0010466467356309295, 'learning_rate': 2.2833935018050544e-05, 'epoch': 1.84}
対照学習損失: 0.00012521175085566938
対照学習損失: 4.031748176203109e-05
{'loss': 0.0001, 'grad_norm': 0.0032925461418926716, 'learning_rate': 2.2815884476534298e-05, 'epoch': 1.84}
対照学習損失: 0.009258691221475601
対照学習損失: 2.0540588593576103e-05
{'loss': 0.0046, 'grad_norm': 0.43070778250694275, 'learning_rate': 2.279783393501805e-05, 'epoch': 1.84}
対照学習損失: 1.4587763871531934e-05
対照学習損失: 3.735395512194373e-05
{'loss': 0.0, 'grad_norm': 0.0007610816974192858, 'learning_rate': 2.2779783393501805e-05, 'epoch': 1.84}
対照学習損失: 2.740132185863331e-05
対照学習損失: 2.118784141202923e-05
{'loss': 0.0, 'grad_norm': 0.001076791901141405, 'learning_rate': 2.276173285198556e-05, 'epoch': 1.84}
対照学習損失: 8.679817256052047e-06
対照学習損失: 9.253508324036375e-06
{'loss': 0.0, 'grad_norm': 0.00026983083807863295, 'learning_rate': 2.2743682310469316e-05, 'epoch': 1.84}
対照学習損失: 4.118738797842525e-05
対照学習損失: 4.728038766188547e-05
{'loss': 0.0, 'grad_norm': 0.0016244598664343357, 'learning_rate': 2.272563176895307e-05, 'epoch': 1.85}
対照学習損失: 0.000160433555720374
対照学習損失: 2.4861265046638437e-05
{'loss': 0.0001, 'grad_norm': 0.003665453987196088, 'learning_rate': 2.2707581227436824e-05, 'epoch': 1.85}
対照学習損失: 4.7662833821959794e-05
対照学習損失: 1.1771741810662206e-05
{'loss': 0.0, 'grad_norm': 0.000900260463822633, 'learning_rate': 2.2689530685920578e-05, 'epoch': 1.85}
対照学習損失: 0.00021876642131246626
対照学習損失: 2.5829553123912774e-05
{'loss': 0.0001, 'grad_norm': 0.0060993279330432415, 'learning_rate': 2.2671480144404335e-05, 'epoch': 1.85}
対照学習損失: 1.6614068954368122e-05
対照学習損失: 0.00018107343930751085
{'loss': 0.0001, 'grad_norm': 0.005148140713572502, 'learning_rate': 2.265342960288809e-05, 'epoch': 1.85}
対照学習損失: 0.000397619791328907
対照学習損失: 3.896137422998436e-05
{'loss': 0.0002, 'grad_norm': 0.019240817055106163, 'learning_rate': 2.2635379061371843e-05, 'epoch': 1.85}
対照学習損失: 9.842070539889392e-06
対照学習損失: 4.2304636735934764e-05
{'loss': 0.0, 'grad_norm': 0.0047034104354679585, 'learning_rate': 2.2617328519855597e-05, 'epoch': 1.85}
対照学習損失: 1.4297373127192259e-05
対照学習損失: 1.5206312127702404e-05
{'loss': 0.0, 'grad_norm': 0.000348552071955055, 'learning_rate': 2.259927797833935e-05, 'epoch': 1.85}
対照学習損失: 4.629540489986539e-05
対照学習損失: 1.4304428077593911e-05
{'loss': 0.0, 'grad_norm': 0.0012776999501511455, 'learning_rate': 2.2581227436823104e-05, 'epoch': 1.85}
対照学習損失: 1.4826398910372518e-05
対照学習損失: 6.275626947171986e-05
{'loss': 0.0, 'grad_norm': 0.0009564005304127932, 'learning_rate': 2.2563176895306858e-05, 'epoch': 1.85}
対照学習損失: 2.187321661040187e-05
対照学習損失: 4.637318852473982e-05
{'loss': 0.0, 'grad_norm': 0.0010717835975810885, 'learning_rate': 2.2545126353790615e-05, 'epoch': 1.85}
対照学習損失: 0.00038703097379766405
対照学習損失: 9.323385893367231e-05
{'loss': 0.0002, 'grad_norm': 0.03100704960525036, 'learning_rate': 2.252707581227437e-05, 'epoch': 1.86}
対照学習損失: 1.3298910744197201e-05
対照学習損失: 0.0002898353268392384
{'loss': 0.0002, 'grad_norm': 0.006985858082771301, 'learning_rate': 2.2509025270758123e-05, 'epoch': 1.86}
対照学習損失: 4.6959401515778154e-05
対照学習損失: 1.5928673747112043e-05
{'loss': 0.0, 'grad_norm': 0.001220293459482491, 'learning_rate': 2.249097472924188e-05, 'epoch': 1.86}
対照学習損失: 6.986435619182885e-05
対照学習損失: 2.661127109604422e-05
{'loss': 0.0, 'grad_norm': 0.003282556775957346, 'learning_rate': 2.2472924187725634e-05, 'epoch': 1.86}
対照学習損失: 0.000140846474096179
対照学習損失: 1.8737726350082085e-05
{'loss': 0.0001, 'grad_norm': 0.0037685486022382975, 'learning_rate': 2.2454873646209388e-05, 'epoch': 1.86}
対照学習損失: 2.7123782274429686e-05
対照学習損失: 3.0782037356402725e-05
{'loss': 0.0, 'grad_norm': 0.0008454027120023966, 'learning_rate': 2.243682310469314e-05, 'epoch': 1.86}
対照学習損失: 8.900605462258682e-05
対照学習損失: 0.00014231126988306642
{'loss': 0.0001, 'grad_norm': 0.005273269489407539, 'learning_rate': 2.2418772563176895e-05, 'epoch': 1.86}
対照学習損失: 3.776139783440158e-05
対照学習損失: 1.458044152968796e-05
{'loss': 0.0, 'grad_norm': 0.000970121007412672, 'learning_rate': 2.240072202166065e-05, 'epoch': 1.86}
対照学習損失: 0.0010385680943727493
対照学習損失: 8.756549505051225e-05
{'loss': 0.0006, 'grad_norm': 0.11083940416574478, 'learning_rate': 2.2382671480144403e-05, 'epoch': 1.86}
対照学習損失: 6.0550024500116706e-05
対照学習損失: 3.508998634060845e-05
{'loss': 0.0, 'grad_norm': 0.0012414284283295274, 'learning_rate': 2.236462093862816e-05, 'epoch': 1.86}
対照学習損失: 1.8328002624912187e-05
対照学習損失: 0.00014655521954409778
{'loss': 0.0001, 'grad_norm': 0.006932910066097975, 'learning_rate': 2.2346570397111914e-05, 'epoch': 1.86}
対照学習損失: 3.8681515434291214e-05
対照学習損失: 2.3453012545360252e-05
{'loss': 0.0, 'grad_norm': 0.0007109997677616775, 'learning_rate': 2.232851985559567e-05, 'epoch': 1.87}
対照学習損失: 0.00014596099208574742
対照学習損失: 2.058382597169839e-05
{'loss': 0.0001, 'grad_norm': 0.005206122528761625, 'learning_rate': 2.2310469314079425e-05, 'epoch': 1.87}
対照学習損失: 2.7452253561932594e-05
対照学習損失: 1.799222081899643e-05
{'loss': 0.0, 'grad_norm': 0.0008807696867734194, 'learning_rate': 2.229241877256318e-05, 'epoch': 1.87}
対照学習損失: 6.60722580505535e-05
対照学習損失: 7.174867732828716e-06
{'loss': 0.0, 'grad_norm': 0.0019379807636141777, 'learning_rate': 2.2274368231046933e-05, 'epoch': 1.87}
対照学習損失: 8.822058589430526e-05
対照学習損失: 7.592056590510765e-06
{'loss': 0.0, 'grad_norm': 0.0015917300479486585, 'learning_rate': 2.2256317689530686e-05, 'epoch': 1.87}
対照学習損失: 2.2595730115426704e-05
対照学習損失: 6.864838360343128e-05
{'loss': 0.0, 'grad_norm': 0.0016509846318513155, 'learning_rate': 2.223826714801444e-05, 'epoch': 1.87}
対照学習損失: 4.7661356802564114e-05
対照学習損失: 1.8752194591797888e-05
{'loss': 0.0, 'grad_norm': 0.0023062853142619133, 'learning_rate': 2.2220216606498194e-05, 'epoch': 1.87}
対照学習損失: 0.0007002524798735976
対照学習損失: 3.1834446417633444e-05
{'loss': 0.0004, 'grad_norm': 0.020888693630695343, 'learning_rate': 2.220216606498195e-05, 'epoch': 1.87}
対照学習損失: 5.4984840971883386e-05
対照学習損失: 0.00118619529530406
{'loss': 0.0006, 'grad_norm': 0.07351532578468323, 'learning_rate': 2.2184115523465705e-05, 'epoch': 1.87}
対照学習損失: 0.0008273374987766147
対照学習損失: 2.9748020097031258e-05
{'loss': 0.0004, 'grad_norm': 0.02719983458518982, 'learning_rate': 2.2166064981949462e-05, 'epoch': 1.87}
対照学習損失: 0.00012644712114706635
対照学習損失: 1.8007520338869654e-05
{'loss': 0.0001, 'grad_norm': 0.008636735379695892, 'learning_rate': 2.2148014440433216e-05, 'epoch': 1.87}
対照学習損失: 4.993181210011244e-05
対照学習損失: 0.00012951699318364263
{'loss': 0.0001, 'grad_norm': 0.006273797247558832, 'learning_rate': 2.212996389891697e-05, 'epoch': 1.88}
対照学習損失: 5.757166582043283e-05
対照学習損失: 1.601838812348433e-05
{'loss': 0.0, 'grad_norm': 0.001083008828572929, 'learning_rate': 2.2111913357400724e-05, 'epoch': 1.88}
対照学習損失: 2.0502782717812806e-05
対照学習損失: 0.0001565614074934274
{'loss': 0.0001, 'grad_norm': 0.0030838351231068373, 'learning_rate': 2.2093862815884478e-05, 'epoch': 1.88}
対照学習損失: 4.843106580665335e-05
対照学習損失: 1.969120785361156e-05
{'loss': 0.0, 'grad_norm': 0.0013439576141536236, 'learning_rate': 2.207581227436823e-05, 'epoch': 1.88}
対照学習損失: 6.208282866282389e-05
対照学習損失: 1.6278696421068162e-05
{'loss': 0.0, 'grad_norm': 0.0023593709338456392, 'learning_rate': 2.2057761732851985e-05, 'epoch': 1.88}
対照学習損失: 1.2360238542896695e-05
対照学習損失: 6.941211177036166e-05
{'loss': 0.0, 'grad_norm': 0.001384821138344705, 'learning_rate': 2.2039711191335742e-05, 'epoch': 1.88}
対照学習損失: 0.0015363427810370922
対照学習損失: 5.2802770369453356e-05
{'loss': 0.0008, 'grad_norm': 0.13656428456306458, 'learning_rate': 2.2021660649819496e-05, 'epoch': 1.88}
対照学習損失: 7.307316263904795e-05
対照学習損失: 4.000947228632867e-06
{'loss': 0.0, 'grad_norm': 0.001656201435253024, 'learning_rate': 2.200361010830325e-05, 'epoch': 1.88}
対照学習損失: 0.0021716991905122995
対照学習損失: 5.710228288080543e-05
{'loss': 0.0011, 'grad_norm': 0.06940066814422607, 'learning_rate': 2.1985559566787004e-05, 'epoch': 1.88}
対照学習損失: 2.3356134988716803e-05
対照学習損失: 7.083360105752945e-05
{'loss': 0.0, 'grad_norm': 0.001411110395565629, 'learning_rate': 2.196750902527076e-05, 'epoch': 1.88}
対照学習損失: 0.00013175401545595378
対照学習損失: 8.389270078623667e-06
{'loss': 0.0001, 'grad_norm': 0.004003243520855904, 'learning_rate': 2.1949458483754515e-05, 'epoch': 1.88}
対照学習損失: 0.00032878652564249933
対照学習損失: 1.5355264622485265e-05
{'loss': 0.0002, 'grad_norm': 0.008052242919802666, 'learning_rate': 2.193140794223827e-05, 'epoch': 1.89}
対照学習損失: 0.0001249661872861907
対照学習損失: 0.0026864255778491497
{'loss': 0.0014, 'grad_norm': 0.10251887142658234, 'learning_rate': 2.1913357400722022e-05, 'epoch': 1.89}
対照学習損失: 5.012477049604058e-05
対照学習損失: 2.1047373593319207e-05
{'loss': 0.0, 'grad_norm': 0.0012601841008290648, 'learning_rate': 2.1895306859205776e-05, 'epoch': 1.89}
対照学習損失: 0.000394618371501565
対照学習損失: 2.460116593283601e-05
{'loss': 0.0002, 'grad_norm': 0.017564568668603897, 'learning_rate': 2.187725631768953e-05, 'epoch': 1.89}
対照学習損失: 3.2570336770731956e-05
対照学習損失: 4.9501231842441484e-05
{'loss': 0.0, 'grad_norm': 0.0012541243340820074, 'learning_rate': 2.1859205776173287e-05, 'epoch': 1.89}
対照学習損失: 1.3671611668542027e-05
対照学習損失: 1.3433038475341164e-05
{'loss': 0.0, 'grad_norm': 0.00048248135135509074, 'learning_rate': 2.184115523465704e-05, 'epoch': 1.89}
対照学習損失: 4.8354063437727746e-06
対照学習損失: 2.5665918656159192e-05
{'loss': 0.0, 'grad_norm': 0.0012749852612614632, 'learning_rate': 2.1823104693140795e-05, 'epoch': 1.89}
対照学習損失: 7.436923624482006e-05
対照学習損失: 5.788258567918092e-05
{'loss': 0.0001, 'grad_norm': 0.0031375479884445667, 'learning_rate': 2.180505415162455e-05, 'epoch': 1.89}
対照学習損失: 0.0013225552393123507
対照学習損失: 3.7921803595963866e-05
{'loss': 0.0007, 'grad_norm': 0.036797963082790375, 'learning_rate': 2.1787003610108303e-05, 'epoch': 1.89}
対照学習損失: 3.824826126219705e-05
対照学習損失: 6.101980943640228e-06
{'loss': 0.0, 'grad_norm': 0.0006850762874819338, 'learning_rate': 2.176895306859206e-05, 'epoch': 1.89}
対照学習損失: 4.541617090580985e-05
対照学習損失: 1.8602675481815822e-05
{'loss': 0.0, 'grad_norm': 0.0014067915035411716, 'learning_rate': 2.1750902527075814e-05, 'epoch': 1.89}
対照学習損失: 0.00016735115787014365
対照学習損失: 8.836255801725201e-06
{'loss': 0.0001, 'grad_norm': 0.004070209804922342, 'learning_rate': 2.1732851985559567e-05, 'epoch': 1.9}
対照学習損失: 1.484100448578829e-05
対照学習損失: 7.189166353782639e-05
{'loss': 0.0, 'grad_norm': 0.0012811659835278988, 'learning_rate': 2.171480144404332e-05, 'epoch': 1.9}
対照学習損失: 2.75348993454827e-05
対照学習損失: 8.82882704900112e-06
{'loss': 0.0, 'grad_norm': 0.000659678946249187, 'learning_rate': 2.169675090252708e-05, 'epoch': 1.9}
対照学習損失: 1.3075294191366993e-05
対照学習損失: 0.0002473086351528764
{'loss': 0.0001, 'grad_norm': 0.016763370484113693, 'learning_rate': 2.1678700361010832e-05, 'epoch': 1.9}
対照学習損失: 0.0001024666999001056
対照学習損失: 4.4640441046794876e-05
{'loss': 0.0001, 'grad_norm': 0.0030094480607658625, 'learning_rate': 2.1660649819494586e-05, 'epoch': 1.9}
対照学習損失: 6.160153861856088e-05
対照学習損失: 2.7654168661683798e-05
{'loss': 0.0, 'grad_norm': 0.0015189801342785358, 'learning_rate': 2.164259927797834e-05, 'epoch': 1.9}
対照学習損失: 1.816412986954674e-05
対照学習損失: 8.701076149009168e-05
{'loss': 0.0001, 'grad_norm': 0.0035720360465347767, 'learning_rate': 2.1624548736462094e-05, 'epoch': 1.9}
対照学習損失: 1.899080234579742e-05
対照学習損失: 0.00017115898663178086
{'loss': 0.0001, 'grad_norm': 0.0038289830554276705, 'learning_rate': 2.1606498194945848e-05, 'epoch': 1.9}
対照学習損失: 1.4557798749592621e-05
対照学習損失: 1.2084673471690621e-05
{'loss': 0.0, 'grad_norm': 0.0004096029733773321, 'learning_rate': 2.15884476534296e-05, 'epoch': 1.9}
対照学習損失: 0.0036460880655795336
対照学習損失: 2.2305674065137282e-05
{'loss': 0.0018, 'grad_norm': 0.10155162960290909, 'learning_rate': 2.157039711191336e-05, 'epoch': 1.9}
対照学習損失: 2.2089610865805298e-05
対照学習損失: 9.666880214354023e-05
{'loss': 0.0001, 'grad_norm': 0.003209258196875453, 'learning_rate': 2.1552346570397112e-05, 'epoch': 1.9}
対照学習損失: 0.0027901181019842625
対照学習損失: 1.157028236775659e-05
{'loss': 0.0014, 'grad_norm': 0.07648918777704239, 'learning_rate': 2.153429602888087e-05, 'epoch': 1.91}
対照学習損失: 6.154126822366379e-06
対照学習損失: 5.942080315435305e-05
{'loss': 0.0, 'grad_norm': 0.0033367278520017862, 'learning_rate': 2.1516245487364623e-05, 'epoch': 1.91}
対照学習損失: 1.8282771634403616e-05
対照学習損失: 1.0222008313576225e-05
{'loss': 0.0, 'grad_norm': 0.00038969350862316787, 'learning_rate': 2.1498194945848377e-05, 'epoch': 1.91}
対照学習損失: 0.0136027866974473
対照学習損失: 1.2360133951005992e-05
{'loss': 0.0068, 'grad_norm': 0.446023166179657, 'learning_rate': 2.148014440433213e-05, 'epoch': 1.91}
対照学習損失: 0.0004271256329957396
対照学習損失: 2.422818579361774e-05
{'loss': 0.0002, 'grad_norm': 0.024577120319008827, 'learning_rate': 2.1462093862815885e-05, 'epoch': 1.91}
対照学習損失: 4.571848694467917e-05
対照学習損失: 0.00016153363685589284
{'loss': 0.0001, 'grad_norm': 0.004802112001925707, 'learning_rate': 2.144404332129964e-05, 'epoch': 1.91}
対照学習損失: 1.9862749468302354e-05
対照学習損失: 0.00010685926099540666
{'loss': 0.0001, 'grad_norm': 0.003115634201094508, 'learning_rate': 2.1425992779783392e-05, 'epoch': 1.91}
対照学習損失: 8.80697334650904e-05
対照学習損失: 0.00019479796173982322
{'loss': 0.0001, 'grad_norm': 0.012749305926263332, 'learning_rate': 2.1407942238267146e-05, 'epoch': 1.91}
対照学習損失: 0.04607708752155304
対照学習損失: 4.484660166781396e-05
{'loss': 0.0231, 'grad_norm': 0.59925776720047, 'learning_rate': 2.1389891696750904e-05, 'epoch': 1.91}
対照学習損失: 1.3112308806739748e-05
対照学習損失: 6.756784569006413e-05
{'loss': 0.0, 'grad_norm': 0.001347093959338963, 'learning_rate': 2.137184115523466e-05, 'epoch': 1.91}
対照学習損失: 3.8516096537932754e-05
対照学習損失: 6.377659701684024e-06
{'loss': 0.0, 'grad_norm': 0.0006600855267606676, 'learning_rate': 2.1353790613718415e-05, 'epoch': 1.91}
対照学習損失: 3.459612707956694e-05
対照学習損失: 1.6658930690027773e-05
{'loss': 0.0, 'grad_norm': 0.0019959723576903343, 'learning_rate': 2.133574007220217e-05, 'epoch': 1.92}
対照学習損失: 4.0564318624092266e-05
対照学習損失: 8.754352165851742e-06
{'loss': 0.0, 'grad_norm': 0.0007678348920308053, 'learning_rate': 2.1317689530685922e-05, 'epoch': 1.92}
対照学習損失: 0.0002212780964327976
対照学習損失: 3.896637736033881e-06
{'loss': 0.0001, 'grad_norm': 0.005111709237098694, 'learning_rate': 2.1299638989169676e-05, 'epoch': 1.92}
対照学習損失: 1.1197809726581909e-05
対照学習損失: 2.2246411390369758e-05
{'loss': 0.0, 'grad_norm': 0.0005365170072764158, 'learning_rate': 2.128158844765343e-05, 'epoch': 1.92}
対照学習損失: 3.216856930521317e-05
対照学習損失: 1.4610284779337235e-05
{'loss': 0.0, 'grad_norm': 0.0007017467869445682, 'learning_rate': 2.1263537906137184e-05, 'epoch': 1.92}
対照学習損失: 8.419056939601433e-06
対照学習損失: 0.008126873522996902
{'loss': 0.0041, 'grad_norm': 0.24734735488891602, 'learning_rate': 2.1245487364620937e-05, 'epoch': 1.92}
対照学習損失: 0.00013910638517700136
対照学習損失: 0.0003320077375974506
{'loss': 0.0002, 'grad_norm': 0.022369399666786194, 'learning_rate': 2.1227436823104695e-05, 'epoch': 1.92}
対照学習損失: 0.0005244340864010155
対照学習損失: 5.3473828302230686e-05
{'loss': 0.0003, 'grad_norm': 0.03223292902112007, 'learning_rate': 2.120938628158845e-05, 'epoch': 1.92}
対照学習損失: 0.00010645759903127328
対照学習損失: 1.693458398221992e-05
{'loss': 0.0001, 'grad_norm': 0.0022615306079387665, 'learning_rate': 2.1191335740072206e-05, 'epoch': 1.92}
対照学習損失: 3.91821340599563e-05
対照学習損失: 0.00012366811279207468
{'loss': 0.0001, 'grad_norm': 0.0047718314453959465, 'learning_rate': 2.117328519855596e-05, 'epoch': 1.92}
対照学習損失: 3.7738198443548754e-05
対照学習損失: 0.00036143301986157894
{'loss': 0.0002, 'grad_norm': 0.010764990001916885, 'learning_rate': 2.1155234657039713e-05, 'epoch': 1.92}
対照学習損失: 0.00018592477135825902
対照学習損失: 0.0006830694619566202
{'loss': 0.0004, 'grad_norm': 0.022105442360043526, 'learning_rate': 2.1137184115523467e-05, 'epoch': 1.93}
対照学習損失: 0.01040017232298851
対照学習損失: 0.0005097535322420299
{'loss': 0.0055, 'grad_norm': 0.5432827472686768, 'learning_rate': 2.111913357400722e-05, 'epoch': 1.93}
対照学習損失: 8.895896826288663e-06
対照学習損失: 7.438647298840806e-05
{'loss': 0.0, 'grad_norm': 0.004148291889578104, 'learning_rate': 2.1101083032490975e-05, 'epoch': 1.93}
対照学習損失: 2.963643055409193e-05
対照学習損失: 0.001967301359400153
{'loss': 0.001, 'grad_norm': 0.06902668625116348, 'learning_rate': 2.108303249097473e-05, 'epoch': 1.93}
対照学習損失: 1.265833452634979e-05
対照学習損失: 3.5565644793678075e-05
{'loss': 0.0, 'grad_norm': 0.0006306635914370418, 'learning_rate': 2.1064981949458486e-05, 'epoch': 1.93}
対照学習損失: 0.00019648556190077215
対照学習損失: 4.954614269081503e-06
{'loss': 0.0001, 'grad_norm': 0.0286727212369442, 'learning_rate': 2.104693140794224e-05, 'epoch': 1.93}
対照学習損失: 1.1130944585602265e-05
対照学習損失: 3.465831105131656e-05
{'loss': 0.0, 'grad_norm': 0.0008211774402298033, 'learning_rate': 2.1028880866425993e-05, 'epoch': 1.93}
対照学習損失: 1.623442949494347e-05
対照学習損失: 5.862394391442649e-05
{'loss': 0.0, 'grad_norm': 0.0025197297800332308, 'learning_rate': 2.1010830324909747e-05, 'epoch': 1.93}
対照学習損失: 2.551615034462884e-05
対照学習損失: 2.615060657262802e-05
{'loss': 0.0, 'grad_norm': 0.0016433355631306767, 'learning_rate': 2.0992779783393504e-05, 'epoch': 1.93}
対照学習損失: 0.0006705812411382794
対照学習損失: 0.0002133717352990061
{'loss': 0.0004, 'grad_norm': 0.023729201406240463, 'learning_rate': 2.0974729241877258e-05, 'epoch': 1.93}
対照学習損失: 4.984399311069865e-06
対照学習損失: 0.00019359526049811393
{'loss': 0.0001, 'grad_norm': 0.005371501203626394, 'learning_rate': 2.0956678700361012e-05, 'epoch': 1.93}
対照学習損失: 9.149218385573477e-06
対照学習損失: 7.764811743982136e-05
{'loss': 0.0, 'grad_norm': 0.0015122507466003299, 'learning_rate': 2.0938628158844766e-05, 'epoch': 1.94}
対照学習損失: 0.01934923604130745
対照学習損失: 4.480285497265868e-05
{'loss': 0.0097, 'grad_norm': 0.4693857729434967, 'learning_rate': 2.092057761732852e-05, 'epoch': 1.94}
対照学習損失: 2.942793071269989e-05
対照学習損失: 0.0007714766543358564
{'loss': 0.0004, 'grad_norm': 0.022784147411584854, 'learning_rate': 2.0902527075812277e-05, 'epoch': 1.94}
対照学習損失: 4.1526200220687315e-05
対照学習損失: 0.0006124370265752077
{'loss': 0.0003, 'grad_norm': 0.0411272831261158, 'learning_rate': 2.088447653429603e-05, 'epoch': 1.94}
対照学習損失: 0.005198131315410137
対照学習損失: 2.4174334612325765e-05
{'loss': 0.0026, 'grad_norm': 0.1531592160463333, 'learning_rate': 2.0866425992779785e-05, 'epoch': 1.94}
対照学習損失: 3.0968560167821124e-05
対照学習損失: 1.7612388546695e-05
{'loss': 0.0, 'grad_norm': 0.0006753310444764793, 'learning_rate': 2.084837545126354e-05, 'epoch': 1.94}
対照学習損失: 9.88791070994921e-05
対照学習損失: 0.0006445692852139473
{'loss': 0.0004, 'grad_norm': 0.017252912744879723, 'learning_rate': 2.0830324909747292e-05, 'epoch': 1.94}
対照学習損失: 6.571369340235833e-06
対照学習損失: 0.00014046594151295722
{'loss': 0.0001, 'grad_norm': 0.0031116886530071497, 'learning_rate': 2.0812274368231046e-05, 'epoch': 1.94}
対照学習損失: 1.5146617442951538e-05
対照学習損失: 4.458996045286767e-05
{'loss': 0.0, 'grad_norm': 0.001369287259876728, 'learning_rate': 2.0794223826714803e-05, 'epoch': 1.94}
対照学習損失: 8.32570149214007e-05
対照学習損失: 0.00017729625687934458
{'loss': 0.0001, 'grad_norm': 0.018432732671499252, 'learning_rate': 2.0776173285198557e-05, 'epoch': 1.94}
対照学習損失: 3.605625533964485e-05
対照学習損失: 8.575530046073254e-06
{'loss': 0.0, 'grad_norm': 0.0006027235649526119, 'learning_rate': 2.075812274368231e-05, 'epoch': 1.94}
対照学習損失: 0.00014702770567964762
対照学習損失: 0.00045590734225697815
{'loss': 0.0003, 'grad_norm': 0.013691348023712635, 'learning_rate': 2.0740072202166068e-05, 'epoch': 1.95}
対照学習損失: 0.0010203812271356583
対照学習損失: 0.00024118475266732275
{'loss': 0.0006, 'grad_norm': 0.03182540833950043, 'learning_rate': 2.0722021660649822e-05, 'epoch': 1.95}
対照学習損失: 0.0011559048434719443
対照学習損失: 0.00014364911476150155
{'loss': 0.0006, 'grad_norm': 0.030112186446785927, 'learning_rate': 2.0703971119133576e-05, 'epoch': 1.95}
対照学習損失: 8.250008249888197e-05
対照学習損失: 0.00014592547086067498
{'loss': 0.0001, 'grad_norm': 0.011047495529055595, 'learning_rate': 2.068592057761733e-05, 'epoch': 1.95}
対照学習損失: 1.1414140317356214e-05
対照学習損失: 7.169552554842085e-05
{'loss': 0.0, 'grad_norm': 0.001546805608086288, 'learning_rate': 2.0667870036101083e-05, 'epoch': 1.95}
対照学習損失: 7.025861123111099e-06
対照学習損失: 0.0006721248500980437
{'loss': 0.0003, 'grad_norm': 0.03422125428915024, 'learning_rate': 2.0649819494584837e-05, 'epoch': 1.95}
対照学習損失: 1.2151701412221882e-05
対照学習損失: 0.00016421038890257478
{'loss': 0.0001, 'grad_norm': 0.003629262326285243, 'learning_rate': 2.063176895306859e-05, 'epoch': 1.95}
対照学習損失: 0.00015187966346275061
対照学習損失: 0.00015223991067614406
{'loss': 0.0002, 'grad_norm': 0.010920824483036995, 'learning_rate': 2.0613718411552345e-05, 'epoch': 1.95}
対照学習損失: 0.00012621338828466833
対照学習損失: 2.845226481440477e-05
{'loss': 0.0001, 'grad_norm': 0.005115291569381952, 'learning_rate': 2.0595667870036102e-05, 'epoch': 1.95}
対照学習損失: 2.092707836709451e-05
対照学習損失: 3.000138167408295e-05
{'loss': 0.0, 'grad_norm': 0.0008616529521532357, 'learning_rate': 2.057761732851986e-05, 'epoch': 1.95}
対照学習損失: 0.001163120032288134
対照学習損失: 2.6999354304280132e-05
{'loss': 0.0006, 'grad_norm': 0.08027400076389313, 'learning_rate': 2.0559566787003613e-05, 'epoch': 1.96}
対照学習損失: 5.9142017562408e-05
対照学習損失: 2.3706863430561498e-05
{'loss': 0.0, 'grad_norm': 0.0010706589091569185, 'learning_rate': 2.0541516245487367e-05, 'epoch': 1.96}
対照学習損失: 0.028929561376571655
対照学習損失: 2.6960624381899834e-05
{'loss': 0.0145, 'grad_norm': 0.7581290602684021, 'learning_rate': 2.052346570397112e-05, 'epoch': 1.96}
対照学習損失: 0.0002515228115953505
対照学習損失: 6.936369572940748e-06
{'loss': 0.0001, 'grad_norm': 0.007164590992033482, 'learning_rate': 2.0505415162454874e-05, 'epoch': 1.96}
対照学習損失: 0.0004398239543661475
対照学習損失: 6.566642696270719e-05
{'loss': 0.0003, 'grad_norm': 0.021796833723783493, 'learning_rate': 2.0487364620938628e-05, 'epoch': 1.96}
対照学習損失: 0.00011148545308969915
対照学習損失: 4.082552914042026e-05
{'loss': 0.0001, 'grad_norm': 0.0028752225916832685, 'learning_rate': 2.0469314079422382e-05, 'epoch': 1.96}
対照学習損失: 1.6070302081061527e-05
対照学習損失: 1.2628552212845534e-05
{'loss': 0.0, 'grad_norm': 0.00030621440964750946, 'learning_rate': 2.0451263537906136e-05, 'epoch': 1.96}
対照学習損失: 0.005111936014145613
対照学習損失: 1.6949559721979313e-05
{'loss': 0.0026, 'grad_norm': 0.2955322265625, 'learning_rate': 2.0433212996389893e-05, 'epoch': 1.96}
対照学習損失: 1.5735360648250207e-05
対照学習損失: 1.2181390957266558e-05
{'loss': 0.0, 'grad_norm': 0.0003087687073275447, 'learning_rate': 2.0415162454873647e-05, 'epoch': 1.96}
対照学習損失: 3.96795803681016e-05
対照学習損失: 8.041891123866662e-05
{'loss': 0.0001, 'grad_norm': 0.0019116876646876335, 'learning_rate': 2.0397111913357404e-05, 'epoch': 1.96}
対照学習損失: 2.5954450393328443e-05
対照学習損失: 7.39526585675776e-05
{'loss': 0.0, 'grad_norm': 0.006256272085011005, 'learning_rate': 2.0379061371841158e-05, 'epoch': 1.96}
対照学習損失: 2.1291709344950505e-05
対照学習損失: 0.0005893722991459072
{'loss': 0.0003, 'grad_norm': 0.060695331543684006, 'learning_rate': 2.0361010830324912e-05, 'epoch': 1.97}
対照学習損失: 0.0003472268581390381
対照学習損失: 1.6383528418373317e-05
{'loss': 0.0002, 'grad_norm': 0.010344518348574638, 'learning_rate': 2.0342960288808666e-05, 'epoch': 1.97}
対照学習損失: 8.493991481373087e-05
対照学習損失: 1.0631764780555386e-05
{'loss': 0.0, 'grad_norm': 0.0024282115045934916, 'learning_rate': 2.032490974729242e-05, 'epoch': 1.97}
対照学習損失: 2.8198730433359742e-05
対照学習損失: 1.8752627511275932e-05
{'loss': 0.0, 'grad_norm': 0.001155084348283708, 'learning_rate': 2.0306859205776173e-05, 'epoch': 1.97}
対照学習損失: 0.0020955034997314215
対照学習損失: 3.308604937046766e-05
{'loss': 0.0011, 'grad_norm': 0.07489548623561859, 'learning_rate': 2.0288808664259927e-05, 'epoch': 1.97}
対照学習損失: 0.0003876399132423103
対照学習損失: 5.130663339514285e-05
{'loss': 0.0002, 'grad_norm': 0.012363674119114876, 'learning_rate': 2.0270758122743684e-05, 'epoch': 1.97}
対照学習損失: 9.048653009813279e-05
対照学習損失: 0.00011340403580106795
{'loss': 0.0001, 'grad_norm': 0.006242293864488602, 'learning_rate': 2.0252707581227438e-05, 'epoch': 1.97}
対照学習損失: 8.543676813133061e-05
対照学習損失: 9.305673302151263e-06
{'loss': 0.0, 'grad_norm': 0.002142750658094883, 'learning_rate': 2.0234657039711192e-05, 'epoch': 1.97}
対照学習損失: 0.00028244982240721583
対照学習損失: 4.9670139560475945e-05
{'loss': 0.0002, 'grad_norm': 0.008498053066432476, 'learning_rate': 2.0216606498194946e-05, 'epoch': 1.97}
対照学習損失: 2.2365897166309878e-05
対照学習損失: 0.0018570981919765472
{'loss': 0.0009, 'grad_norm': 0.16811268031597137, 'learning_rate': 2.0198555956678703e-05, 'epoch': 1.97}
対照学習損失: 0.0004839061584789306
対照学習損失: 2.196378591179382e-05
{'loss': 0.0003, 'grad_norm': 0.014806026592850685, 'learning_rate': 2.0180505415162457e-05, 'epoch': 1.97}
対照学習損失: 3.276591814938001e-05
対照学習損失: 1.6577238056925125e-05
{'loss': 0.0, 'grad_norm': 0.0008161904406733811, 'learning_rate': 2.016245487364621e-05, 'epoch': 1.98}
対照学習損失: 2.875729296647478e-05
対照学習損失: 8.522960706613958e-05
{'loss': 0.0001, 'grad_norm': 0.0038430599961429834, 'learning_rate': 2.0144404332129964e-05, 'epoch': 1.98}
対照学習損失: 1.5824425645405427e-05
対照学習損失: 0.00042380220838822424
{'loss': 0.0002, 'grad_norm': 0.036189500242471695, 'learning_rate': 2.0126353790613718e-05, 'epoch': 1.98}
対照学習損失: 9.42295155255124e-05
対照学習損失: 8.806862751953304e-05
{'loss': 0.0001, 'grad_norm': 0.0037549815606325865, 'learning_rate': 2.0108303249097475e-05, 'epoch': 1.98}
対照学習損失: 4.149613596382551e-05
対照学習損失: 0.0017958979588001966
{'loss': 0.0009, 'grad_norm': 0.15179142355918884, 'learning_rate': 2.009025270758123e-05, 'epoch': 1.98}
対照学習損失: 0.00011837194324471056
対照学習損失: 1.9512495782691985e-05
{'loss': 0.0001, 'grad_norm': 0.002844474744051695, 'learning_rate': 2.0072202166064983e-05, 'epoch': 1.98}
対照学習損失: 4.5248580136103556e-05
対照学習損失: 7.525024557253346e-06
{'loss': 0.0, 'grad_norm': 0.0013336315751075745, 'learning_rate': 2.0054151624548737e-05, 'epoch': 1.98}
対照学習損失: 2.1725469196098857e-05
対照学習損失: 6.079085869714618e-05
{'loss': 0.0, 'grad_norm': 0.0028385818004608154, 'learning_rate': 2.003610108303249e-05, 'epoch': 1.98}
対照学習損失: 2.515938649594318e-05
対照学習損失: 1.8045015167444944e-05
{'loss': 0.0, 'grad_norm': 0.0005553870578296483, 'learning_rate': 2.0018050541516248e-05, 'epoch': 1.98}
対照学習損失: 6.376392411766574e-05
対照学習損失: 1.4036513675819151e-05
{'loss': 0.0, 'grad_norm': 0.0014670484233647585, 'learning_rate': 2e-05, 'epoch': 1.98}
対照学習損失: 0.0033022488933056593
対照学習損失: 2.2633717890130356e-05
{'loss': 0.0017, 'grad_norm': 0.06647603958845139, 'learning_rate': 1.9981949458483755e-05, 'epoch': 1.98}
対照学習損失: 1.9966912077507004e-05
対照学習損失: 0.0006533835548907518
{'loss': 0.0003, 'grad_norm': 0.019621552899479866, 'learning_rate': 1.996389891696751e-05, 'epoch': 1.99}
対照学習損失: 0.00024245152599178255
対照学習損失: 1.5683142919442616e-05
{'loss': 0.0001, 'grad_norm': 0.004613271448761225, 'learning_rate': 1.9945848375451266e-05, 'epoch': 1.99}
対照学習損失: 5.8789533795788884e-05
対照学習損失: 3.9918915717862546e-05
{'loss': 0.0, 'grad_norm': 0.002673783805221319, 'learning_rate': 1.992779783393502e-05, 'epoch': 1.99}
対照学習損失: 2.641824903548695e-05
対照学習損失: 1.2859534763265401e-05
{'loss': 0.0, 'grad_norm': 0.0007315389811992645, 'learning_rate': 1.9909747292418774e-05, 'epoch': 1.99}
対照学習損失: 2.075671909551602e-05
対照学習損失: 4.075451670360053e-06
{'loss': 0.0, 'grad_norm': 0.0003836217219941318, 'learning_rate': 1.9891696750902528e-05, 'epoch': 1.99}
対照学習損失: 0.00019504183728713542
対照学習損失: 2.2894315407029353e-05
{'loss': 0.0001, 'grad_norm': 0.006660789251327515, 'learning_rate': 1.9873646209386282e-05, 'epoch': 1.99}
対照学習損失: 1.858868017734494e-05
対照学習損失: 5.363783202483319e-05
{'loss': 0.0, 'grad_norm': 0.0011135865934193134, 'learning_rate': 1.9855595667870036e-05, 'epoch': 1.99}
対照学習損失: 0.00022093112056609243
対照学習損失: 1.799234451027587e-05
{'loss': 0.0001, 'grad_norm': 0.005185440648347139, 'learning_rate': 1.983754512635379e-05, 'epoch': 1.99}
対照学習損失: 2.7655274607241154e-05
対照学習損失: 9.167155076283962e-05
{'loss': 0.0001, 'grad_norm': 0.0025747520849108696, 'learning_rate': 1.9819494584837547e-05, 'epoch': 1.99}
対照学習損失: 6.250616570468992e-05
対照学習損失: 7.041863136691973e-05
{'loss': 0.0001, 'grad_norm': 0.0026113695930689573, 'learning_rate': 1.98014440433213e-05, 'epoch': 1.99}
対照学習損失: 1.232306931342464e-05
対照学習損失: 0.002947359811514616
{'loss': 0.0015, 'grad_norm': 0.08581393957138062, 'learning_rate': 1.9783393501805054e-05, 'epoch': 1.99}
対照学習損失: 0.00021042069420218468
対照学習損失: 5.178535866434686e-05
{'loss': 0.0001, 'grad_norm': 0.010211417451500893, 'learning_rate': 1.976534296028881e-05, 'epoch': 2.0}
対照学習損失: 4.2136503907386214e-05
対照学習損失: 5.952362698735669e-05
{'loss': 0.0001, 'grad_norm': 0.001458899350836873, 'learning_rate': 1.9747292418772565e-05, 'epoch': 2.0}
対照学習損失: 0.00026612638612277806
対照学習損失: 3.4761116694426164e-05
{'loss': 0.0002, 'grad_norm': 0.005921852774918079, 'learning_rate': 1.972924187725632e-05, 'epoch': 2.0}
対照学習損失: 0.00010678017133614048
対照学習損失: 0.0001532904279883951
{'loss': 0.0001, 'grad_norm': 0.00472474517300725, 'learning_rate': 1.9711191335740073e-05, 'epoch': 2.0}
対照学習損失: 2.5129169443971477e-05
対照学習損失: 0.0001376819855067879
{'loss': 0.0001, 'grad_norm': 0.005771524738520384, 'learning_rate': 1.9693140794223827e-05, 'epoch': 2.0}
対照学習損失: 7.21500109648332e-05
対照学習損失: 1.8676097397474223e-06
{'loss': 0.0, 'grad_norm': 0.0026297573931515217, 'learning_rate': 1.967509025270758e-05, 'epoch': 2.0}
対照学習損失: 6.750107786501758e-06
対照学習損失: 2.9219758289400488e-05
対照学習損失: 8.627620445622597e-06
対照学習損失: 7.733595339232124e-06
対照学習損失: 4.185316356597468e-05
対照学習損失: 3.859385287796613e-06
対照学習損失: 0.000126349477795884
対照学習損失: 4.678943696490023e-06
対照学習損失: 0.00015008570335339755
対照学習損失: 0.00010164124250877649
対照学習損失: 3.2186435419134796e-06
対照学習損失: 1.7686752471490763e-05
対照学習損失: 3.0696198791702045e-06
対照学習損失: 0.0009599998593330383
対照学習損失: 1.3529968782677315e-05
対照学習損失: 3.501761511870427e-06
対照学習損失: 7.927245860628318e-06
対照学習損失: 1.546687781228684e-05
対照学習損失: 3.665673148134374e-06
対照学習損失: 0.0003265438135713339
対照学習損失: 0.0005266173975542188
対照学習損失: 1.4289857062976807e-05
対照学習損失: 2.1308628674887586e-06
対照学習損失: 3.450671647442505e-05
対照学習損失: 6.347854196064873e-06
対照学習損失: 8.165759027178865e-06
対照学習損失: 6.556422249559546e-06
対照学習損失: 2.3543777842860436e-06
対照学習損失: 5.602762030321173e-06
対照学習損失: 2.4586847757746e-06
対照学習損失: 0.0030197675805538893
対照学習損失: 5.826317647006363e-06
対照学習損失: 1.996751961996779e-06
対照学習損失: 6.154477159725502e-05
対照学習損失: 0.0003695814812090248
対照学習損失: 3.74017008653027e-06
対照学習損失: 3.0398296075873077e-06
対照学習損失: 0.0002718015748541802
対照学習損失: 7.420670044666622e-06
対照学習損失: 4.142501893511508e-06
対照学習損失: 3.173769800923765e-05
対照学習損失: 1.4692187505715992e-05
対照学習損失: 0.0007771168602630496
対照学習損失: 7.3015012276300695e-06
対照学習損失: 4.440527391125215e-06
対照学習損失: 0.0007893014699220657
対照学習損失: 4.202108357276302e-06
対照学習損失: 4.1404062358196825e-05
対照学習損失: 3.7699808217439568e-06
対照学習損失: 1.6093238173198188e-06
対照学習損失: 1.9967508251284016e-06
対照学習損失: 0.0001202535058837384
対照学習損失: 1.9370647351024672e-05
対照学習損失: 9.372676686325576e-06
対照学習損失: 1.718046041787602e-05
対照学習損失: 1.43644419949851e-05
対照学習損失: 8.866061762091704e-06
対照学習損失: 4.364219421404414e-05
対照学習損失: 2.577735904196743e-05
対照学習損失: 8.806469850242138e-06
対照学習損失: 1.3544933608500287e-05
対照学習損失: 3.416637991904281e-05
対照学習損失: 0.0012697546044364572
対照学習損失: 7.167361218307633e-06
対照学習損失: 0.04480556398630142
対照学習損失: 4.574617378239054e-06
対照学習損失: 1.4662450666946825e-05
対照学習損失: 1.3336123629414942e-05
対照学習損失: 2.7579379093367606e-05
対照学習損失: 0.0068550389260053635
対照学習損失: 2.682202193682315e-06
対照学習損失: 1.7433858374715783e-05
対照学習損失: 1.494520347478101e-05
対照学習損失: 1.5347737644333392e-05
対照学習損失: 6.273318376770476e-06
対照学習損失: 7.301466212084051e-06
対照学習損失: 1.0683987056836486e-05
対照学習損失: 0.0001626263401703909
対照学習損失: 1.634615364309866e-05
対照学習損失: 2.4288797249027994e-06
対照学習損失: 3.5762618608714547e-06
対照学習損失: 8.031651304918341e-06
対照学習損失: 2.7865071388077922e-06
対照学習損失: 6.736873183399439e-05
対照学習損失: 5.811431037727743e-06
対照学習損失: 4.470332896744367e-06
対照学習損失: 0.00022502268257085234
対照学習損失: 2.5002593247336335e-05
対照学習損失: 1.2025092473777477e-05
対照学習損失: 2.2500703380501363e-06
対照学習損失: 4.3922736949753016e-05
対照学習損失: 8.314748811244499e-06
対照学習損失: 1.5392293789773248e-05
対照学習損失: 5.42400357517181e-06
対照学習損失: 4.097794771951158e-06
対照学習損失: 6.075670171412639e-05
対照学習損失: 1.2442305887816474e-05
対照学習損失: 1.153328048530966e-05
対照学習損失: 8.91078707354609e-06
対照学習損失: 1.6539348507649265e-05
対照学習損失: 0.0001660098205320537
対照学習損失: 0.005337041337043047
対照学習損失: 1.0594379091344308e-05
対照学習損失: 0.0004266606702003628
対照学習損失: 1.6301650248351507e-05
対照学習損失: 1.355968379357364e-05
対照学習損失: 1.9371473172213882e-06
対照学習損失: 5.602803867077455e-06
対照学習損失: 4.066044493811205e-05
対照学習損失: 0.0031925805378705263
対照学習損失: 2.1277957785059698e-05
対照学習損失: 1.3872789168090094e-05
対照学習損失: 3.248439725211938e-06
対照学習損失: 2.841436435119249e-05
対照学習損失: 8.642604370834306e-06
対照学習損失: 1.2129316019127145e-05
対照学習損失: 1.507939123257529e-05
対照学習損失: 3.1469618988921866e-05
対照学習損失: 6.556477273989003e-06
対照学習損失: 0.00017151849169749767
対照学習損失: 2.847441464837175e-05
対照学習損失: 5.200457508180989e-06
対照学習損失: 2.0861589291598648e-06
対照学習損失: 1.0341304005123675e-05
対照学習損失: 9.372741260449402e-06
対照学習損失: 7.254256343003362e-05
対照学習損失: 6.139205197541742e-06
対照学習損失: 3.755076022571302e-06
対照学習損失: 0.0003562111232895404
対照学習損失: 5.677311946783448e-06
対照学習損失: 2.3676955606788397e-05
対照学習損失: 1.6137537386384793e-05
対照学習損失: 0.006242209579795599
対照学習損失: 2.458679546180065e-06
対照学習損失: 8.076342965068761e-06
対照学習損失: 1.1786523828050122e-05
対照学習損失: 8.610002259956673e-05
対照学習損失: 1.4871129678795114e-05
対照学習損失: 6.40744156044093e-06
対照学習損失: 5.818150384584442e-05
対照学習損失: 1.028179440254462e-06
対照学習損失: 1.1235258170927409e-05
対照学習損失: 3.7252775655360892e-06
対照学習損失: 5.90083845963818e-06
対照学習損失: 7.450488737958949e-06
対照学習損失: 0.0001741945743560791
対照学習損失: 6.19883940089494e-06
対照学習損失: 4.109344081371091e-05
対照学習損失: 1.691233592282515e-05
対照学習損失: 2.443784524075454e-06
対照学習損失: 1.2844591765315272e-05
対照学習損失: 0.0005738983163610101
対照学習損失: 9.536606739857234e-06
対照学習損失: 3.6652450944529846e-05
対照学習損失: 7.003431619523326e-06
対照学習損失: 4.708727374236332e-06
対照学習損失: 2.56299108514213e-06
対照学習損失: 1.4796532923355699e-05
対照学習損失: 5.707116997655248e-06
対照学習損失: 2.3244996555149555e-05
対照学習損失: 3.278232270531589e-06
対照学習損失: 4.7534340410493314e-06
対照学習損失: 8.255197826656513e-06
対照学習損失: 4.4554017222253606e-06
対照学習損失: 2.84611542156199e-06
対照学習損失: 7.763384928693995e-06
対照学習損失: 3.05472804029705e-06
対照学習損失: 2.6984689611708745e-05
対照学習損失: 1.3887653040001169e-05
対照学習損失: 0.00032734053093008697
対照学習損失: 5.831175440107472e-05
対照学習損失: 1.6525000319234096e-05
対照学習損失: 1.8477408048056532e-06
対照学習損失: 6.034938451193739e-06
対照学習損失: 5.990222689433722e-06
対照学習損失: 1.183109634439461e-05
対照学習損失: 1.4468574590864591e-05
対照学習損失: 4.753444954985753e-06
対照学習損失: 2.5448962333030067e-05
対照学習損失: 8.493506356899161e-06
対照学習損失: 0.00029243496828712523
対照学習損失: 3.055988418054767e-05
対照学習損失: 2.1367324734455906e-05
対照学習損失: 7.227004061860498e-06
対照学習損失: 0.00016313936794176698
対照学習損失: 2.667298986125388e-06
対照学習損失: 1.1086226550105494e-05
対照学習損失: 1.6391256849601632e-06
対照学習損失: 0.0012946062488481402
対照学習損失: 1.1578051271499135e-05
対照学習損失: 3.6209678455634275e-06
対照学習損失: 0.0017482619732618332
対照学習損失: 1.624139076739084e-05
対照学習損失: 8.478667950839736e-06
対照学習損失: 4.1019309719558805e-05
対照学習損失: 9.238623533747159e-06
対照学習損失: 5.09615165356081e-06
対照学習損失: 6.43722614768194e-06
対照学習損失: 9.472433885093778e-05
対照学習損失: 1.2829740626330022e-05
対照学習損失: 9.536551260680426e-06
対照学習損失: 2.847339419531636e-05
対照学習損失: 7.182229637692217e-06
対照学習損失: 3.0664865334983915e-05
対照学習損失: 3.26334566125297e-06
対照学習損失: 3.93352638639044e-05
対照学習損失: 0.00020301798940636218
対照学習損失: 2.8741680580424145e-05
対照学習損失: 1.2367785529932007e-05
対照学習損失: 0.0006743623525835574
対照学習損失: 2.9206210001575528e-06
対照学習損失: 7.405707037833054e-06
対照学習損失: 4.589527179632569e-06
対照学習損失: 4.649145012081135e-06
対照学習損失: 1.9862171029672027e-05
対照学習損失: 8.210380656237248e-06
対照学習損失: 0.0007014810107648373
対照学習損失: 1.8313046894036233e-05
対照学習損失: 1.9161947420798242e-05
対照学習損失: 2.6790798074216582e-05
対照学習損失: 2.1873305740882643e-05
対照学習損失: 4.72364899906097e-06
対照学習損失: 2.0443634639377706e-05
対照学習損失: 7.390905921056401e-06
対照学習損失: 4.329777220846154e-05
対照学習損失: 2.861015673261136e-06
対照学習損失: 0.000766245590057224
対照学習損失: 2.60769820670248e-06
対照学習損失: 6.258434041228611e-06
対照学習損失: 1.5034980606287718e-05
対照学習損失: 0.00012896944826934487
対照学習損失: 2.0458843209780753e-05
対照学習損失: 9.812654025154188e-05
対照学習損失: 1.837263334891759e-05
対照学習損失: 6.213698270585155e-06
対照学習損失: 3.318264862173237e-05
対照学習損失: 4.748435821966268e-05
対照学習損失: 5.271051486488432e-05
対照学習損失: 5.036566108174156e-06
対照学習損失: 1.2322920156293549e-05
対照学習損失: 0.0007737950654700398
対照学習損失: 3.327168451505713e-05
対照学習損失: 1.8044820535578765e-05
対照学習損失: 1.5317604265874252e-05
対照学習損失: 0.002725803991779685
対照学習損失: 0.0004351792158558965
対照学習損失: 0.00031950019183568656
対照学習損失: 1.3157479770598002e-05
対照学習損失: 5.677311492036097e-06
対照学習損失: 5.230269380263053e-06
対照学習損失: 4.038196493638679e-06
対照学習損失: 0.00016515831521246582
対照学習損失: 0.014106950722634792
対照学習損失: 0.0016555526526644826
対照学習損失: 8.716975571587682e-06
対照学習損失: 3.039827106476878e-06
対照学習損失: 3.44215868608444e-06
対照学習損失: 9.5813220468699e-06
対照学習損失: 0.00012143656931584701
対照学習損失: 2.753498665697407e-05
対照学習損失: 3.908373764716089e-05
対照学習損失: 8.433933544438332e-06
対照学習損失: 9.357690942124464e-06
対照学習損失: 7.902833021944389e-05
対照学習損失: 2.5718087272252887e-05
対照学習損失: 5.268332824925892e-05
対照学習損失: 5.900809810555074e-06
対照学習損失: 2.29477427637903e-06
対照学習損失: 1.466216599510517e-05
対照学習損失: 0.00024153041886165738
対照学習損失: 4.6491395551129244e-06
対照学習損失: 0.00041276178671978414
対照学習損失: 5.260087164060678e-06
対照学習損失: 0.00011969065963057801
対照学習損失: 4.634224296751199e-06
対照学習損失: 5.862427860847674e-05
対照学習損失: 1.8626412838784745e-06
対照学習損失: 3.054724402318243e-06
対照学習損失: 1.4498493328574114e-05
対照学習損失: 0.002961288206279278
対照学習損失: 8.019765664357692e-05
対照学習損失: 6.601163931918563e-06
対照学習損失: 2.0130424672970548e-05
対照学習損失: 1.3411033705779118e-06
対照学習損失: 3.933886546292342e-06
対照学習損失: 3.2037385153671494e-06
対照学習損失: 0.00012366475129965693
対照学習損失: 6.0200313782843295e-06
対照学習損失: 5.724562652176246e-05
対照学習損失: 8.273861021734774e-05
対照学習損失: 7.003499376878608e-06
対照学習損失: 2.3543766474176664e-06
対照学習損失: 0.003180177416652441
対照学習損失: 1.0117695637745783e-05
対照学習損失: 8.135937605402432e-06
対照学習損失: 4.536637061391957e-05
対照学習損失: 2.324575916645699e-06
対照学習損失: 2.488488007656997e-06
対照学習損失: 3.2782427297206596e-06
対照学習損失: 9.596235940989573e-06
対照学習損失: 7.265796011779457e-05
対照学習損失: 1.0743566235760227e-05
対照学習損失: 2.9504203666874673e-06
対照学習損失: 5.289893124427181e-06
対照学習損失: 3.306458529550582e-05
対照学習損失: 5.274987415759824e-06
対照学習損失: 7.897543582657818e-06
対照学習損失: 8.257392619270831e-05
対照学習損失: 0.0004871538549195975
対照学習損失: 1.3947258594271261e-05
対照学習損失: 2.3349108232650906e-05
対照学習損失: 1.8328402120459941e-06
対照学習損失: 0.0001894448505481705
対照学習損失: 5.871009307156783e-06
対照学習損失: 4.500135219132062e-06
対照学習損失: 3.6507751701719826e-06
対照学習損失: 2.0712584500870435e-06
対照学習損失: 1.5377408999484032e-05
対照学習損失: 2.7177815354662016e-05
対照学習損失: 1.7136314909294015e-06
対照学習損失: 5.274970590107841e-06
対照学習損失: 0.0022450401447713375
対照学習損失: 5.572995632974198e-06
対照学習損失: 1.4483623090200126e-05
対照学習損失: 1.9371464077266864e-06
対照学習損失: 1.8030373212241102e-06
対照学習損失: 0.00022463928326033056
対照学習損失: 8.016718311409932e-06
対照学習損失: 0.0038413084112107754
対照学習損失: 8.806521691440139e-06
対照学習損失: 0.00031600677175447345
対照学習損失: 4.7683497541584074e-06
対照学習損失: 8.25518600322539e-06
対照学習損失: 2.667301487235818e-06
対照学習損失: 2.1367515728343278e-05
対照学習損失: 7.423086935887113e-05
対照学習損失: 2.171033702325076e-05
対照学習損失: 2.4884898266464006e-06
対照学習損失: 8.404161235375796e-06
対照学習損失: 0.001187879708595574
対照学習損失: 9.268400390283205e-06
対照学習損失: 0.0002498673857189715
対照学習損失: 4.145134153077379e-05
対照学習損失: 0.0016149262664839625
対照学習損失: 1.1250149327679537e-05
対照学習損失: 2.0264660633984022e-05
対照学習損失: 1.6107735063997097e-05
対照学習損失: 5.841212441737298e-06
対照学習損失: 1.4423829270526767e-05
対照学習損失: 1.2814758520107716e-05
対照学習損失: 6.363004649756476e-05
対照学習損失: 1.311300820816541e-06
対照学習損失: 8.955537850852124e-06
対照学習損失: 5.533630246645771e-05
対照学習損失: 0.00042002322152256966
対照学習損失: 7.2568018367746845e-06
対照学習損失: 7.465400358341867e-06
対照学習損失: 3.367647877894342e-06
対照学習損失: 0.0222487635910511
対照学習損失: 6.0498073253256734e-06
対照学習損失: 1.8238330085296184e-05
対照学習損失: 3.233545612602029e-06
対照学習損失: 0.00010128204303327948
対照学習損失: 6.6905740823131055e-06
対照学習損失: 8.44880923978053e-06
対照学習損失: 2.0995186787331477e-05
対照学習損失: 2.533191945985891e-06
対照学習損失: 0.0023539960384368896
対照学習損失: 0.0003737223451025784
対照学習損失: 0.00018074034596793354
対照学習損失: 6.034897523932159e-06
対照学習損失: 2.001127359108068e-05
対照学習損失: 1.6495088857482187e-05
対照学習損失: 5.0669561460381374e-05
対照学習損失: 9.49189779930748e-06
対照学習損失: 1.850684202509001e-05
対照学習損失: 2.263419264636468e-05
対照学習損失: 3.138052124995738e-05
対照学習損失: 1.3411022337095346e-06
対照学習損失: 0.00013549785944633186
対照学習損失: 1.1756706953747198e-05
対照学習損失: 4.962047114531742e-06
対照学習損失: 0.0053862170316278934
対照学習損失: 1.7285021385760047e-05
対照学習損失: 1.7463469703216106e-05
対照学習損失: 4.902457931166282e-06
対照学習損失: 2.7567075449042022e-06
対照学習損失: 5.543203769775573e-06
対照学習損失: 5.9455328482727055e-06
対照学習損失: 1.6450529074063525e-05
対照学習損失: 2.1143259800737724e-05
対照学習損失: 2.1322290194802918e-05
対照学習損失: 3.948791345464997e-06
対照学習損失: 1.0683786968002096e-05
対照学習損失: 2.1903948436374776e-05
対照学習損失: 0.0010084083769470453
対照学習損失: 3.060403105337173e-05
対照学習損失: 9.134145329880994e-06
対照学習損失: 5.70706151847844e-06
対照学習損失: 2.407912688795477e-05
対照学習損失: 2.473584572726395e-06
対照学習損失: 2.8698132155113854e-05
対照学習損失: 1.9594383047660813e-05
対照学習損失: 0.0002411257300991565
対照学習損失: 9.23847528611077e-06
対照学習損失: 7.875538722146302e-05
対照学習損失: 3.765188012039289e-05
対照学習損失: 6.824681804573629e-06
対照学習損失: 4.6307693992275745e-05
対照学習損失: 1.3515010323317256e-05
対照学習損失: 2.4973531253635883e-05
対照学習損失: 8.389242793782614e-06
対照学習損失: 0.0007814459386281669
対照学習損失: 2.3304688511416316e-05
対照学習損失: 8.463762242172379e-06
対照学習損失: 1.7672373360255733e-05
対照学習損失: 2.399082404735964e-06
対照学習損失: 5.409043751569698e-06
対照学習損失: 2.6121240807697177e-05
対照学習損失: 5.1855713536497205e-06
対照学習損失: 8.527494355803356e-05
対照学習損失: 1.2740234524244443e-05
対照学習損失: 1.4155719327391125e-05
対照学習損失: 0.0013283517910167575
対照学習損失: 8.463730409857817e-06
対照学習損失: 8.985349268186837e-06
対照学習損失: 2.953107286884915e-05
対照学習損失: 6.854481398477219e-06
対照学習損失: 0.00020569941261783242
対照学習損失: 2.2604319383390248e-05
対照学習損失: 0.0001140381209552288
対照学習損失: 7.867717613407876e-06
対照学習損失: 1.5869050912442617e-05
対照学習損失: 1.253158461622661e-05
対照学習損失: 4.4107227950007655e-06
対照学習損失: 7.763402209093329e-06
対照学習損失: 3.360067057656124e-05
対照学習損失: 1.4707051377627067e-05
対照学習損失: 2.8115788154536858e-05
対照学習損失: 2.6673008051147917e-06
対照学習損失: 2.1412048226920888e-05
対照学習損失: 2.0592617147485726e-05
対照学習損失: 2.396003037574701e-05
対照学習損失: 6.154123184387572e-06
対照学習損失: 0.000387361302273348
対照学習損失: 9.149193829216529e-06
対照学習損失: 1.8774482668959536e-05
対照学習損失: 5.810705624753609e-05
対照学習損失: 9.61114710662514e-06
対照学習損失: 1.533284921606537e-05
対照学習損失: 2.2619255105382763e-05
対照学習損失: 0.00035493168979883194
対照学習損失: 0.0001237377873621881
対照学習損失: 8.508466635248624e-06
対照学習損失: 8.902262197807431e-05
対照学習損失: 1.3276647223392501e-05
対照学習損失: 5.975312888040207e-06
対照学習損失: 3.6060700949747115e-06
対照学習損失: 9.548117668600753e-05
対照学習損失: 0.0027539392467588186
対照学習損失: 6.6458223955123685e-06
対照学習損失: 2.178460272261873e-05
対照学習損失: 2.0563002181006595e-05
対照学習損失: 9.179037078865804e-06
対照学習損失: 3.3080414141295478e-06
対照学習損失: 3.3851283660624176e-05
対照学習損失: 5.0663534238992725e-06
対照学習損失: 1.6316545952577144e-05
対照学習損失: 1.1279850696155336e-05
対照学習損失: 2.5778963390621357e-06
対照学習損失: 6.552699051098898e-05
対照学習損失: 0.0021438966505229473
対照学習損失: 3.933429979952052e-05
対照学習損失: 2.2172400349518284e-05
対照学習損失: 1.6093223393909284e-06
対照学習損失: 9.029940883920062e-06
対照学習損失: 1.549718831483915e-06
対照学習損失: 0.00021854924852959812
対照学習損失: 1.0073006706079468e-05
対照学習損失: 5.274953309708508e-06
対照学習損失: 3.2335422019968973e-06
対照学習損失: 4.1425023482588585e-06
対照学習損失: 7.390886821667664e-06
対照学習損失: 3.561364792403765e-06
対照学習損失: 3.576278118089249e-07
{'eval_loss': 0.0003488188376650214, 'eval_runtime': 54.3643, 'eval_samples_per_second': 71.297, 'eval_steps_per_second': 8.921, 'epoch': 2.0}
対照学習損失: 4.030745003547054e-06
対照学習損失: 3.663045208668336e-05
{'loss': 0.0, 'grad_norm': 0.0007256861426867545, 'learning_rate': 1.9657039711191334e-05, 'epoch': 2.0}
対照学習損失: 3.2363252103095874e-05
対照学習損失: 7.111841114237905e-05
{'loss': 0.0001, 'grad_norm': 0.0017847665585577488, 'learning_rate': 1.963898916967509e-05, 'epoch': 2.0}
対照学習損失: 0.0002820380905177444
対照学習損失: 0.00012305630662012845
{'loss': 0.0002, 'grad_norm': 0.010013301856815815, 'learning_rate': 1.9620938628158845e-05, 'epoch': 2.0}
対照学習損失: 1.4431404451897833e-05
対照学習損失: 2.929421498265583e-05
{'loss': 0.0, 'grad_norm': 0.0006630165153183043, 'learning_rate': 1.9602888086642603e-05, 'epoch': 2.0}
対照学習損失: 2.3781361960573122e-05
対照学習損失: 5.252640221442562e-06
{'loss': 0.0, 'grad_norm': 0.0003137376916129142, 'learning_rate': 1.9584837545126356e-05, 'epoch': 2.0}
対照学習損失: 0.00011805043322965503
対照学習損失: 1.225603682541987e-05
{'loss': 0.0001, 'grad_norm': 0.002669818000867963, 'learning_rate': 1.956678700361011e-05, 'epoch': 2.01}
対照学習損失: 7.882661520852707e-06
対照学習損失: 6.395491072908044e-05
{'loss': 0.0, 'grad_norm': 0.0016999444924294949, 'learning_rate': 1.9548736462093864e-05, 'epoch': 2.01}
対照学習損失: 3.071665560128167e-05
対照学習損失: 4.006158997071907e-05
{'loss': 0.0, 'grad_norm': 0.001205430831760168, 'learning_rate': 1.9530685920577618e-05, 'epoch': 2.01}
対照学習損失: 0.00026417025947012007
対照学習損失: 0.00012123212218284607
{'loss': 0.0002, 'grad_norm': 0.008145834319293499, 'learning_rate': 1.951263537906137e-05, 'epoch': 2.01}
対照学習損失: 3.914698027074337e-05
対照学習損失: 3.2967487641144544e-05
{'loss': 0.0, 'grad_norm': 0.0010299620917066932, 'learning_rate': 1.9494584837545125e-05, 'epoch': 2.01}
対照学習損失: 1.6375843188143335e-05
対照学習損失: 6.04239266976947e-06
{'loss': 0.0, 'grad_norm': 0.0005142457084730268, 'learning_rate': 1.9476534296028883e-05, 'epoch': 2.01}
対照学習損失: 9.670757208368741e-06
対照学習損失: 0.0003415395913179964
{'loss': 0.0002, 'grad_norm': 0.010162673890590668, 'learning_rate': 1.9458483754512636e-05, 'epoch': 2.01}
対照学習損失: 5.923186108702794e-06
対照学習損失: 2.7401400075177662e-05
{'loss': 0.0, 'grad_norm': 0.0004958398640155792, 'learning_rate': 1.944043321299639e-05, 'epoch': 2.01}
対照学習損失: 9.73769710981287e-05
対照学習損失: 9.760145985637791e-06
{'loss': 0.0001, 'grad_norm': 0.0029154648073017597, 'learning_rate': 1.9422382671480147e-05, 'epoch': 2.01}
対照学習損失: 3.204340100637637e-05
対照学習損失: 6.353440403472632e-05
{'loss': 0.0, 'grad_norm': 0.0014372539008036256, 'learning_rate': 1.94043321299639e-05, 'epoch': 2.01}
対照学習損失: 0.0001287388731725514
対照学習損失: 0.00019806611817330122
{'loss': 0.0002, 'grad_norm': 0.007338907103985548, 'learning_rate': 1.9386281588447655e-05, 'epoch': 2.01}
対照学習損失: 5.116729880683124e-05
対照学習損失: 6.533951091114432e-05
{'loss': 0.0001, 'grad_norm': 0.0027997863944619894, 'learning_rate': 1.936823104693141e-05, 'epoch': 2.02}
対照学習損失: 9.827171197684947e-06
対照学習損失: 1.141410757554695e-05
{'loss': 0.0, 'grad_norm': 0.0004002675705123693, 'learning_rate': 1.9350180505415163e-05, 'epoch': 2.02}
対照学習損失: 0.00015491197700612247
対照学習損失: 5.721118577639572e-05
{'loss': 0.0001, 'grad_norm': 0.006011168472468853, 'learning_rate': 1.9332129963898917e-05, 'epoch': 2.02}
対照学習損失: 1.969782533706166e-05
対照学習損失: 4.1157636587740853e-05
{'loss': 0.0, 'grad_norm': 0.00329143600538373, 'learning_rate': 1.9314079422382674e-05, 'epoch': 2.02}
対照学習損失: 8.918264938984066e-06
対照学習損失: 0.00037133635487407446
{'loss': 0.0002, 'grad_norm': 0.033781442791223526, 'learning_rate': 1.9296028880866428e-05, 'epoch': 2.02}
対照学習損失: 9.506842616247013e-06
対照学習損失: 4.6590623242082074e-05
{'loss': 0.0, 'grad_norm': 0.0008446607971563935, 'learning_rate': 1.927797833935018e-05, 'epoch': 2.02}
対照学習損失: 3.0417490052059293e-05
対照学習損失: 0.00014519253454636782
{'loss': 0.0001, 'grad_norm': 0.0031928038224577904, 'learning_rate': 1.9259927797833935e-05, 'epoch': 2.02}
対照学習損失: 1.0490317436051555e-05
対照学習損失: 4.1659644921310246e-05
{'loss': 0.0, 'grad_norm': 0.002424142323434353, 'learning_rate': 1.924187725631769e-05, 'epoch': 2.02}
対照学習損失: 4.178691597189754e-05
対照学習損失: 0.0001872624270617962
{'loss': 0.0001, 'grad_norm': 0.005068854428827763, 'learning_rate': 1.9223826714801446e-05, 'epoch': 2.02}
対照学習損失: 5.4707779781892896e-05
対照学習損失: 4.7770136006874964e-05
{'loss': 0.0001, 'grad_norm': 0.001571614295244217, 'learning_rate': 1.92057761732852e-05, 'epoch': 2.02}
対照学習損失: 3.088126686634496e-05
対照学習損失: 3.1162599043454975e-05
{'loss': 0.0, 'grad_norm': 0.0009862981969490647, 'learning_rate': 1.9187725631768954e-05, 'epoch': 2.02}
対照学習損失: 0.0001664890005486086
対照学習損失: 0.0005393957835622132
{'loss': 0.0004, 'grad_norm': 0.034700166434049606, 'learning_rate': 1.9169675090252708e-05, 'epoch': 2.03}
対照学習損失: 5.021668130211765e-06
対照学習損失: 3.4353208320681006e-05
{'loss': 0.0, 'grad_norm': 0.0004589086165651679, 'learning_rate': 1.915162454873646e-05, 'epoch': 2.03}
対照学習損失: 7.680966518819332e-05
対照学習損失: 0.0007854925352148712
{'loss': 0.0004, 'grad_norm': 0.05481795221567154, 'learning_rate': 1.913357400722022e-05, 'epoch': 2.03}
対照学習損失: 8.851086931827012e-06
対照学習損失: 6.0682534240186214e-05
{'loss': 0.0, 'grad_norm': 0.003069311147555709, 'learning_rate': 1.9115523465703973e-05, 'epoch': 2.03}
対照学習損失: 6.112230767030269e-05
対照学習損失: 1.5198480468825437e-05
{'loss': 0.0, 'grad_norm': 0.0023979695979505777, 'learning_rate': 1.9097472924187726e-05, 'epoch': 2.03}
対照学習損失: 4.6331049816217273e-05
対照学習損失: 1.7999795090872794e-05
{'loss': 0.0, 'grad_norm': 0.0013746886979788542, 'learning_rate': 1.907942238267148e-05, 'epoch': 2.03}
対照学習損失: 9.10258968360722e-05
対照学習損失: 3.259429649915546e-05
{'loss': 0.0001, 'grad_norm': 0.00260509317740798, 'learning_rate': 1.9061371841155234e-05, 'epoch': 2.03}
対照学習損失: 0.00012871494982391596
対照学習損失: 5.5580981097591575e-06
{'loss': 0.0001, 'grad_norm': 0.006336403079330921, 'learning_rate': 1.9043321299638988e-05, 'epoch': 2.03}
対照学習損失: 2.8198192012496293e-05
対照学習損失: 2.851843055395875e-05
{'loss': 0.0, 'grad_norm': 0.0012137136654928327, 'learning_rate': 1.9025270758122745e-05, 'epoch': 2.03}
対照学習損失: 0.0006928194779902697
対照学習損失: 6.494403351098299e-05
{'loss': 0.0004, 'grad_norm': 0.031197551637887955, 'learning_rate': 1.90072202166065e-05, 'epoch': 2.03}
対照学習損失: 1.7500806279713288e-05
対照学習損失: 4.9204205424757674e-05
{'loss': 0.0, 'grad_norm': 0.0007960285292938352, 'learning_rate': 1.8989169675090253e-05, 'epoch': 2.03}
対照学習損失: 2.68283074547071e-05
対照学習損失: 8.270085345429834e-06
{'loss': 0.0, 'grad_norm': 0.0005435948260128498, 'learning_rate': 1.897111913357401e-05, 'epoch': 2.04}
対照学習損失: 4.835524669033475e-05
対照学習損失: 8.635132871859241e-06
{'loss': 0.0, 'grad_norm': 0.0012848193291574717, 'learning_rate': 1.8953068592057764e-05, 'epoch': 2.04}
対照学習損失: 4.0758997783996165e-05
対照学習損失: 1.715799226076342e-05
{'loss': 0.0, 'grad_norm': 0.0007230211049318314, 'learning_rate': 1.8935018050541517e-05, 'epoch': 2.04}
対照学習損失: 5.388536737882532e-05
対照学習損失: 0.0003698551736306399
{'loss': 0.0002, 'grad_norm': 0.023511137813329697, 'learning_rate': 1.891696750902527e-05, 'epoch': 2.04}
対照学習損失: 2.065239823423326e-05
対照学習損失: 7.03067853464745e-05
{'loss': 0.0, 'grad_norm': 0.0014871367020532489, 'learning_rate': 1.8898916967509025e-05, 'epoch': 2.04}
対照学習損失: 2.4288248823722824e-05
対照学習損失: 0.00012753435294143856
{'loss': 0.0001, 'grad_norm': 0.0034194556064903736, 'learning_rate': 1.888086642599278e-05, 'epoch': 2.04}
対照学習損失: 0.00010153350012842566
対照学習損失: 1.4759042642253917e-05
{'loss': 0.0001, 'grad_norm': 0.0027966320049017668, 'learning_rate': 1.8862815884476533e-05, 'epoch': 2.04}
対照学習損失: 2.2677413653582335e-05
対照学習損失: 0.00010793461842695251
{'loss': 0.0001, 'grad_norm': 0.007064833305776119, 'learning_rate': 1.884476534296029e-05, 'epoch': 2.04}
対照学習損失: 1.840206459746696e-05
対照学習損失: 1.5697900380473584e-05
{'loss': 0.0, 'grad_norm': 0.00045561493607237935, 'learning_rate': 1.8826714801444044e-05, 'epoch': 2.04}
対照学習損失: 4.401174737722613e-05
対照学習損失: 4.754521432914771e-05
{'loss': 0.0, 'grad_norm': 0.0011578669073060155, 'learning_rate': 1.88086642599278e-05, 'epoch': 2.04}
対照学習損失: 4.6000983275007457e-05
対照学習損失: 3.910475061275065e-05
{'loss': 0.0, 'grad_norm': 0.0011384945828467607, 'learning_rate': 1.8790613718411555e-05, 'epoch': 2.04}
対照学習損失: 9.710704762255773e-05
対照学習損失: 2.5716177333379164e-05
{'loss': 0.0001, 'grad_norm': 0.003282953519374132, 'learning_rate': 1.877256317689531e-05, 'epoch': 2.05}
対照学習損失: 1.938592322403565e-05
対照学習損失: 0.00016958128253463656
{'loss': 0.0001, 'grad_norm': 0.014024339616298676, 'learning_rate': 1.8754512635379062e-05, 'epoch': 2.05}
対照学習損失: 0.0004075916367582977
対照学習損失: 3.9296006434597075e-05
{'loss': 0.0002, 'grad_norm': 0.023593425750732422, 'learning_rate': 1.8736462093862816e-05, 'epoch': 2.05}
対照学習損失: 0.00014583388110622764
対照学習損失: 1.2360274922684766e-05
{'loss': 0.0001, 'grad_norm': 0.005002123303711414, 'learning_rate': 1.871841155234657e-05, 'epoch': 2.05}
対照学習損失: 0.00024208576360251755
対照学習損失: 0.00012364910799078643
{'loss': 0.0002, 'grad_norm': 0.01078557688742876, 'learning_rate': 1.8700361010830324e-05, 'epoch': 2.05}
対照学習損失: 1.942297603818588e-05
対照学習損失: 0.00011672079563140869
{'loss': 0.0001, 'grad_norm': 0.0038532058242708445, 'learning_rate': 1.8682310469314078e-05, 'epoch': 2.05}
対照学習損失: 0.00018164294306188822
対照学習損失: 2.349823625991121e-05
{'loss': 0.0001, 'grad_norm': 0.003978646360337734, 'learning_rate': 1.8664259927797835e-05, 'epoch': 2.05}
対照学習損失: 2.4742228561080992e-05
対照学習損失: 6.816587847424671e-05
{'loss': 0.0, 'grad_norm': 0.003030675696209073, 'learning_rate': 1.8646209386281592e-05, 'epoch': 2.05}
対照学習損失: 0.0015164001379162073
対照学習損失: 1.1905882274731994e-05
{'loss': 0.0008, 'grad_norm': 0.05059138312935829, 'learning_rate': 1.8628158844765346e-05, 'epoch': 2.05}
対照学習損失: 3.1728242902318016e-05
対照学習損失: 1.2069726835761685e-05
{'loss': 0.0, 'grad_norm': 0.0007350080413743854, 'learning_rate': 1.86101083032491e-05, 'epoch': 2.05}
対照学習損失: 2.2231201000977308e-05
対照学習損失: 9.146303636953235e-05
{'loss': 0.0001, 'grad_norm': 0.003717366373166442, 'learning_rate': 1.8592057761732854e-05, 'epoch': 2.06}
対照学習損失: 6.430166104109958e-05
対照学習損失: 0.0004726935876533389
{'loss': 0.0003, 'grad_norm': 0.021438714116811752, 'learning_rate': 1.8574007220216607e-05, 'epoch': 2.06}
対照学習損失: 1.8916514818556607e-05
対照学習損失: 1.1294910109427292e-05
{'loss': 0.0, 'grad_norm': 0.0003224836545996368, 'learning_rate': 1.855595667870036e-05, 'epoch': 2.06}
対照学習損失: 1.9095177776762284e-05
対照学習損失: 2.809489524224773e-05
{'loss': 0.0, 'grad_norm': 0.0006060973973944783, 'learning_rate': 1.8537906137184115e-05, 'epoch': 2.06}
対照学習損失: 3.752929114853032e-05
対照学習損失: 2.9755914511042647e-05
{'loss': 0.0, 'grad_norm': 0.0010501103242859244, 'learning_rate': 1.851985559566787e-05, 'epoch': 2.06}
対照学習損失: 6.111607217462733e-05
対照学習損失: 6.674034375464544e-05
{'loss': 0.0001, 'grad_norm': 0.0016607629368081689, 'learning_rate': 1.8501805054151626e-05, 'epoch': 2.06}
対照学習損失: 2.9315926440176554e-05
対照学習損失: 5.116329703014344e-05
{'loss': 0.0, 'grad_norm': 0.001428397954441607, 'learning_rate': 1.848375451263538e-05, 'epoch': 2.06}
対照学習損失: 0.00017071371257770807
対照学習損失: 7.302493031602353e-05
{'loss': 0.0001, 'grad_norm': 0.005948124919086695, 'learning_rate': 1.8465703971119134e-05, 'epoch': 2.06}
対照学習損失: 0.0002963269071187824
対照学習損失: 7.022043428150937e-05
{'loss': 0.0002, 'grad_norm': 0.0064619448967278, 'learning_rate': 1.844765342960289e-05, 'epoch': 2.06}
対照学習損失: 7.50988838262856e-05
対照学習損失: 1.0013399332819972e-05
{'loss': 0.0, 'grad_norm': 0.0011858722427859902, 'learning_rate': 1.8429602888086645e-05, 'epoch': 2.06}
対照学習損失: 1.6145006156875752e-05
対照学習損失: 2.8921345801791176e-05
{'loss': 0.0, 'grad_norm': 0.0008909330936148763, 'learning_rate': 1.84115523465704e-05, 'epoch': 2.06}
対照学習損失: 4.002833156846464e-05
対照学習損失: 1.4982553693698719e-05
{'loss': 0.0, 'grad_norm': 0.0009073395049199462, 'learning_rate': 1.8393501805054152e-05, 'epoch': 2.07}
対照学習損失: 2.6455880288267508e-05
対照学習損失: 3.879660653183237e-05
{'loss': 0.0, 'grad_norm': 0.0009365446749143302, 'learning_rate': 1.8375451263537906e-05, 'epoch': 2.07}
対照学習損失: 1.9698658434208483e-05
対照学習損失: 6.653333912254311e-06
{'loss': 0.0, 'grad_norm': 0.0003711574536282569, 'learning_rate': 1.835740072202166e-05, 'epoch': 2.07}
対照学習損失: 3.378271503606811e-05
対照学習損失: 4.8956528189592063e-05
{'loss': 0.0, 'grad_norm': 0.001234130235388875, 'learning_rate': 1.8339350180505417e-05, 'epoch': 2.07}
対照学習損失: 3.9783819374861196e-05
対照学習損失: 0.0001983008987735957
{'loss': 0.0001, 'grad_norm': 0.005656776949763298, 'learning_rate': 1.832129963898917e-05, 'epoch': 2.07}
対照学習損失: 6.481377931777388e-05
対照学習損失: 0.00017211420345120132
{'loss': 0.0001, 'grad_norm': 0.009843176230788231, 'learning_rate': 1.8303249097472925e-05, 'epoch': 2.07}
対照学習損失: 0.0001361971371807158
対照学習損失: 0.00029830256244167686
{'loss': 0.0002, 'grad_norm': 0.0055062067694962025, 'learning_rate': 1.828519855595668e-05, 'epoch': 2.07}
対照学習損失: 6.568097887793556e-05
対照学習損失: 1.735187652229797e-05
{'loss': 0.0, 'grad_norm': 0.0016299940180033445, 'learning_rate': 1.8267148014440432e-05, 'epoch': 2.07}
対照学習損失: 5.3777697758050635e-05
対照学習損失: 3.326278238091618e-05
{'loss': 0.0, 'grad_norm': 0.0019922491628676653, 'learning_rate': 1.824909747292419e-05, 'epoch': 2.07}
対照学習損失: 4.285723116481677e-05
対照学習損失: 0.00021086791821289808
{'loss': 0.0001, 'grad_norm': 0.006255012936890125, 'learning_rate': 1.8231046931407943e-05, 'epoch': 2.07}
対照学習損失: 2.678781675058417e-05
対照学習損失: 5.532614886760712e-05
{'loss': 0.0, 'grad_norm': 0.0013951635919511318, 'learning_rate': 1.8212996389891697e-05, 'epoch': 2.07}
対照学習損失: 0.00024025497259572148
対照学習損失: 8.296637679450214e-05
{'loss': 0.0002, 'grad_norm': 0.01951795071363449, 'learning_rate': 1.819494584837545e-05, 'epoch': 2.08}
対照学習損失: 4.3414336687419564e-05
対照学習損失: 2.5532397557981312e-05
{'loss': 0.0, 'grad_norm': 0.0010608447482809424, 'learning_rate': 1.8176895306859208e-05, 'epoch': 2.08}
対照学習損失: 1.0728401321102865e-05
対照学習損失: 0.00011489410826470703
{'loss': 0.0001, 'grad_norm': 0.004530702717602253, 'learning_rate': 1.8158844765342962e-05, 'epoch': 2.08}
対照学習損失: 3.0322720704134554e-05
対照学習損失: 2.81540269497782e-05
{'loss': 0.0, 'grad_norm': 0.000638202007394284, 'learning_rate': 1.8140794223826716e-05, 'epoch': 2.08}
対照学習損失: 1.1510967851791065e-05
対照学習損失: 0.00019188679289072752
{'loss': 0.0001, 'grad_norm': 0.006332380697131157, 'learning_rate': 1.812274368231047e-05, 'epoch': 2.08}
対照学習損失: 9.130022954195738e-05
対照学習損失: 2.2559119315701537e-05
{'loss': 0.0001, 'grad_norm': 0.002599719213321805, 'learning_rate': 1.8104693140794224e-05, 'epoch': 2.08}
対照学習損失: 0.0039737229235470295
対照学習損失: 1.2360238542896695e-05
{'loss': 0.002, 'grad_norm': 0.11863960325717926, 'learning_rate': 1.8086642599277977e-05, 'epoch': 2.08}
対照学習損失: 7.925213139969856e-05
対照学習損失: 4.603339039022103e-05
{'loss': 0.0001, 'grad_norm': 0.0018608281388878822, 'learning_rate': 1.806859205776173e-05, 'epoch': 2.08}
対照学習損失: 5.630906889564358e-05
対照学習損失: 9.588731700205244e-06
{'loss': 0.0, 'grad_norm': 0.0014043222181499004, 'learning_rate': 1.805054151624549e-05, 'epoch': 2.08}
対照学習損失: 3.3419561077607796e-05
対照学習損失: 3.999493128503673e-05
{'loss': 0.0, 'grad_norm': 0.001215378288179636, 'learning_rate': 1.8032490974729242e-05, 'epoch': 2.08}
対照学習損失: 7.4356312325107865e-06
対照学習損失: 3.623514930950478e-05
{'loss': 0.0, 'grad_norm': 0.0006491024978458881, 'learning_rate': 1.8014440433213e-05, 'epoch': 2.08}
対照学習損失: 0.00015552592230960727
対照学習損失: 1.3455557564157061e-05
{'loss': 0.0001, 'grad_norm': 0.0044201435521245, 'learning_rate': 1.7996389891696753e-05, 'epoch': 2.09}
対照学習損失: 9.58131204242818e-06
対照学習損失: 8.24023754830705e-06
{'loss': 0.0, 'grad_norm': 0.00020397589833009988, 'learning_rate': 1.7978339350180507e-05, 'epoch': 2.09}
対照学習損失: 4.448434629011899e-05
対照学習損失: 2.8824917535530403e-05
{'loss': 0.0, 'grad_norm': 0.0011659892043098807, 'learning_rate': 1.796028880866426e-05, 'epoch': 2.09}
対照学習損失: 0.00017567866598255932
対照学習損失: 0.00021058447600807995
{'loss': 0.0002, 'grad_norm': 0.009232156910002232, 'learning_rate': 1.7942238267148015e-05, 'epoch': 2.09}
対照学習損失: 8.973443618742749e-05
対照学習損失: 0.0005112220533192158
{'loss': 0.0003, 'grad_norm': 0.06026369705796242, 'learning_rate': 1.792418772563177e-05, 'epoch': 2.09}
対照学習損失: 4.6193435991881415e-06
対照学習損失: 2.0458594008232467e-05
{'loss': 0.0, 'grad_norm': 0.0002796703774947673, 'learning_rate': 1.7906137184115522e-05, 'epoch': 2.09}
対照学習損失: 0.00010657348320819438
対照学習損失: 1.0274208761984482e-05
{'loss': 0.0001, 'grad_norm': 0.003202793188393116, 'learning_rate': 1.7888086642599276e-05, 'epoch': 2.09}
対照学習損失: 6.0861999372718856e-05
対照学習損失: 4.101463491679169e-05
{'loss': 0.0001, 'grad_norm': 0.0018134500132873654, 'learning_rate': 1.7870036101083033e-05, 'epoch': 2.09}
対照学習損失: 0.00020479757222346961
対照学習損失: 1.0825410754478071e-05
{'loss': 0.0001, 'grad_norm': 0.004504435695707798, 'learning_rate': 1.785198555956679e-05, 'epoch': 2.09}
対照学習損失: 9.536629477224778e-06
対照学習損失: 2.6090763640240766e-05
{'loss': 0.0, 'grad_norm': 0.0005520841805264354, 'learning_rate': 1.7833935018050544e-05, 'epoch': 2.09}
対照学習損失: 0.000711187138222158
対照学習損失: 5.133751255925745e-05
{'loss': 0.0004, 'grad_norm': 0.019880790263414383, 'learning_rate': 1.7815884476534298e-05, 'epoch': 2.09}
対照学習損失: 3.206265682820231e-05
対照学習損失: 4.835724757867865e-05
{'loss': 0.0, 'grad_norm': 0.0014681183965876698, 'learning_rate': 1.7797833935018052e-05, 'epoch': 2.1}
対照学習損失: 0.0006834632367826998
対照学習損失: 3.598551484174095e-05
{'loss': 0.0004, 'grad_norm': 0.023822486400604248, 'learning_rate': 1.7779783393501806e-05, 'epoch': 2.1}
対照学習損失: 1.407388936058851e-05
対照学習損失: 9.585064981365576e-05
{'loss': 0.0001, 'grad_norm': 0.005297920200973749, 'learning_rate': 1.776173285198556e-05, 'epoch': 2.1}
対照学習損失: 2.1345096683944575e-05
対照学習損失: 5.65581358387135e-05
{'loss': 0.0, 'grad_norm': 0.001177375204861164, 'learning_rate': 1.7743682310469313e-05, 'epoch': 2.1}
対照学習損失: 7.94223160482943e-06
対照学習損失: 2.1970869056531228e-05
{'loss': 0.0, 'grad_norm': 0.00039858880336396396, 'learning_rate': 1.7725631768953067e-05, 'epoch': 2.1}
対照学習損失: 0.0002414186456007883
対照学習損失: 1.4207960703060962e-05
{'loss': 0.0001, 'grad_norm': 0.006560225505381823, 'learning_rate': 1.7707581227436824e-05, 'epoch': 2.1}
対照学習損失: 1.7292248230660334e-05
対照学習損失: 8.086510933935642e-05
{'loss': 0.0, 'grad_norm': 0.002492630621418357, 'learning_rate': 1.768953068592058e-05, 'epoch': 2.1}
対照学習損失: 0.00012086913920938969
対照学習損失: 0.00029129942413419485
{'loss': 0.0002, 'grad_norm': 0.019616754725575447, 'learning_rate': 1.7671480144404336e-05, 'epoch': 2.1}
対照学習損失: 1.364921081403736e-05
対照学習損失: 2.496434171916917e-05
{'loss': 0.0, 'grad_norm': 0.0006342524429783225, 'learning_rate': 1.765342960288809e-05, 'epoch': 2.1}
対照学習損失: 2.6588651962811127e-05
対照学習損失: 3.08640519506298e-05
{'loss': 0.0, 'grad_norm': 0.0011046378640457988, 'learning_rate': 1.7635379061371843e-05, 'epoch': 2.1}
対照学習損失: 0.00016034120926633477
対照学習損失: 5.5883239838294685e-05
{'loss': 0.0001, 'grad_norm': 0.006189352832734585, 'learning_rate': 1.7617328519855597e-05, 'epoch': 2.1}
対照学習損失: 5.6652770581422374e-05
対照学習損失: 2.2671374608762562e-05
{'loss': 0.0, 'grad_norm': 0.0012002995936200023, 'learning_rate': 1.759927797833935e-05, 'epoch': 2.11}
対照学習損失: 0.0001924933458212763
対照学習損失: 2.495876105967909e-05
{'loss': 0.0001, 'grad_norm': 0.005602037999778986, 'learning_rate': 1.7581227436823105e-05, 'epoch': 2.11}
対照学習損失: 5.708896424039267e-05
対照学習損失: 2.776592918962706e-05
{'loss': 0.0, 'grad_norm': 0.001694092177785933, 'learning_rate': 1.756317689530686e-05, 'epoch': 2.11}
対照学習損失: 4.388373781694099e-06
対照学習損失: 1.471449650125578e-05
{'loss': 0.0, 'grad_norm': 0.00023694058472756296, 'learning_rate': 1.7545126353790616e-05, 'epoch': 2.11}
対照学習損失: 7.915309106465429e-05
対照学習損失: 3.0135961424093693e-05
{'loss': 0.0001, 'grad_norm': 0.0024215951561927795, 'learning_rate': 1.752707581227437e-05, 'epoch': 2.11}
対照学習損失: 8.419070582021959e-06
対照学習損失: 1.6144867913681082e-05
{'loss': 0.0, 'grad_norm': 0.0003865437174681574, 'learning_rate': 1.7509025270758123e-05, 'epoch': 2.11}
対照学習損失: 4.8459904064657167e-05
対照学習損失: 1.782131403160747e-05
{'loss': 0.0, 'grad_norm': 0.001036986242979765, 'learning_rate': 1.7490974729241877e-05, 'epoch': 2.11}
対照学習損失: 6.043002213118598e-05
対照学習損失: 2.3192800654214807e-05
{'loss': 0.0, 'grad_norm': 0.0021680935751646757, 'learning_rate': 1.7472924187725634e-05, 'epoch': 2.11}
対照学習損失: 9.921469609253109e-05
対照学習損失: 0.0012185011291876435
{'loss': 0.0007, 'grad_norm': 0.03985505923628807, 'learning_rate': 1.7454873646209388e-05, 'epoch': 2.11}
対照学習損失: 1.0333824320696294e-05
対照学習損失: 0.00017255703278351575
{'loss': 0.0001, 'grad_norm': 0.004314710386097431, 'learning_rate': 1.7436823104693142e-05, 'epoch': 2.11}
対照学習損失: 0.0015253255842253566
対照学習損失: 7.875164556025993e-06
{'loss': 0.0008, 'grad_norm': 0.04330118000507355, 'learning_rate': 1.7418772563176896e-05, 'epoch': 2.11}
対照学習損失: 2.004766975005623e-05
対照学習損失: 1.405861257808283e-05
{'loss': 0.0, 'grad_norm': 0.001492293900810182, 'learning_rate': 1.740072202166065e-05, 'epoch': 2.12}
対照学習損失: 8.722502388991416e-05
対照学習損失: 2.736328860919457e-05
{'loss': 0.0001, 'grad_norm': 0.005171280354261398, 'learning_rate': 1.7382671480144407e-05, 'epoch': 2.12}
対照学習損失: 4.080947837792337e-05
対照学習損失: 3.4091157431248575e-05
{'loss': 0.0, 'grad_norm': 0.0011056724470108747, 'learning_rate': 1.736462093862816e-05, 'epoch': 2.12}
対照学習損失: 4.059947968926281e-05
対照学習損失: 1.3850305549567565e-05
{'loss': 0.0, 'grad_norm': 0.0007877808529883623, 'learning_rate': 1.7346570397111914e-05, 'epoch': 2.12}
対照学習損失: 1.5839425032027066e-05
対照学習損失: 7.271508366102353e-05
{'loss': 0.0, 'grad_norm': 0.0015827101888135076, 'learning_rate': 1.7328519855595668e-05, 'epoch': 2.12}
対照学習損失: 4.80522467114497e-05
対照学習損失: 1.270303619094193e-05
{'loss': 0.0, 'grad_norm': 0.0009960262104868889, 'learning_rate': 1.7310469314079422e-05, 'epoch': 2.12}
対照学習損失: 2.2403246475732885e-05
対照学習損失: 0.0001591533946339041
{'loss': 0.0001, 'grad_norm': 0.019643621519207954, 'learning_rate': 1.7292418772563176e-05, 'epoch': 2.12}
対照学習損失: 5.096155291539617e-06
対照学習損失: 1.9609198716352694e-05
{'loss': 0.0, 'grad_norm': 0.00038034236058592796, 'learning_rate': 1.7274368231046933e-05, 'epoch': 2.12}
対照学習損失: 3.105005816905759e-05
対照学習損失: 1.7173138985526748e-05
{'loss': 0.0, 'grad_norm': 0.0014655457343906164, 'learning_rate': 1.7256317689530687e-05, 'epoch': 2.12}
対照学習損失: 2.336372017452959e-05
対照学習損失: 1.3857776139047928e-05
{'loss': 0.0, 'grad_norm': 0.0006983442581258714, 'learning_rate': 1.723826714801444e-05, 'epoch': 2.12}
対照学習損失: 2.8415310225682333e-05
対照学習損失: 2.741653224802576e-05
{'loss': 0.0, 'grad_norm': 0.0007113972678780556, 'learning_rate': 1.7220216606498198e-05, 'epoch': 2.12}
対照学習損失: 3.392826329218224e-05
対照学習損失: 3.717482104548253e-05
{'loss': 0.0, 'grad_norm': 0.001521412399597466, 'learning_rate': 1.7202166064981952e-05, 'epoch': 2.13}
対照学習損失: 0.0005404914845712483
対照学習損失: 8.384011744055897e-05
{'loss': 0.0003, 'grad_norm': 0.009559028781950474, 'learning_rate': 1.7184115523465706e-05, 'epoch': 2.13}
対照学習損失: 4.924796940031229e-06
対照学習損失: 3.927641955669969e-05
{'loss': 0.0, 'grad_norm': 0.0006611925200559199, 'learning_rate': 1.716606498194946e-05, 'epoch': 2.13}
対照学習損失: 5.655380300595425e-05
対照学習損失: 7.45359793654643e-05
{'loss': 0.0001, 'grad_norm': 0.004703528713434935, 'learning_rate': 1.7148014440433213e-05, 'epoch': 2.13}
対照学習損失: 5.1378294301684946e-05
対照学習損失: 0.0001513036695541814
{'loss': 0.0001, 'grad_norm': 0.0031725172884762287, 'learning_rate': 1.7129963898916967e-05, 'epoch': 2.13}
対照学習損失: 9.463603055337444e-05
対照学習損失: 4.413884744280949e-05
{'loss': 0.0001, 'grad_norm': 0.0026010307483375072, 'learning_rate': 1.711191335740072e-05, 'epoch': 2.13}
対照学習損失: 7.606968210893683e-06
対照学習損失: 3.17969752359204e-05
{'loss': 0.0, 'grad_norm': 0.0007423058850690722, 'learning_rate': 1.7093862815884475e-05, 'epoch': 2.13}
対照学習損失: 7.666569217690267e-06
対照学習損失: 1.716551196295768e-05
{'loss': 0.0, 'grad_norm': 0.00036352916504256427, 'learning_rate': 1.7075812274368232e-05, 'epoch': 2.13}
対照学習損失: 6.155761366244406e-05
対照学習損失: 0.0003274882910773158
{'loss': 0.0002, 'grad_norm': 0.010808337479829788, 'learning_rate': 1.7057761732851986e-05, 'epoch': 2.13}
対照学習損失: 4.184248246019706e-05
対照学習損失: 7.80811024014838e-06
{'loss': 0.0, 'grad_norm': 0.0009262633393518627, 'learning_rate': 1.7039711191335743e-05, 'epoch': 2.13}
対照学習損失: 5.231617979006842e-05
対照学習損失: 0.0001951046724570915
{'loss': 0.0001, 'grad_norm': 0.004546081647276878, 'learning_rate': 1.7021660649819497e-05, 'epoch': 2.13}
対照学習損失: 8.560446985939052e-06
対照学習損失: 8.90327191882534e-06
{'loss': 0.0, 'grad_norm': 0.00022768403869122267, 'learning_rate': 1.700361010830325e-05, 'epoch': 2.14}
対照学習損失: 0.00011090020416304469
対照学習損失: 7.995266059879214e-05
{'loss': 0.0001, 'grad_norm': 0.004625021480023861, 'learning_rate': 1.6985559566787004e-05, 'epoch': 2.14}
対照学習損失: 0.0003768008027691394
対照学習損失: 1.2337912266957574e-05
{'loss': 0.0002, 'grad_norm': 0.009035115130245686, 'learning_rate': 1.6967509025270758e-05, 'epoch': 2.14}
対照学習損失: 2.309590308868792e-05
対照学習損失: 9.663704986451194e-05
{'loss': 0.0001, 'grad_norm': 0.007676136679947376, 'learning_rate': 1.6949458483754512e-05, 'epoch': 2.14}
対照学習損失: 2.323703301954083e-05
対照学習損失: 1.0318926797481254e-05
{'loss': 0.0, 'grad_norm': 0.00045613813563250005, 'learning_rate': 1.6931407942238266e-05, 'epoch': 2.14}
対照学習損失: 9.268383109883871e-06
対照学習損失: 0.00011939211981371045
{'loss': 0.0001, 'grad_norm': 0.004082366358488798, 'learning_rate': 1.6913357400722023e-05, 'epoch': 2.14}
対照学習損失: 8.153796079568565e-05
対照学習損失: 9.089577361010015e-06
{'loss': 0.0, 'grad_norm': 0.0010751689551398158, 'learning_rate': 1.6895306859205777e-05, 'epoch': 2.14}
対照学習損失: 1.473698102927301e-05
対照学習損失: 1.803624036256224e-05
{'loss': 0.0, 'grad_norm': 0.00039599122828803957, 'learning_rate': 1.6877256317689534e-05, 'epoch': 2.14}
対照学習損失: 4.612838529283181e-05
対照学習損失: 4.4321473978925496e-05
{'loss': 0.0, 'grad_norm': 0.0015061977319419384, 'learning_rate': 1.6859205776173288e-05, 'epoch': 2.14}
対照学習損失: 2.7072394004790112e-05
対照学習損失: 3.358515095897019e-05
{'loss': 0.0, 'grad_norm': 0.002042676554992795, 'learning_rate': 1.684115523465704e-05, 'epoch': 2.14}
対照学習損失: 2.6992111088475212e-05
対照学習損失: 6.168076652102172e-05
{'loss': 0.0, 'grad_norm': 0.0015209336997941136, 'learning_rate': 1.6823104693140795e-05, 'epoch': 2.14}
対照学習損失: 8.39127751532942e-05
対照学習損失: 0.00019097633776254952
{'loss': 0.0001, 'grad_norm': 0.0054774838499724865, 'learning_rate': 1.680505415162455e-05, 'epoch': 2.15}
対照学習損失: 3.482897227513604e-05
対照学習損失: 0.0001198834361275658
{'loss': 0.0001, 'grad_norm': 0.0037666568532586098, 'learning_rate': 1.6787003610108303e-05, 'epoch': 2.15}
対照学習損失: 0.0005417790380306542
対照学習損失: 1.5332847397075966e-05
{'loss': 0.0003, 'grad_norm': 0.015425179153680801, 'learning_rate': 1.6768953068592057e-05, 'epoch': 2.15}
対照学習損失: 0.0003034871187992394
対照学習損失: 3.698152431752533e-05
{'loss': 0.0002, 'grad_norm': 0.009475874714553356, 'learning_rate': 1.6750902527075814e-05, 'epoch': 2.15}
対照学習損失: 0.00047595726209692657
対照学習損失: 2.1888559786020778e-05
{'loss': 0.0002, 'grad_norm': 0.012531387619674206, 'learning_rate': 1.6732851985559568e-05, 'epoch': 2.15}
対照学習損失: 0.00013668571773450822
対照学習損失: 5.3278552513802424e-05
{'loss': 0.0001, 'grad_norm': 0.010309652425348759, 'learning_rate': 1.6714801444043322e-05, 'epoch': 2.15}
対照学習損失: 2.212753133790102e-05
対照学習損失: 2.4228145775850862e-05
{'loss': 0.0, 'grad_norm': 0.0008260220638476312, 'learning_rate': 1.669675090252708e-05, 'epoch': 2.15}
対照学習損失: 1.1130948223581072e-05
対照学習損失: 1.4595412721973844e-05
{'loss': 0.0, 'grad_norm': 0.0002650769311003387, 'learning_rate': 1.6678700361010833e-05, 'epoch': 2.15}
対照学習損失: 3.112540071015246e-05
対照学習損失: 0.00023435382172465324
{'loss': 0.0001, 'grad_norm': 0.005702502094209194, 'learning_rate': 1.6660649819494587e-05, 'epoch': 2.15}
対照学習損失: 4.2769119318109006e-05
対照学習損失: 2.5187278879457153e-05
{'loss': 0.0, 'grad_norm': 0.000985794235020876, 'learning_rate': 1.664259927797834e-05, 'epoch': 2.15}
対照学習損失: 8.184666512534022e-05
対照学習損失: 3.1819698051549494e-05
{'loss': 0.0001, 'grad_norm': 0.0028587346896529198, 'learning_rate': 1.6624548736462094e-05, 'epoch': 2.15}
対照学習損失: 5.023882840760052e-05
対照学習損失: 1.7976894014282152e-05
{'loss': 0.0, 'grad_norm': 0.0010266995523124933, 'learning_rate': 1.6606498194945848e-05, 'epoch': 2.16}
対照学習損失: 1.6748195776017383e-05
対照学習損失: 9.77505260379985e-06
{'loss': 0.0, 'grad_norm': 0.0002970894565805793, 'learning_rate': 1.6588447653429605e-05, 'epoch': 2.16}
対照学習損失: 4.55667068308685e-05
対照学習損失: 4.814265776076354e-05
{'loss': 0.0, 'grad_norm': 0.0016631168546155095, 'learning_rate': 1.657039711191336e-05, 'epoch': 2.16}
対照学習損失: 4.068001089763129e-06
対照学習損失: 2.4241186110884883e-05
{'loss': 0.0, 'grad_norm': 0.0005648003425449133, 'learning_rate': 1.6552346570397113e-05, 'epoch': 2.16}
対照学習損失: 5.7944049331126735e-05
対照学習損失: 3.2757248845882714e-05
{'loss': 0.0, 'grad_norm': 0.0016091817524284124, 'learning_rate': 1.6534296028880867e-05, 'epoch': 2.16}
対照学習損失: 9.36073120101355e-05
対照学習損失: 0.0002274400758324191
{'loss': 0.0002, 'grad_norm': 0.00862551387399435, 'learning_rate': 1.651624548736462e-05, 'epoch': 2.16}
対照学習損失: 1.25092647067504e-05
対照学習損失: 0.00012904992036055773
{'loss': 0.0001, 'grad_norm': 0.0032049461733549833, 'learning_rate': 1.6498194945848378e-05, 'epoch': 2.16}
対照学習損失: 2.6842555598705076e-05
対照学習損失: 2.7021706046070904e-05
{'loss': 0.0, 'grad_norm': 0.0006401983555406332, 'learning_rate': 1.648014440433213e-05, 'epoch': 2.16}
対照学習損失: 2.0264445993234403e-05
対照学習損失: 1.3276721801958047e-05
{'loss': 0.0, 'grad_norm': 0.0005264496430754662, 'learning_rate': 1.6462093862815885e-05, 'epoch': 2.16}
対照学習損失: 0.0005756148020736873
対照学習損失: 9.961286195903085e-06
{'loss': 0.0003, 'grad_norm': 0.047037720680236816, 'learning_rate': 1.644404332129964e-05, 'epoch': 2.16}
対照学習損失: 3.745148569578305e-05
対照学習損失: 3.784594446187839e-05
{'loss': 0.0, 'grad_norm': 0.0009324607090093195, 'learning_rate': 1.6425992779783393e-05, 'epoch': 2.17}
対照学習損失: 1.3775705156149343e-05
対照学習損失: 3.470544106676243e-05
{'loss': 0.0, 'grad_norm': 0.00114208844024688, 'learning_rate': 1.640794223826715e-05, 'epoch': 2.17}
対照学習損失: 1.3224513168097474e-05
対照学習損失: 0.0005171562661416829
{'loss': 0.0003, 'grad_norm': 0.012138953432440758, 'learning_rate': 1.6389891696750904e-05, 'epoch': 2.17}
対照学習損失: 6.4894070419541094e-06
対照学習損失: 0.00023398299526888877
{'loss': 0.0001, 'grad_norm': 0.007077143527567387, 'learning_rate': 1.6371841155234658e-05, 'epoch': 2.17}
対照学習損失: 8.001838068594225e-06
対照学習損失: 1.9497550965752453e-05
{'loss': 0.0, 'grad_norm': 0.00036460463888943195, 'learning_rate': 1.635379061371841e-05, 'epoch': 2.17}
対照学習損失: 1.4639749679190572e-05
対照学習損失: 3.0261864594649523e-05
{'loss': 0.0, 'grad_norm': 0.0006821318529546261, 'learning_rate': 1.6335740072202165e-05, 'epoch': 2.17}
対照学習損失: 1.3045756531937514e-05
対照学習損失: 7.125597767299041e-05
{'loss': 0.0, 'grad_norm': 0.002212325343862176, 'learning_rate': 1.631768953068592e-05, 'epoch': 2.17}
対照学習損失: 9.342936209577601e-06
対照学習損失: 6.139231118140742e-06
{'loss': 0.0, 'grad_norm': 0.00015924031322356313, 'learning_rate': 1.6299638989169676e-05, 'epoch': 2.17}
対照学習損失: 0.0001595993817318231
対照学習損失: 1.3179736015445087e-05
{'loss': 0.0001, 'grad_norm': 0.008512742817401886, 'learning_rate': 1.628158844765343e-05, 'epoch': 2.17}
対照学習損失: 6.83756879880093e-05
対照学習損失: 1.6003479686332867e-05
{'loss': 0.0, 'grad_norm': 0.0014545796439051628, 'learning_rate': 1.6263537906137184e-05, 'epoch': 2.17}
対照学習損失: 1.7701677279546857e-05
対照学習損失: 2.26636984734796e-05
{'loss': 0.0, 'grad_norm': 0.0006704008555971086, 'learning_rate': 1.624548736462094e-05, 'epoch': 2.17}
対照学習損失: 7.391421240754426e-05
対照学習損失: 0.00010468550317455083
{'loss': 0.0001, 'grad_norm': 0.004601429682224989, 'learning_rate': 1.6227436823104695e-05, 'epoch': 2.18}
対照学習損失: 2.4980226953630336e-05
対照学習損失: 0.0006017109844833612
{'loss': 0.0003, 'grad_norm': 0.013665641658008099, 'learning_rate': 1.620938628158845e-05, 'epoch': 2.18}
対照学習損失: 2.275337465107441e-05
対照学習損失: 3.340604962431826e-05
{'loss': 0.0, 'grad_norm': 0.0006617846665903926, 'learning_rate': 1.6191335740072203e-05, 'epoch': 2.18}
対照学習損失: 9.529199815005995e-06
対照学習損失: 2.81168358924333e-05
{'loss': 0.0, 'grad_norm': 0.0007232128991745412, 'learning_rate': 1.6173285198555957e-05, 'epoch': 2.18}
対照学習損失: 9.588769898982719e-06
対照学習損失: 3.423138696234673e-05
{'loss': 0.0, 'grad_norm': 0.0006430068169720471, 'learning_rate': 1.615523465703971e-05, 'epoch': 2.18}
対照学習損失: 1.1585480024223216e-05
対照学習損失: 9.496141865383834e-05
{'loss': 0.0001, 'grad_norm': 0.006982211954891682, 'learning_rate': 1.6137184115523464e-05, 'epoch': 2.18}
対照学習損失: 1.4625114090449642e-05
対照学習損失: 3.191375799360685e-05
{'loss': 0.0, 'grad_norm': 0.0009516924619674683, 'learning_rate': 1.611913357400722e-05, 'epoch': 2.18}
対照学習損失: 4.308765346650034e-05
対照学習損失: 3.2624913728795946e-05
{'loss': 0.0, 'grad_norm': 0.0013957301853224635, 'learning_rate': 1.6101083032490975e-05, 'epoch': 2.18}
対照学習損失: 4.018521212856285e-05
対照学習損失: 1.712841185508296e-05
{'loss': 0.0, 'grad_norm': 0.001174724893644452, 'learning_rate': 1.6083032490974732e-05, 'epoch': 2.18}
対照学習損失: 2.5658557206043042e-05
対照学習損失: 8.302478090627119e-05
{'loss': 0.0001, 'grad_norm': 0.006933591328561306, 'learning_rate': 1.6064981949458486e-05, 'epoch': 2.18}
対照学習損失: 0.00022100057685747743
対照学習損失: 4.301516310079023e-05
{'loss': 0.0001, 'grad_norm': 0.005093113984912634, 'learning_rate': 1.604693140794224e-05, 'epoch': 2.18}
対照学習損失: 1.1011718925146852e-05
対照学習損失: 4.858837928622961e-05
{'loss': 0.0, 'grad_norm': 0.003919339273124933, 'learning_rate': 1.6028880866425994e-05, 'epoch': 2.19}
対照学習損失: 5.787298869108781e-05
対照学習損失: 9.93878347799182e-06
{'loss': 0.0, 'grad_norm': 0.0023881231900304556, 'learning_rate': 1.6010830324909748e-05, 'epoch': 2.19}
対照学習損失: 2.8921185730723664e-05
対照学習損失: 3.303334233351052e-05
{'loss': 0.0, 'grad_norm': 0.001272727968171239, 'learning_rate': 1.59927797833935e-05, 'epoch': 2.19}
対照学習損失: 2.1180343537707813e-05
対照学習損失: 2.7282654627924785e-05
{'loss': 0.0, 'grad_norm': 0.0007746491464786232, 'learning_rate': 1.5974729241877255e-05, 'epoch': 2.19}
対照学習損失: 2.8935181035194546e-05
対照学習損失: 5.5059249461919535e-06
{'loss': 0.0, 'grad_norm': 0.0007751298835501075, 'learning_rate': 1.595667870036101e-05, 'epoch': 2.19}
対照学習損失: 0.00019910061382688582
対照学習損失: 1.013267865346279e-05
{'loss': 0.0001, 'grad_norm': 0.0056768120266497135, 'learning_rate': 1.5938628158844766e-05, 'epoch': 2.19}
対照学習損失: 9.251580922864377e-05
対照学習損失: 0.0001523395476397127
{'loss': 0.0001, 'grad_norm': 0.004835052415728569, 'learning_rate': 1.592057761732852e-05, 'epoch': 2.19}
対照学習損失: 7.234473741846159e-06
対照学習損失: 5.416511157818604e-06
{'loss': 0.0, 'grad_norm': 0.00011267952504567802, 'learning_rate': 1.5902527075812277e-05, 'epoch': 2.19}
対照学習損失: 5.633958426187746e-05
対照学習損失: 0.0006396820535883307
{'loss': 0.0003, 'grad_norm': 0.020004158839583397, 'learning_rate': 1.588447653429603e-05, 'epoch': 2.19}
対照学習損失: 1.7024120097630657e-05
対照学習損失: 0.0002440719836158678
{'loss': 0.0001, 'grad_norm': 0.01533274631947279, 'learning_rate': 1.5866425992779785e-05, 'epoch': 2.19}
対照学習損失: 7.867697604524437e-06
対照学習損失: 4.3665815610438585e-05
{'loss': 0.0, 'grad_norm': 0.000890759751200676, 'learning_rate': 1.584837545126354e-05, 'epoch': 2.19}
対照学習損失: 0.00010029774421127513
対照学習損失: 0.0002520785201340914
{'loss': 0.0002, 'grad_norm': 0.01482090912759304, 'learning_rate': 1.5830324909747293e-05, 'epoch': 2.2}
対照学習損失: 3.4844237234210595e-05
対照学習損失: 0.0008371221483685076
{'loss': 0.0004, 'grad_norm': 0.03238436207175255, 'learning_rate': 1.5812274368231046e-05, 'epoch': 2.2}
対照学習損失: 0.0011029585730284452
対照学習損失: 3.956076398026198e-05
{'loss': 0.0006, 'grad_norm': 0.03261025995016098, 'learning_rate': 1.57942238267148e-05, 'epoch': 2.2}
対照学習損失: 1.955676452780608e-05
対照学習損失: 0.00011735226144082844
{'loss': 0.0001, 'grad_norm': 0.0037114236038178205, 'learning_rate': 1.5776173285198557e-05, 'epoch': 2.2}
対照学習損失: 0.0005130972131155431
対照学習損失: 1.5518779036938213e-05
{'loss': 0.0003, 'grad_norm': 0.01661236770451069, 'learning_rate': 1.575812274368231e-05, 'epoch': 2.2}
対照学習損失: 3.461910455371253e-05
対照学習損失: 0.00010553450556471944
{'loss': 0.0001, 'grad_norm': 0.0027999598532915115, 'learning_rate': 1.5740072202166065e-05, 'epoch': 2.2}
対照学習損失: 1.0654152902134228e-05
対照学習損失: 1.3746084732702002e-05
{'loss': 0.0, 'grad_norm': 0.00036596148856915534, 'learning_rate': 1.5722021660649822e-05, 'epoch': 2.2}
対照学習損失: 0.00014278816524893045
対照学習損失: 0.0019434092100709677
{'loss': 0.001, 'grad_norm': 0.09972648322582245, 'learning_rate': 1.5703971119133576e-05, 'epoch': 2.2}
対照学習損失: 4.935389733873308e-05
対照学習損失: 0.0005437995423562825
{'loss': 0.0003, 'grad_norm': 0.013657798059284687, 'learning_rate': 1.568592057761733e-05, 'epoch': 2.2}
対照学習損失: 5.525080632651225e-05
対照学習損失: 7.264434680109844e-05
{'loss': 0.0001, 'grad_norm': 0.0041437610052526, 'learning_rate': 1.5667870036101084e-05, 'epoch': 2.2}
対照学習損失: 0.0004330112424213439
対照学習損失: 4.741662269225344e-05
{'loss': 0.0002, 'grad_norm': 0.00878149550408125, 'learning_rate': 1.5649819494584838e-05, 'epoch': 2.2}
対照学習損失: 3.754982753889635e-05
対照学習損失: 1.9585826521506533e-05
{'loss': 0.0, 'grad_norm': 0.0015353858470916748, 'learning_rate': 1.563176895306859e-05, 'epoch': 2.21}
対照学習損失: 7.577164797112346e-05
対照学習損失: 6.881565786898136e-05
{'loss': 0.0001, 'grad_norm': 0.006668057758361101, 'learning_rate': 1.561371841155235e-05, 'epoch': 2.21}
対照学習損失: 0.00017067034787032753
対照学習損失: 1.2844560842495412e-05
{'loss': 0.0001, 'grad_norm': 0.006191891152411699, 'learning_rate': 1.5595667870036102e-05, 'epoch': 2.21}
対照学習損失: 8.542760042473674e-05
対照学習損失: 0.00014913831546436995
{'loss': 0.0001, 'grad_norm': 0.004652086645364761, 'learning_rate': 1.5577617328519856e-05, 'epoch': 2.21}
対照学習損失: 0.0001072493614628911
対照学習損失: 7.710589852649719e-05
{'loss': 0.0001, 'grad_norm': 0.0041886307299137115, 'learning_rate': 1.555956678700361e-05, 'epoch': 2.21}
対照学習損失: 5.990227691654582e-06
対照学習損失: 9.82720302999951e-06
{'loss': 0.0, 'grad_norm': 0.0001555286580696702, 'learning_rate': 1.5541516245487364e-05, 'epoch': 2.21}
対照学習損失: 6.374698568833992e-05
対照学習損失: 3.380068301339634e-05
{'loss': 0.0, 'grad_norm': 0.0019061046186834574, 'learning_rate': 1.552346570397112e-05, 'epoch': 2.21}
対照学習損失: 0.0003987461677752435
対照学習損失: 2.182020034524612e-05
{'loss': 0.0002, 'grad_norm': 0.013635360635817051, 'learning_rate': 1.5505415162454875e-05, 'epoch': 2.21}
対照学習損失: 3.5447035770630464e-05
対照学習損失: 1.0117404599441215e-05
{'loss': 0.0, 'grad_norm': 0.0006126993102952838, 'learning_rate': 1.548736462093863e-05, 'epoch': 2.21}
対照学習損失: 5.454104029922746e-05
対照学習損失: 1.6435673387604766e-05
{'loss': 0.0, 'grad_norm': 0.0016439371975138783, 'learning_rate': 1.5469314079422383e-05, 'epoch': 2.21}
対照学習損失: 2.1307931092451327e-05
対照学習損失: 4.78769070468843e-05
{'loss': 0.0, 'grad_norm': 0.0010921115754172206, 'learning_rate': 1.545126353790614e-05, 'epoch': 2.21}
対照学習損失: 2.38337270275224e-05
対照学習損失: 7.883202488301322e-05
{'loss': 0.0001, 'grad_norm': 0.0039686583913862705, 'learning_rate': 1.5433212996389894e-05, 'epoch': 2.22}
対照学習損失: 0.00020444430992938578
対照学習損失: 3.7742425774922594e-05
{'loss': 0.0001, 'grad_norm': 0.004410113207995892, 'learning_rate': 1.5415162454873647e-05, 'epoch': 2.22}
対照学習損失: 6.311843753792346e-05
対照学習損失: 2.1300558728398755e-05
{'loss': 0.0, 'grad_norm': 0.0013649552129209042, 'learning_rate': 1.53971119133574e-05, 'epoch': 2.22}
対照学習損失: 2.0302126358728856e-05
対照学習損失: 8.679838174430188e-06
{'loss': 0.0, 'grad_norm': 0.00032615341478958726, 'learning_rate': 1.5379061371841155e-05, 'epoch': 2.22}
対照学習損失: 5.255338328424841e-05
対照学習損失: 1.2576265362440608e-05
{'loss': 0.0, 'grad_norm': 0.0013356489362195134, 'learning_rate': 1.536101083032491e-05, 'epoch': 2.22}
対照学習損失: 0.0018066762713715434
対照学習損失: 3.3298849302809685e-05
{'loss': 0.0009, 'grad_norm': 0.1171952486038208, 'learning_rate': 1.5342960288808663e-05, 'epoch': 2.22}
対照学習損失: 1.4431214367505163e-05
対照学習損失: 0.0001229886111104861
{'loss': 0.0001, 'grad_norm': 0.0037434997502714396, 'learning_rate': 1.532490974729242e-05, 'epoch': 2.22}
対照学習損失: 6.456543633248657e-05
対照学習損失: 2.8754255254170857e-05
{'loss': 0.0, 'grad_norm': 0.0020285409409552813, 'learning_rate': 1.5306859205776174e-05, 'epoch': 2.22}
対照学習損失: 3.355060835019685e-05
対照学習損失: 2.7371865144232288e-05
{'loss': 0.0, 'grad_norm': 0.0010892354184761643, 'learning_rate': 1.528880866425993e-05, 'epoch': 2.22}
対照学習損失: 1.8364176867180504e-05
対照学習損失: 1.8766730136121623e-05
{'loss': 0.0, 'grad_norm': 0.0005855353083461523, 'learning_rate': 1.5270758122743685e-05, 'epoch': 2.22}
対照学習損失: 6.01256761001423e-06
対照学習損失: 3.1410116207553074e-05
{'loss': 0.0, 'grad_norm': 0.0005375590408220887, 'learning_rate': 1.5252707581227438e-05, 'epoch': 2.22}
対照学習損失: 8.023793634492904e-05
対照学習損失: 0.0002838479995261878
{'loss': 0.0002, 'grad_norm': 0.008410118520259857, 'learning_rate': 1.5234657039711192e-05, 'epoch': 2.23}
対照学習損失: 3.764274515560828e-05
対照学習損失: 3.226598710170947e-05
{'loss': 0.0, 'grad_norm': 0.0014222913887351751, 'learning_rate': 1.5216606498194946e-05, 'epoch': 2.23}
対照学習損失: 0.00016804836923256516
対照学習損失: 9.186356692225672e-06
{'loss': 0.0001, 'grad_norm': 0.0030621872283518314, 'learning_rate': 1.51985559566787e-05, 'epoch': 2.23}
対照学習損失: 7.964599717524834e-06
対照学習損失: 1.6994134057313204e-05
{'loss': 0.0, 'grad_norm': 0.00032822179491631687, 'learning_rate': 1.5180505415162455e-05, 'epoch': 2.23}
対照学習損失: 5.423504262580536e-05
対照学習損失: 7.420720976369921e-06
{'loss': 0.0, 'grad_norm': 0.000783406023401767, 'learning_rate': 1.516245487364621e-05, 'epoch': 2.23}
対照学習損失: 1.382035952701699e-05
対照学習損失: 6.79892473272048e-05
{'loss': 0.0, 'grad_norm': 0.001569936634041369, 'learning_rate': 1.5144404332129963e-05, 'epoch': 2.23}
対照学習損失: 1.2546459402074106e-05
対照学習損失: 1.9072427676292136e-05
{'loss': 0.0, 'grad_norm': 0.0005432519246824086, 'learning_rate': 1.512635379061372e-05, 'epoch': 2.23}
対照学習損失: 5.9084632084704936e-05
対照学習損失: 1.8625714801601134e-05
{'loss': 0.0, 'grad_norm': 0.001307929982431233, 'learning_rate': 1.5108303249097474e-05, 'epoch': 2.23}
対照学習損失: 4.44256656919606e-05
対照学習損失: 1.115339546231553e-05
{'loss': 0.0, 'grad_norm': 0.0012853164225816727, 'learning_rate': 1.509025270758123e-05, 'epoch': 2.23}
対照学習損失: 4.6024564653635025e-05
対照学習損失: 9.397569374414161e-05
{'loss': 0.0001, 'grad_norm': 0.0031193990726023912, 'learning_rate': 1.5072202166064983e-05, 'epoch': 2.23}
対照学習損失: 5.788599810330197e-05
対照学習損失: 2.6751904442789964e-05
{'loss': 0.0, 'grad_norm': 0.002174661960452795, 'learning_rate': 1.5054151624548737e-05, 'epoch': 2.23}
対照学習損失: 3.9447117160307243e-05
対照学習損失: 4.157392686465755e-05
{'loss': 0.0, 'grad_norm': 0.0012600980699062347, 'learning_rate': 1.5036101083032491e-05, 'epoch': 2.24}
対照学習損失: 2.853460500773508e-05
対照学習損失: 1.0169722372666001e-05
{'loss': 0.0, 'grad_norm': 0.0004671711940318346, 'learning_rate': 1.5018050541516247e-05, 'epoch': 2.24}
対照学習損失: 0.00010287592158420011
対照学習損失: 5.3196736189420335e-06
{'loss': 0.0001, 'grad_norm': 0.002598121529445052, 'learning_rate': 1.5e-05, 'epoch': 2.24}
対照学習損失: 1.0542446034378372e-05
対照学習損失: 0.00010187110456172377
{'loss': 0.0001, 'grad_norm': 0.003450728254392743, 'learning_rate': 1.4981949458483754e-05, 'epoch': 2.24}
対照学習損失: 1.659952431509737e-05
対照学習損失: 8.840578084345907e-05
{'loss': 0.0001, 'grad_norm': 0.0035738288424909115, 'learning_rate': 1.4963898916967508e-05, 'epoch': 2.24}
対照学習損失: 9.499417501501739e-06
対照学習損失: 2.6076963877130765e-06
{'loss': 0.0, 'grad_norm': 0.00010422759805805981, 'learning_rate': 1.4945848375451264e-05, 'epoch': 2.24}
対照学習損失: 0.04269637539982796
対照学習損失: 0.00015108109801076353
{'loss': 0.0214, 'grad_norm': 0.8251321911811829, 'learning_rate': 1.492779783393502e-05, 'epoch': 2.24}
対照学習損失: 0.00013095272879581898
対照学習損失: 0.021289728581905365
{'loss': 0.0107, 'grad_norm': 0.27591243386268616, 'learning_rate': 1.4909747292418775e-05, 'epoch': 2.24}
対照学習損失: 6.701633537886664e-05
対照学習損失: 0.00043153221486136317
{'loss': 0.0002, 'grad_norm': 0.010413771495223045, 'learning_rate': 1.4891696750902528e-05, 'epoch': 2.24}
対照学習損失: 0.00011535603698575869
対照学習損失: 5.438892458187183e-06
{'loss': 0.0001, 'grad_norm': 0.004470680374652147, 'learning_rate': 1.4873646209386282e-05, 'epoch': 2.24}
対照学習損失: 1.327671816397924e-05
対照学習損失: 2.5598565116524696e-05
{'loss': 0.0, 'grad_norm': 0.0005360382492654026, 'learning_rate': 1.4855595667870038e-05, 'epoch': 2.24}
対照学習損失: 1.9064258594880812e-05
対照学習損失: 0.0004583452537190169
{'loss': 0.0002, 'grad_norm': 0.012438824400305748, 'learning_rate': 1.4837545126353792e-05, 'epoch': 2.25}
対照学習損失: 0.0002738823532126844
対照学習損失: 2.3020869775791653e-05
{'loss': 0.0001, 'grad_norm': 0.00488899415358901, 'learning_rate': 1.4819494584837545e-05, 'epoch': 2.25}
対照学習損失: 0.00039293727604672313
対照学習損失: 5.774159035354387e-06
{'loss': 0.0002, 'grad_norm': 0.013441374525427818, 'learning_rate': 1.4801444043321299e-05, 'epoch': 2.25}
対照学習損失: 4.184652061667293e-05
対照学習損失: 5.9690421039704233e-05
{'loss': 0.0001, 'grad_norm': 0.0017091602785512805, 'learning_rate': 1.4783393501805055e-05, 'epoch': 2.25}
対照学習損失: 1.8111797544406727e-05
対照学習損失: 2.599358049337752e-05
{'loss': 0.0, 'grad_norm': 0.0005178328137844801, 'learning_rate': 1.4765342960288808e-05, 'epoch': 2.25}
対照学習損失: 3.991463017882779e-05
対照学習損失: 8.009263183339499e-06
{'loss': 0.0, 'grad_norm': 0.0012175807496532798, 'learning_rate': 1.4747292418772562e-05, 'epoch': 2.25}
対照学習損失: 8.679868187755346e-06
対照学習損失: 1.9474688087939285e-05
{'loss': 0.0, 'grad_norm': 0.0004969849833287299, 'learning_rate': 1.472924187725632e-05, 'epoch': 2.25}
対照学習損失: 1.9989080101368017e-05
対照学習損失: 8.38183859741548e-06
{'loss': 0.0, 'grad_norm': 0.00044491683365777135, 'learning_rate': 1.4711191335740073e-05, 'epoch': 2.25}
対照学習損失: 2.899545870604925e-05
対照学習損失: 4.386848013382405e-05
{'loss': 0.0, 'grad_norm': 0.002129749860614538, 'learning_rate': 1.4693140794223829e-05, 'epoch': 2.25}
対照学習損失: 3.503499465296045e-05
対照学習損失: 1.3216709703556262e-05
{'loss': 0.0, 'grad_norm': 0.001097873435355723, 'learning_rate': 1.4675090252707583e-05, 'epoch': 2.25}
対照学習損失: 2.80052609014092e-05
対照学習損失: 0.00023548409808427095
{'loss': 0.0001, 'grad_norm': 0.01072356104850769, 'learning_rate': 1.4657039711191336e-05, 'epoch': 2.25}
対照学習損失: 7.073042797856033e-05
対照学習損失: 0.0005392839666455984
{'loss': 0.0003, 'grad_norm': 0.014542542397975922, 'learning_rate': 1.463898916967509e-05, 'epoch': 2.26}
対照学習損失: 4.6599292545579374e-05
対照学習損失: 3.3265274396399036e-05
{'loss': 0.0, 'grad_norm': 0.0010784863261505961, 'learning_rate': 1.4620938628158846e-05, 'epoch': 2.26}
対照学習損失: 8.093741780612618e-05
対照学習損失: 9.670765393821057e-06
{'loss': 0.0, 'grad_norm': 0.002680913545191288, 'learning_rate': 1.46028880866426e-05, 'epoch': 2.26}
対照学習損失: 1.367905679217074e-05
対照学習損失: 6.386648601619527e-05
{'loss': 0.0, 'grad_norm': 0.0015357935335487127, 'learning_rate': 1.4584837545126353e-05, 'epoch': 2.26}
対照学習損失: 1.3052951544523239e-05
対照学習損失: 1.3909858353144955e-05
{'loss': 0.0, 'grad_norm': 0.0003629709535744041, 'learning_rate': 1.4566787003610107e-05, 'epoch': 2.26}
対照学習損失: 0.00012663596135098487
対照学習損失: 0.00014667458890471607
{'loss': 0.0001, 'grad_norm': 0.0047134822234511375, 'learning_rate': 1.4548736462093864e-05, 'epoch': 2.26}
対照学習損失: 4.2448878957657143e-05
対照学習損失: 1.4148321497486904e-05
{'loss': 0.0, 'grad_norm': 0.000967318715993315, 'learning_rate': 1.453068592057762e-05, 'epoch': 2.26}
対照学習損失: 1.6189866073546e-05
対照学習損失: 2.1642348656314425e-05
{'loss': 0.0, 'grad_norm': 0.0006261045346036553, 'learning_rate': 1.4512635379061374e-05, 'epoch': 2.26}
対照学習損失: 0.0037625085096806288
対照学習損失: 0.00011809794523287565
{'loss': 0.0019, 'grad_norm': 0.11414218693971634, 'learning_rate': 1.4494584837545128e-05, 'epoch': 2.26}
対照学習損失: 0.00021255314641166478
対照学習損失: 9.29762318264693e-05
{'loss': 0.0002, 'grad_norm': 0.010702870786190033, 'learning_rate': 1.4476534296028881e-05, 'epoch': 2.26}
対照学習損失: 0.000502634618896991
対照学習損失: 1.0117743840964977e-05
{'loss': 0.0003, 'grad_norm': 0.01234481856226921, 'learning_rate': 1.4458483754512637e-05, 'epoch': 2.27}
対照学習損失: 7.58393362048082e-05
対照学習損失: 1.7836275219451636e-05
{'loss': 0.0, 'grad_norm': 0.0019188288133591413, 'learning_rate': 1.444043321299639e-05, 'epoch': 2.27}
対照学習損失: 0.0005169317009858787
対照学習損失: 4.598739542416297e-05
{'loss': 0.0003, 'grad_norm': 0.014974256046116352, 'learning_rate': 1.4422382671480145e-05, 'epoch': 2.27}
対照学習損失: 8.620210792287253e-06
対照学習損失: 4.778876609634608e-05
{'loss': 0.0, 'grad_norm': 0.0008850705926306546, 'learning_rate': 1.4404332129963898e-05, 'epoch': 2.27}
対照学習損失: 2.6082874683197588e-05
対照学習損失: 0.00045988132478669286
{'loss': 0.0002, 'grad_norm': 0.013684913516044617, 'learning_rate': 1.4386281588447654e-05, 'epoch': 2.27}
対照学習損失: 2.1276639017742127e-05
対照学習損失: 1.591389809618704e-05
{'loss': 0.0, 'grad_norm': 0.0006744064739905298, 'learning_rate': 1.4368231046931408e-05, 'epoch': 2.27}
対照学習損失: 4.834102946915664e-05
対照学習損失: 5.930634870310314e-06
{'loss': 0.0, 'grad_norm': 0.0008552918443456292, 'learning_rate': 1.4350180505415165e-05, 'epoch': 2.27}
対照学習損失: 1.7061418475350365e-05
対照学習損失: 5.3995856433175504e-05
{'loss': 0.0, 'grad_norm': 0.0011193499667569995, 'learning_rate': 1.4332129963898919e-05, 'epoch': 2.27}
対照学習損失: 3.0097569833742455e-05
対照学習損失: 9.851131471805274e-05
{'loss': 0.0001, 'grad_norm': 0.002339847618713975, 'learning_rate': 1.4314079422382673e-05, 'epoch': 2.27}
対照学習損失: 2.3952537958393805e-05
対照学習損失: 9.931508429872338e-06
{'loss': 0.0, 'grad_norm': 0.00049576599849388, 'learning_rate': 1.4296028880866428e-05, 'epoch': 2.27}
対照学習損失: 3.635688699432649e-05
対照学習損失: 2.2364816686604172e-05
{'loss': 0.0, 'grad_norm': 0.0008531363564543426, 'learning_rate': 1.4277978339350182e-05, 'epoch': 2.27}
対照学習損失: 2.9636485123774037e-05
対照学習損失: 5.3827810916118324e-05
{'loss': 0.0, 'grad_norm': 0.0015907114138826728, 'learning_rate': 1.4259927797833936e-05, 'epoch': 2.28}
対照学習損失: 3.442653905949555e-05
対照学習損失: 6.65420520817861e-05
{'loss': 0.0001, 'grad_norm': 0.0026685858611017466, 'learning_rate': 1.424187725631769e-05, 'epoch': 2.28}
対照学習損失: 2.8853170078946277e-05
対照学習損失: 0.0007952661835588515
{'loss': 0.0004, 'grad_norm': 0.020793357864022255, 'learning_rate': 1.4223826714801445e-05, 'epoch': 2.28}
対照学習損失: 0.00023941426479723305
対照学習損失: 7.592394103994593e-05
{'loss': 0.0002, 'grad_norm': 0.015530250035226345, 'learning_rate': 1.4205776173285199e-05, 'epoch': 2.28}
対照学習損失: 2.045116707449779e-05
対照学習損失: 2.5561967049725354e-05
{'loss': 0.0, 'grad_norm': 0.000885084446053952, 'learning_rate': 1.4187725631768953e-05, 'epoch': 2.28}
対照学習損失: 9.104486707656179e-06
対照学習損失: 0.0001080025831470266
{'loss': 0.0001, 'grad_norm': 0.00278641190379858, 'learning_rate': 1.4169675090252706e-05, 'epoch': 2.28}
対照学習損失: 4.3725383875425905e-05
対照学習損失: 0.0001459534978494048
{'loss': 0.0001, 'grad_norm': 0.0038143827114254236, 'learning_rate': 1.4151624548736464e-05, 'epoch': 2.28}
対照学習損失: 2.0622366719180718e-05
対照学習損失: 1.870047526608687e-05
{'loss': 0.0, 'grad_norm': 0.0004566390416584909, 'learning_rate': 1.4133574007220217e-05, 'epoch': 2.28}
対照学習損失: 1.2032485756208189e-05
対照学習損失: 3.201293293386698e-05
{'loss': 0.0, 'grad_norm': 0.0009594886796548963, 'learning_rate': 1.4115523465703973e-05, 'epoch': 2.28}
対照学習損失: 3.233308598282747e-05
対照学習損失: 3.490293602226302e-05
{'loss': 0.0, 'grad_norm': 0.001148226554505527, 'learning_rate': 1.4097472924187727e-05, 'epoch': 2.28}
対照学習損失: 1.0959554856526665e-05
対照学習損失: 0.00015699045616202056
{'loss': 0.0001, 'grad_norm': 0.0037729167379438877, 'learning_rate': 1.407942238267148e-05, 'epoch': 2.28}
対照学習損失: 1.3239111467555631e-05
対照学習損失: 4.1625058656791225e-05
{'loss': 0.0, 'grad_norm': 0.0022629862651228905, 'learning_rate': 1.4061371841155236e-05, 'epoch': 2.29}
対照学習損失: 7.402696792269126e-05
対照学習損失: 0.00025570139405317605
{'loss': 0.0002, 'grad_norm': 0.006485958117991686, 'learning_rate': 1.404332129963899e-05, 'epoch': 2.29}
対照学習損失: 0.0028174228500574827
対照学習損失: 0.00018307841673959047
{'loss': 0.0015, 'grad_norm': 0.06664349883794785, 'learning_rate': 1.4025270758122744e-05, 'epoch': 2.29}
対照学習損失: 1.2203720871184487e-05
対照学習損失: 3.77384785679169e-05
{'loss': 0.0, 'grad_norm': 0.0009380121482536197, 'learning_rate': 1.4007220216606498e-05, 'epoch': 2.29}
対照学習損失: 7.24187157175038e-06
対照学習損失: 1.5503974282182753e-05
{'loss': 0.0, 'grad_norm': 0.0003660151269286871, 'learning_rate': 1.3989169675090253e-05, 'epoch': 2.29}
対照学習損失: 7.978549547260627e-05
対照学習損失: 2.375867188675329e-05
{'loss': 0.0001, 'grad_norm': 0.001641536713577807, 'learning_rate': 1.3971119133574007e-05, 'epoch': 2.29}
対照学習損失: 3.1380481232190505e-05
対照学習損失: 2.306584246980492e-05
{'loss': 0.0, 'grad_norm': 0.0007465498056262732, 'learning_rate': 1.3953068592057764e-05, 'epoch': 2.29}
対照学習損失: 2.6216013793600723e-05
対照学習損失: 0.0005853268085047603
{'loss': 0.0003, 'grad_norm': 0.02877047099173069, 'learning_rate': 1.3935018050541518e-05, 'epoch': 2.29}
対照学習損失: 4.78565925732255e-05
対照学習損失: 1.552645153424237e-05
{'loss': 0.0, 'grad_norm': 0.0008852255414240062, 'learning_rate': 1.3916967509025272e-05, 'epoch': 2.29}
対照学習損失: 3.7628342397511005e-05
対照学習損失: 4.053095381095773e-06
{'loss': 0.0, 'grad_norm': 0.0013774617109447718, 'learning_rate': 1.3898916967509026e-05, 'epoch': 2.29}
対照学習損失: 6.310588105407078e-06
対照学習損失: 2.723845318541862e-05
{'loss': 0.0, 'grad_norm': 0.000776692817453295, 'learning_rate': 1.3880866425992781e-05, 'epoch': 2.29}
対照学習損失: 3.9961949369171634e-05
対照学習損失: 0.0004054342280142009
{'loss': 0.0002, 'grad_norm': 0.0122815677896142, 'learning_rate': 1.3862815884476535e-05, 'epoch': 2.3}
対照学習損失: 3.462043241597712e-05
対照学習損失: 1.9400376913836226e-05
{'loss': 0.0, 'grad_norm': 0.0006968801026232541, 'learning_rate': 1.3844765342960289e-05, 'epoch': 2.3}
対照学習損失: 0.00013326096814125776
対照学習損失: 1.0780874617921654e-05
{'loss': 0.0001, 'grad_norm': 0.003305056830868125, 'learning_rate': 1.3826714801444044e-05, 'epoch': 2.3}
対照学習損失: 5.869613596587442e-05
対照学習損失: 5.094024527352303e-05
{'loss': 0.0001, 'grad_norm': 0.0023375595919787884, 'learning_rate': 1.3808664259927798e-05, 'epoch': 2.3}
対照学習損失: 1.572014662087895e-05
対照学習損失: 0.007827172987163067
{'loss': 0.0039, 'grad_norm': 0.27208688855171204, 'learning_rate': 1.3790613718411552e-05, 'epoch': 2.3}
対照学習損失: 2.0555311493808404e-05
対照学習損失: 2.0749201212311164e-05
{'loss': 0.0, 'grad_norm': 0.0005642944597639143, 'learning_rate': 1.3772563176895306e-05, 'epoch': 2.3}
対照学習損失: 6.351325282594189e-05
対照学習損失: 0.00024766632122918963
{'loss': 0.0002, 'grad_norm': 0.00832288060337305, 'learning_rate': 1.3754512635379063e-05, 'epoch': 2.3}
対照学習損失: 3.251227462897077e-05
対照学習損失: 0.00011902727419510484
{'loss': 0.0001, 'grad_norm': 0.0032202056609094143, 'learning_rate': 1.3736462093862817e-05, 'epoch': 2.3}
対照学習損失: 1.3589321497420315e-05
対照学習損失: 5.30675970367156e-05
{'loss': 0.0, 'grad_norm': 0.0020661307498812675, 'learning_rate': 1.3718411552346572e-05, 'epoch': 2.3}
対照学習損失: 2.8094964363845065e-05
対照学習損失: 1.3872343515686225e-05
{'loss': 0.0, 'grad_norm': 0.0010114922188222408, 'learning_rate': 1.3700361010830326e-05, 'epoch': 2.3}
対照学習損失: 1.542237077956088e-05
対照学習損失: 3.6133576941210777e-05
{'loss': 0.0, 'grad_norm': 0.0008020811947062612, 'learning_rate': 1.368231046931408e-05, 'epoch': 2.3}
対照学習損失: 3.4806034818757325e-05
対照学習損失: 1.1846210327348672e-05
{'loss': 0.0, 'grad_norm': 0.0007688262849114835, 'learning_rate': 1.3664259927797834e-05, 'epoch': 2.31}
対照学習損失: 5.439473534352146e-05
対照学習損失: 2.9419998099911027e-05
{'loss': 0.0, 'grad_norm': 0.0017915957141667604, 'learning_rate': 1.364620938628159e-05, 'epoch': 2.31}
対照学習損失: 3.829366687568836e-05
対照学習損失: 2.5710709451232105e-05
{'loss': 0.0, 'grad_norm': 0.0008274936117231846, 'learning_rate': 1.3628158844765343e-05, 'epoch': 2.31}
対照学習損失: 1.911718209157698e-05
対照学習損失: 1.9288991097710095e-05
{'loss': 0.0, 'grad_norm': 0.0005562967271544039, 'learning_rate': 1.3610108303249097e-05, 'epoch': 2.31}
対照学習損失: 7.9869487308315e-06
対照学習損失: 7.159927008615341e-06
{'loss': 0.0, 'grad_norm': 0.00019039970356971025, 'learning_rate': 1.3592057761732852e-05, 'epoch': 2.31}
対照学習損失: 0.0019218276720494032
対照学習損失: 7.02933466527611e-05
{'loss': 0.001, 'grad_norm': 0.25597915053367615, 'learning_rate': 1.3574007220216608e-05, 'epoch': 2.31}
対照学習損失: 0.000525925075635314
対照学習損失: 0.00010342199675505981
{'loss': 0.0003, 'grad_norm': 0.02245364710688591, 'learning_rate': 1.3555956678700363e-05, 'epoch': 2.31}
対照学習損失: 8.01764108473435e-05
対照学習損失: 5.791832154500298e-05
{'loss': 0.0001, 'grad_norm': 0.002135474467650056, 'learning_rate': 1.3537906137184117e-05, 'epoch': 2.31}
対照学習損失: 0.0001817608717828989
対照学習損失: 0.00021191241103224456
{'loss': 0.0002, 'grad_norm': 0.0056952377781271935, 'learning_rate': 1.3519855595667871e-05, 'epoch': 2.31}
対照学習損失: 1.0304013812856283e-05
対照学習損失: 3.416492836549878e-05
{'loss': 0.0, 'grad_norm': 0.0007988041616044939, 'learning_rate': 1.3501805054151625e-05, 'epoch': 2.31}
対照学習損失: 1.9810264348052442e-05
対照学習損失: 2.0801235223188996e-05
{'loss': 0.0, 'grad_norm': 0.0008956784731708467, 'learning_rate': 1.348375451263538e-05, 'epoch': 2.31}
対照学習損失: 4.1584804421290755e-05
対照学習損失: 3.241538070142269e-05
{'loss': 0.0, 'grad_norm': 0.0009102036710828543, 'learning_rate': 1.3465703971119134e-05, 'epoch': 2.32}
対照学習損失: 3.882625242113136e-05
対照学習損失: 2.833997677953448e-05
{'loss': 0.0, 'grad_norm': 0.001738844788633287, 'learning_rate': 1.3447653429602888e-05, 'epoch': 2.32}
対照学習損失: 3.5691748053068295e-05
対照学習損失: 7.636773261765484e-06
{'loss': 0.0, 'grad_norm': 0.000960102304816246, 'learning_rate': 1.3429602888086642e-05, 'epoch': 2.32}
対照学習損失: 0.00016954205057118088
対照学習損失: 0.0012173117138445377
{'loss': 0.0007, 'grad_norm': 0.04585856944322586, 'learning_rate': 1.3411552346570397e-05, 'epoch': 2.32}
対照学習損失: 2.568007585068699e-05
対照学習損失: 0.000329531729221344
{'loss': 0.0002, 'grad_norm': 0.009161418303847313, 'learning_rate': 1.3393501805054151e-05, 'epoch': 2.32}
対照学習損失: 3.7477908335858956e-05
対照学習損失: 0.0003275129711255431
{'loss': 0.0002, 'grad_norm': 0.01983347162604332, 'learning_rate': 1.3375451263537908e-05, 'epoch': 2.32}
対照学習損失: 1.4468831068370491e-05
対照学習損失: 1.4036639186087996e-05
{'loss': 0.0, 'grad_norm': 0.00032539592939428985, 'learning_rate': 1.3357400722021662e-05, 'epoch': 2.32}
対照学習損失: 6.987327651586384e-05
対照学習損失: 0.00021250772988423705
{'loss': 0.0001, 'grad_norm': 0.013494467362761497, 'learning_rate': 1.3339350180505416e-05, 'epoch': 2.32}
対照学習損失: 1.4125877896731254e-05
対照学習損失: 0.005935991182923317
{'loss': 0.003, 'grad_norm': 0.12188921123743057, 'learning_rate': 1.3321299638989171e-05, 'epoch': 2.32}
対照学習損失: 3.1303599826060236e-05
対照学習損失: 2.7006621166947298e-05
{'loss': 0.0, 'grad_norm': 0.0009251389419659972, 'learning_rate': 1.3303249097472925e-05, 'epoch': 2.32}
対照学習損失: 0.00022591171727981418
対照学習損失: 0.00010181866673519835
{'loss': 0.0002, 'grad_norm': 0.004938409198075533, 'learning_rate': 1.3285198555956679e-05, 'epoch': 2.32}
対照学習損失: 8.513860666425899e-05
対照学習損失: 9.737781510921195e-06
{'loss': 0.0, 'grad_norm': 0.002128420164808631, 'learning_rate': 1.3267148014440433e-05, 'epoch': 2.33}
対照学習損失: 4.591594552039169e-05
対照学習損失: 2.2566366169485264e-05
{'loss': 0.0, 'grad_norm': 0.0023639535065740347, 'learning_rate': 1.3249097472924188e-05, 'epoch': 2.33}
対照学習損失: 1.7195610780618154e-05
対照学習損失: 0.0002910969196818769
{'loss': 0.0002, 'grad_norm': 0.017446942627429962, 'learning_rate': 1.3231046931407942e-05, 'epoch': 2.33}
対照学習損失: 2.2737705876352265e-05
対照学習損失: 7.800156890880316e-05
{'loss': 0.0001, 'grad_norm': 0.0024207301903516054, 'learning_rate': 1.3212996389891696e-05, 'epoch': 2.33}
対照学習損失: 4.349581286078319e-05
対照学習損失: 1.8439299310557544e-05
{'loss': 0.0, 'grad_norm': 0.001111346296966076, 'learning_rate': 1.319494584837545e-05, 'epoch': 2.33}
対照学習損失: 4.179888128419407e-05
対照学習損失: 0.0003334058856125921
{'loss': 0.0002, 'grad_norm': 0.02519635483622551, 'learning_rate': 1.3176895306859207e-05, 'epoch': 2.33}
対照学習損失: 6.775815563742071e-05
対照学習損失: 1.7537739040562883e-05
{'loss': 0.0, 'grad_norm': 0.002219247166067362, 'learning_rate': 1.3158844765342963e-05, 'epoch': 2.33}
対照学習損失: 3.13390810333658e-05
対照学習損失: 1.633125066291541e-05
{'loss': 0.0, 'grad_norm': 0.00099040602799505, 'learning_rate': 1.3140794223826716e-05, 'epoch': 2.33}
対照学習損失: 5.254225106909871e-05
対照学習損失: 2.7721567676053382e-05
{'loss': 0.0, 'grad_norm': 0.0011650231899693608, 'learning_rate': 1.312274368231047e-05, 'epoch': 2.33}
対照学習損失: 9.484429938311223e-06
対照学習損失: 1.1443902621977031e-05
{'loss': 0.0, 'grad_norm': 0.0002687590313144028, 'learning_rate': 1.3104693140794224e-05, 'epoch': 2.33}
対照学習損失: 1.1846124834846705e-05
対照学習損失: 2.0875237169093452e-05
{'loss': 0.0, 'grad_norm': 0.0004909230046905577, 'learning_rate': 1.308664259927798e-05, 'epoch': 2.33}
対照学習損失: 1.7560567357577384e-05
対照学習損失: 1.355957101623062e-05
{'loss': 0.0, 'grad_norm': 0.00033110257936641574, 'learning_rate': 1.3068592057761733e-05, 'epoch': 2.34}
対照学習損失: 9.968710401153658e-06
対照学習損失: 4.96182001370471e-05
{'loss': 0.0, 'grad_norm': 0.0011652291286736727, 'learning_rate': 1.3050541516245487e-05, 'epoch': 2.34}
対照学習損失: 5.531507849809714e-05
対照学習損失: 2.641072205733508e-05
{'loss': 0.0, 'grad_norm': 0.0014576816465705633, 'learning_rate': 1.3032490974729241e-05, 'epoch': 2.34}
対照学習損失: 2.2604199330089614e-05
対照学習損失: 1.73741555045126e-05
{'loss': 0.0, 'grad_norm': 0.0005052168853580952, 'learning_rate': 1.3014440433212996e-05, 'epoch': 2.34}
対照学習損失: 3.0919836717657745e-06
対照学習損失: 8.427190186921507e-05
{'loss': 0.0, 'grad_norm': 0.0015265312977135181, 'learning_rate': 1.299638989169675e-05, 'epoch': 2.34}
対照学習損失: 0.001370683778077364
対照学習損失: 0.0001157135920948349
{'loss': 0.0007, 'grad_norm': 0.052014753222465515, 'learning_rate': 1.2978339350180508e-05, 'epoch': 2.34}
対照学習損失: 7.934797395137139e-06
対照学習損失: 1.792502007447183e-05
{'loss': 0.0, 'grad_norm': 0.0009102048934437335, 'learning_rate': 1.2960288808664261e-05, 'epoch': 2.34}
対照学習損失: 2.7707181288860738e-05
対照学習損失: 1.0974553333653603e-05
{'loss': 0.0, 'grad_norm': 0.0004790782695636153, 'learning_rate': 1.2942238267148015e-05, 'epoch': 2.34}
対照学習損失: 5.891366890864447e-05
対照学習損失: 1.2069738659192808e-05
{'loss': 0.0, 'grad_norm': 0.000861811509821564, 'learning_rate': 1.292418772563177e-05, 'epoch': 2.34}
対照学習損失: 1.046037141350098e-05
対照学習損失: 2.801200025714934e-05
{'loss': 0.0, 'grad_norm': 0.0006819649715907872, 'learning_rate': 1.2906137184115524e-05, 'epoch': 2.34}
対照学習損失: 0.00019620437524281442
対照学習損失: 5.438897460408043e-06
{'loss': 0.0001, 'grad_norm': 0.0037613154854625463, 'learning_rate': 1.2888086642599278e-05, 'epoch': 2.34}
対照学習損失: 0.00019522628281265497
対照学習損失: 6.275004125200212e-05
{'loss': 0.0001, 'grad_norm': 0.007236362900584936, 'learning_rate': 1.2870036101083032e-05, 'epoch': 2.35}
対照学習損失: 1.1503567293402739e-05
対照学習損失: 3.296678187325597e-05
{'loss': 0.0, 'grad_norm': 0.0006900518201291561, 'learning_rate': 1.2851985559566788e-05, 'epoch': 2.35}
対照学習損失: 3.107348675257526e-05
対照学習損失: 8.933075150707737e-06
{'loss': 0.0, 'grad_norm': 0.0007386812940239906, 'learning_rate': 1.2833935018050541e-05, 'epoch': 2.35}
対照学習損失: 0.00011785044625867158
対照学習損失: 4.76904860988725e-05
{'loss': 0.0001, 'grad_norm': 0.0045413849875330925, 'learning_rate': 1.2815884476534295e-05, 'epoch': 2.35}
対照学習損失: 4.6697889047209173e-05
対照学習損失: 0.00013502092042472214
{'loss': 0.0001, 'grad_norm': 0.005599200259894133, 'learning_rate': 1.2797833935018049e-05, 'epoch': 2.35}
対照学習損失: 8.679853635840118e-06
対照学習損失: 2.115823735948652e-05
{'loss': 0.0, 'grad_norm': 0.0010219932300969958, 'learning_rate': 1.2779783393501806e-05, 'epoch': 2.35}
対照学習損失: 1.3060491255600937e-05
対照学習損失: 0.00010953105083899572
{'loss': 0.0001, 'grad_norm': 0.0029731180984526873, 'learning_rate': 1.2761732851985562e-05, 'epoch': 2.35}
対照学習損失: 0.00012182260252302513
対照学習損失: 4.802981857210398e-05
{'loss': 0.0001, 'grad_norm': 0.0033879911061376333, 'learning_rate': 1.2743682310469316e-05, 'epoch': 2.35}
対照学習損失: 2.299113111803308e-05
対照学習損失: 9.722878530737944e-06
{'loss': 0.0, 'grad_norm': 0.0011978225084021688, 'learning_rate': 1.272563176895307e-05, 'epoch': 2.35}
対照学習損失: 0.00010313926031813025
対照学習損失: 4.03696576540824e-05
{'loss': 0.0001, 'grad_norm': 0.0029556178487837315, 'learning_rate': 1.2707581227436823e-05, 'epoch': 2.35}
対照学習損失: 5.050303298048675e-05
対照学習損失: 0.00029031551093794405
{'loss': 0.0002, 'grad_norm': 0.0071532344445586205, 'learning_rate': 1.2689530685920579e-05, 'epoch': 2.35}
対照学習損失: 0.00013429649698082358
対照学習損失: 3.5055938496952876e-05
{'loss': 0.0001, 'grad_norm': 0.0037348675541579723, 'learning_rate': 1.2671480144404333e-05, 'epoch': 2.36}
対照学習損失: 1.9229479221394286e-05
対照学習損失: 2.263424903503619e-05
{'loss': 0.0, 'grad_norm': 0.00048137421254068613, 'learning_rate': 1.2653429602888086e-05, 'epoch': 2.36}
対照学習損失: 0.00022938809706829488
対照学習損失: 0.00012055458500981331
{'loss': 0.0002, 'grad_norm': 0.010752738453447819, 'learning_rate': 1.263537906137184e-05, 'epoch': 2.36}
対照学習損失: 1.3381058124650735e-05
対照学習損失: 6.124340870883316e-06
{'loss': 0.0, 'grad_norm': 0.00019638417870737612, 'learning_rate': 1.2617328519855596e-05, 'epoch': 2.36}
対照学習損失: 4.194662324152887e-06
対照学習損失: 5.9088510170113295e-05
{'loss': 0.0, 'grad_norm': 0.0010411303956061602, 'learning_rate': 1.259927797833935e-05, 'epoch': 2.36}
対照学習損失: 1.0996839591825847e-05
対照学習損失: 5.348904960555956e-05
{'loss': 0.0, 'grad_norm': 0.0012463666498661041, 'learning_rate': 1.2581227436823107e-05, 'epoch': 2.36}
対照学習損失: 1.1823906788777094e-05
対照学習損失: 3.809891495620832e-05
{'loss': 0.0, 'grad_norm': 0.0006887924973852932, 'learning_rate': 1.256317689530686e-05, 'epoch': 2.36}
対照学習損失: 2.852659235941246e-05
対照学習損失: 0.00040880125015974045
{'loss': 0.0002, 'grad_norm': 0.009228204376995564, 'learning_rate': 1.2545126353790614e-05, 'epoch': 2.36}
対照学習損失: 2.8981636205571704e-05
対照学習損失: 2.9323702619876713e-05
{'loss': 0.0, 'grad_norm': 0.0007551621529273689, 'learning_rate': 1.252707581227437e-05, 'epoch': 2.36}
対照学習損失: 2.8860955353593454e-05
対照学習損失: 1.5258117855410092e-05
{'loss': 0.0, 'grad_norm': 0.0007746338378638029, 'learning_rate': 1.2509025270758124e-05, 'epoch': 2.36}
対照学習損失: 1.0415826181997545e-05
対照学習損失: 1.0073086741613224e-05
{'loss': 0.0, 'grad_norm': 0.00018953814287669957, 'learning_rate': 1.2490974729241878e-05, 'epoch': 2.36}
対照学習損失: 8.679733582539484e-05
対照学習損失: 9.298179065808654e-06
{'loss': 0.0, 'grad_norm': 0.002266897354274988, 'learning_rate': 1.2472924187725631e-05, 'epoch': 2.37}
対照学習損失: 1.5854424418648705e-05
対照学習損失: 1.2583741408889182e-05
{'loss': 0.0, 'grad_norm': 0.0006574091967195272, 'learning_rate': 1.2454873646209387e-05, 'epoch': 2.37}
対照学習損失: 3.663723327917978e-05
対照学習損失: 0.00010413265408715233
{'loss': 0.0001, 'grad_norm': 0.003025665180757642, 'learning_rate': 1.2436823104693142e-05, 'epoch': 2.37}
対照学習損失: 0.0007870611734688282
対照学習損失: 1.201016129925847e-05
{'loss': 0.0004, 'grad_norm': 0.03175786882638931, 'learning_rate': 1.2418772563176896e-05, 'epoch': 2.37}
対照学習損失: 0.00018711031589191407
対照学習損失: 0.00024086807388812304
{'loss': 0.0002, 'grad_norm': 0.008884675800800323, 'learning_rate': 1.240072202166065e-05, 'epoch': 2.37}
対照学習損失: 1.550402885186486e-05
対照学習損失: 0.00012413199874572456
{'loss': 0.0001, 'grad_norm': 0.0027919472195208073, 'learning_rate': 1.2382671480144405e-05, 'epoch': 2.37}
対照学習損失: 5.4021886171540245e-05
対照学習損失: 1.3850178220309317e-05
{'loss': 0.0, 'grad_norm': 0.004637658596038818, 'learning_rate': 1.236462093862816e-05, 'epoch': 2.37}
対照学習損失: 2.0294963178457692e-05
対照学習損失: 0.00011152440856676549
{'loss': 0.0001, 'grad_norm': 0.004247881006449461, 'learning_rate': 1.2346570397111915e-05, 'epoch': 2.37}
対照学習損失: 0.0012532450491562486
対照学習損失: 8.411543603870086e-06
{'loss': 0.0006, 'grad_norm': 0.04174268618226051, 'learning_rate': 1.2328519855595669e-05, 'epoch': 2.37}
対照学習損失: 0.0013800375163555145
対照学習損失: 1.2740272723021917e-05
{'loss': 0.0007, 'grad_norm': 0.027857152745127678, 'learning_rate': 1.2310469314079422e-05, 'epoch': 2.37}
対照学習損失: 2.5226758225471713e-05
対照学習損失: 2.6588470063870773e-05
{'loss': 0.0, 'grad_norm': 0.0007163063273765147, 'learning_rate': 1.2292418772563178e-05, 'epoch': 2.38}
対照学習損失: 0.0001772714895196259
対照学習損失: 0.0004394017159938812
{'loss': 0.0003, 'grad_norm': 0.015821758657693863, 'learning_rate': 1.2274368231046932e-05, 'epoch': 2.38}
対照学習損失: 4.51970481663011e-05
対照学習損失: 0.00015267562412191182
{'loss': 0.0001, 'grad_norm': 0.006186882499605417, 'learning_rate': 1.2256317689530686e-05, 'epoch': 2.38}
対照学習損失: 1.549688749946654e-05
対照学習損失: 0.007224841509014368
{'loss': 0.0036, 'grad_norm': 0.11041270941495895, 'learning_rate': 1.2238267148014441e-05, 'epoch': 2.38}
対照学習損失: 3.906048368662596e-05
対照学習損失: 4.557902502710931e-05
{'loss': 0.0, 'grad_norm': 0.003737920895218849, 'learning_rate': 1.2220216606498195e-05, 'epoch': 2.38}
対照学習損失: 1.9236558728152886e-05
対照学習損失: 7.658985123271123e-05
{'loss': 0.0, 'grad_norm': 0.002216554479673505, 'learning_rate': 1.220216606498195e-05, 'epoch': 2.38}
対照学習損失: 3.4421593682054663e-06
対照学習損失: 1.4215272130968515e-05
{'loss': 0.0, 'grad_norm': 0.00023219861031975597, 'learning_rate': 1.2184115523465704e-05, 'epoch': 2.38}
対照学習損失: 2.4630880943732336e-05
対照学習損失: 5.1697636081371456e-05
{'loss': 0.0, 'grad_norm': 0.0017843646928668022, 'learning_rate': 1.2166064981949458e-05, 'epoch': 2.38}
対照学習損失: 0.00012784285354427993
対照学習損失: 9.126821169047616e-06
{'loss': 0.0001, 'grad_norm': 0.006172045134007931, 'learning_rate': 1.2148014440433214e-05, 'epoch': 2.38}
対照学習損失: 0.0002501314738765359
対照学習損失: 2.4689663405297324e-05
{'loss': 0.0001, 'grad_norm': 0.039744358509778976, 'learning_rate': 1.2129963898916969e-05, 'epoch': 2.38}
対照学習損失: 6.427455809898674e-05
対照学習損失: 3.480506711639464e-05
{'loss': 0.0, 'grad_norm': 0.0015080347657203674, 'learning_rate': 1.2111913357400723e-05, 'epoch': 2.38}
対照学習損失: 3.376184031367302e-05
対照学習損失: 8.674701530253515e-05
{'loss': 0.0001, 'grad_norm': 0.0015598159516230226, 'learning_rate': 1.2093862815884477e-05, 'epoch': 2.39}
対照学習損失: 9.29073848965345e-06
対照学習損失: 1.7835973267210647e-05
{'loss': 0.0, 'grad_norm': 0.0009892333764582872, 'learning_rate': 1.207581227436823e-05, 'epoch': 2.39}
対照学習損失: 7.586199353681877e-05
対照学習損失: 3.050510713364929e-05
{'loss': 0.0001, 'grad_norm': 0.0020728670060634613, 'learning_rate': 1.2057761732851986e-05, 'epoch': 2.39}
対照学習損失: 0.0002648312074597925
対照学習損失: 0.00010563545220065862
{'loss': 0.0002, 'grad_norm': 0.016888665035367012, 'learning_rate': 1.2039711191335742e-05, 'epoch': 2.39}
対照学習損失: 6.410016794689e-05
対照学習損失: 0.00046805356396362185
{'loss': 0.0003, 'grad_norm': 0.011080839671194553, 'learning_rate': 1.2021660649819495e-05, 'epoch': 2.39}
対照学習損失: 0.0004092915332876146
対照学習損失: 0.00025446954532526433
{'loss': 0.0003, 'grad_norm': 0.012974659912288189, 'learning_rate': 1.200361010830325e-05, 'epoch': 2.39}
対照学習損失: 0.00022448177332989872
対照学習損失: 8.419083314947784e-06
{'loss': 0.0001, 'grad_norm': 0.00802623014897108, 'learning_rate': 1.1985559566787003e-05, 'epoch': 2.39}
対照学習損失: 0.00015638250624760985
対照学習損失: 1.9802966562565416e-05
{'loss': 0.0001, 'grad_norm': 0.004645413253456354, 'learning_rate': 1.1967509025270759e-05, 'epoch': 2.39}
対照学習損失: 6.124314950284315e-06
対照学習損失: 6.764377758372575e-05
{'loss': 0.0, 'grad_norm': 0.004179750569164753, 'learning_rate': 1.1949458483754514e-05, 'epoch': 2.39}
対照学習損失: 8.933380013331771e-05
対照学習損失: 5.169348878553137e-05
{'loss': 0.0001, 'grad_norm': 0.0022301599383354187, 'learning_rate': 1.1931407942238268e-05, 'epoch': 2.39}
対照学習損失: 1.5094487025635317e-05
対照学習損失: 5.369619248085655e-05
{'loss': 0.0, 'grad_norm': 0.00100241182371974, 'learning_rate': 1.1913357400722022e-05, 'epoch': 2.39}
対照学習損失: 6.346494774334133e-05
対照学習損失: 1.623444040887989e-05
{'loss': 0.0, 'grad_norm': 0.0018216236494481564, 'learning_rate': 1.1895306859205777e-05, 'epoch': 2.4}
対照学習損失: 1.0207150808128063e-05
対照学習損失: 4.0508042729925364e-05
{'loss': 0.0, 'grad_norm': 0.0009755514329299331, 'learning_rate': 1.1877256317689531e-05, 'epoch': 2.4}
対照学習損失: 5.991199577692896e-05
対照学習損失: 1.4505902072414756e-05
{'loss': 0.0, 'grad_norm': 0.0012266236590221524, 'learning_rate': 1.1859205776173287e-05, 'epoch': 2.4}
対照学習損失: 4.3655647459672764e-05
対照学習損失: 0.00017730215040501207
{'loss': 0.0001, 'grad_norm': 0.0034594484604895115, 'learning_rate': 1.184115523465704e-05, 'epoch': 2.4}
対照学習損失: 1.726990012684837e-05
対照学習損失: 7.293971975741442e-06
{'loss': 0.0, 'grad_norm': 0.0003421419532969594, 'learning_rate': 1.1823104693140794e-05, 'epoch': 2.4}
対照学習損失: 0.00046993274008855224
対照学習損失: 0.00018523861945141107
{'loss': 0.0003, 'grad_norm': 0.013669455423951149, 'learning_rate': 1.180505415162455e-05, 'epoch': 2.4}
対照学習損失: 7.599968375870958e-05
対照学習損失: 6.0504680732265115e-05
{'loss': 0.0001, 'grad_norm': 0.0026864928659051657, 'learning_rate': 1.1787003610108303e-05, 'epoch': 2.4}
対照学習損失: 2.4027100153034553e-05
対照学習損失: 5.3513056627707556e-05
{'loss': 0.0, 'grad_norm': 0.001429846277460456, 'learning_rate': 1.1768953068592057e-05, 'epoch': 2.4}
対照学習損失: 4.0006743802223355e-05
対照学習損失: 8.25684837764129e-05
{'loss': 0.0001, 'grad_norm': 0.0030473074875772, 'learning_rate': 1.1750902527075813e-05, 'epoch': 2.4}
対照学習損失: 1.615953260625247e-05
対照学習損失: 3.0158604204189032e-05
{'loss': 0.0, 'grad_norm': 0.0009769827593117952, 'learning_rate': 1.1732851985559568e-05, 'epoch': 2.4}
対照学習損失: 0.00010377938451711088
対照学習損失: 9.059326839633286e-05
{'loss': 0.0001, 'grad_norm': 0.002798408502712846, 'learning_rate': 1.1714801444043322e-05, 'epoch': 2.4}
対照学習損失: 9.242680243914947e-05
対照学習損失: 2.9203998565208167e-05
{'loss': 0.0001, 'grad_norm': 0.002484983531758189, 'learning_rate': 1.1696750902527076e-05, 'epoch': 2.41}
対照学習損失: 0.00029712103423662484
対照学習損失: 3.799782007263275e-06
{'loss': 0.0002, 'grad_norm': 0.006831059698015451, 'learning_rate': 1.167870036101083e-05, 'epoch': 2.41}
対照学習損失: 1.0549820217420347e-05
対照学習損失: 1.6733321899664588e-05
{'loss': 0.0, 'grad_norm': 0.00037597259506583214, 'learning_rate': 1.1660649819494585e-05, 'epoch': 2.41}
対照学習損失: 1.1600375728448853e-05
対照学習損失: 9.387606041855179e-06
{'loss': 0.0, 'grad_norm': 0.00023597994004376233, 'learning_rate': 1.164259927797834e-05, 'epoch': 2.41}
対照学習損失: 9.53510170802474e-05
対照学習損失: 7.249346253956901e-06
{'loss': 0.0001, 'grad_norm': 0.0028433261904865503, 'learning_rate': 1.1624548736462095e-05, 'epoch': 2.41}
対照学習損失: 9.01512976270169e-06
対照学習損失: 1.9258859538240358e-05
{'loss': 0.0, 'grad_norm': 0.00036969847860746086, 'learning_rate': 1.1606498194945848e-05, 'epoch': 2.41}
対照学習損失: 4.0010883822105825e-05
対照学習損失: 8.553757652407512e-05
{'loss': 0.0001, 'grad_norm': 0.002498710062354803, 'learning_rate': 1.1588447653429602e-05, 'epoch': 2.41}
対照学習損失: 3.4998425689991564e-05
対照学習損失: 4.337215796113014e-05
{'loss': 0.0, 'grad_norm': 0.0016570284496992826, 'learning_rate': 1.157039711191336e-05, 'epoch': 2.41}
対照学習損失: 6.495259003713727e-05
対照学習損失: 0.002841825596988201
{'loss': 0.0015, 'grad_norm': 0.08713392168283463, 'learning_rate': 1.1552346570397113e-05, 'epoch': 2.41}
対照学習損失: 6.079624654375948e-06
対照学習損失: 8.81390405993443e-06
{'loss': 0.0, 'grad_norm': 0.00018531673413235694, 'learning_rate': 1.1534296028880867e-05, 'epoch': 2.41}
対照学習損失: 1.6599526134086773e-05
対照学習損失: 1.0058124644274358e-05
{'loss': 0.0, 'grad_norm': 0.00026704827905632555, 'learning_rate': 1.1516245487364621e-05, 'epoch': 2.41}
対照学習損失: 1.1659909432637505e-05
対照学習損失: 1.8960650777444243e-05
{'loss': 0.0, 'grad_norm': 0.0004696856194641441, 'learning_rate': 1.1498194945848376e-05, 'epoch': 2.42}
対照学習損失: 7.204533903859556e-05
対照学習損失: 8.64024186739698e-05
{'loss': 0.0001, 'grad_norm': 0.003482745261862874, 'learning_rate': 1.148014440433213e-05, 'epoch': 2.42}
対照学習損失: 2.5986049877246842e-05
対照学習損失: 6.317075167316943e-05
{'loss': 0.0, 'grad_norm': 0.0018712851451709867, 'learning_rate': 1.1462093862815886e-05, 'epoch': 2.42}
対照学習損失: 6.646606198046356e-05
対照学習損失: 2.4764585759839974e-05
{'loss': 0.0, 'grad_norm': 0.0012960780877619982, 'learning_rate': 1.144404332129964e-05, 'epoch': 2.42}
対照学習損失: 4.659010664909147e-05
対照学習損失: 0.00011461549729574472
{'loss': 0.0001, 'grad_norm': 0.002115282928571105, 'learning_rate': 1.1425992779783393e-05, 'epoch': 2.42}
対照学習損失: 0.0018528548534959555
対照学習損失: 4.234548396198079e-05
{'loss': 0.0009, 'grad_norm': 0.05201355740427971, 'learning_rate': 1.1407942238267149e-05, 'epoch': 2.42}
対照学習損失: 3.559656033758074e-05
対照学習損失: 1.750831688696053e-05
{'loss': 0.0, 'grad_norm': 0.0008022072724997997, 'learning_rate': 1.1389891696750903e-05, 'epoch': 2.42}
対照学習損失: 2.7431535272626206e-05
対照学習損失: 0.00010681108688004315
{'loss': 0.0001, 'grad_norm': 0.004982342943549156, 'learning_rate': 1.1371841155234658e-05, 'epoch': 2.42}
対照学習損失: 6.750122338416986e-06
対照学習損失: 1.3015725016884971e-05
{'loss': 0.0, 'grad_norm': 0.000294842611765489, 'learning_rate': 1.1353790613718412e-05, 'epoch': 2.42}
対照学習損失: 5.1230414101155475e-05
対照学習損失: 1.0661345186235849e-05
{'loss': 0.0, 'grad_norm': 0.0010821190662682056, 'learning_rate': 1.1335740072202168e-05, 'epoch': 2.42}
対照学習損失: 1.6681380657246336e-05
対照学習損失: 1.5012607946118806e-05
{'loss': 0.0, 'grad_norm': 0.0004530229198280722, 'learning_rate': 1.1317689530685921e-05, 'epoch': 2.42}
対照学習損失: 0.0005781325162388384
対照学習損失: 4.125752457184717e-05
{'loss': 0.0003, 'grad_norm': 0.015030601993203163, 'learning_rate': 1.1299638989169675e-05, 'epoch': 2.43}
対照学習損失: 0.00018810914480127394
対照学習損失: 2.1539046429097652e-05
{'loss': 0.0001, 'grad_norm': 0.004270828329026699, 'learning_rate': 1.1281588447653429e-05, 'epoch': 2.43}
対照学習損失: 0.00012587786477524787
対照学習損失: 0.0011583821615204215
{'loss': 0.0006, 'grad_norm': 0.054813724011182785, 'learning_rate': 1.1263537906137184e-05, 'epoch': 2.43}
対照学習損失: 2.9003889721934684e-05
対照学習損失: 1.590672582096886e-05
{'loss': 0.0, 'grad_norm': 0.0005800684448331594, 'learning_rate': 1.124548736462094e-05, 'epoch': 2.43}
対照学習損失: 1.5057154996611644e-05
対照学習損失: 6.54894465696998e-05
{'loss': 0.0, 'grad_norm': 0.001722119515761733, 'learning_rate': 1.1227436823104694e-05, 'epoch': 2.43}
対照学習損失: 5.5687924032099545e-05
対照学習損失: 2.5241050025215372e-05
{'loss': 0.0, 'grad_norm': 0.0014428308932110667, 'learning_rate': 1.1209386281588448e-05, 'epoch': 2.43}
対照学習損失: 0.0025273452047258615
対照学習損失: 2.4346600184799172e-05
{'loss': 0.0013, 'grad_norm': 0.06893032789230347, 'learning_rate': 1.1191335740072201e-05, 'epoch': 2.43}
対照学習損失: 5.482374399434775e-05
対照学習損失: 2.579099418653641e-05
{'loss': 0.0, 'grad_norm': 0.0018228600965812802, 'learning_rate': 1.1173285198555957e-05, 'epoch': 2.43}
対照学習損失: 6.594895239686593e-05
対照学習損失: 5.3472816944122314e-05
{'loss': 0.0001, 'grad_norm': 0.0026066540740430355, 'learning_rate': 1.1155234657039712e-05, 'epoch': 2.43}
対照学習損失: 1.451346088288119e-05
対照学習損失: 6.716136704199016e-05
{'loss': 0.0, 'grad_norm': 0.0022108738776296377, 'learning_rate': 1.1137184115523466e-05, 'epoch': 2.43}
対照学習損失: 9.402535397384781e-06
対照学習損失: 8.634279947727919e-05
{'loss': 0.0, 'grad_norm': 0.0023141640704125166, 'learning_rate': 1.111913357400722e-05, 'epoch': 2.43}
対照学習損失: 0.0003632744192145765
対照学習損失: 0.001339147798717022
{'loss': 0.0009, 'grad_norm': 0.04727654904127121, 'learning_rate': 1.1101083032490976e-05, 'epoch': 2.44}
対照学習損失: 3.487180219963193e-05
対照学習損失: 3.3734198950696737e-05
{'loss': 0.0, 'grad_norm': 0.0010116386692970991, 'learning_rate': 1.1083032490974731e-05, 'epoch': 2.44}
対照学習損失: 2.1225630916887894e-05
対照学習損失: 1.4729506801813841e-05
{'loss': 0.0, 'grad_norm': 0.0004720810102298856, 'learning_rate': 1.1064981949458485e-05, 'epoch': 2.44}
対照学習損失: 7.404651842080057e-05
対照学習損失: 9.760473039932549e-05
{'loss': 0.0001, 'grad_norm': 0.002762343967333436, 'learning_rate': 1.1046931407942239e-05, 'epoch': 2.44}
対照学習損失: 2.8601654776139185e-05
対照学習損失: 0.0001368460652884096
{'loss': 0.0001, 'grad_norm': 0.003972333390265703, 'learning_rate': 1.1028880866425993e-05, 'epoch': 2.44}
対照学習損失: 0.0027503131423145533
対照学習損失: 3.084412674070336e-05
{'loss': 0.0014, 'grad_norm': 0.08641160279512405, 'learning_rate': 1.1010830324909748e-05, 'epoch': 2.44}
対照学習損失: 3.0097457056399435e-05
対照学習損失: 0.0007282028673216701
{'loss': 0.0004, 'grad_norm': 0.07186529785394669, 'learning_rate': 1.0992779783393502e-05, 'epoch': 2.44}
対照学習損失: 0.00013214464706834406
対照学習損失: 1.7090755136450753e-05
{'loss': 0.0001, 'grad_norm': 0.004291570279747248, 'learning_rate': 1.0974729241877257e-05, 'epoch': 2.44}
対照学習損失: 0.0002136231050826609
対照学習損失: 0.0012905659386888146
{'loss': 0.0008, 'grad_norm': 0.07937367260456085, 'learning_rate': 1.0956678700361011e-05, 'epoch': 2.44}
対照学習損失: 0.00014982827997300774
対照学習損失: 3.948796347685857e-06
{'loss': 0.0001, 'grad_norm': 0.008874465711414814, 'learning_rate': 1.0938628158844765e-05, 'epoch': 2.44}
対照学習損失: 0.0002867046860046685
対照学習損失: 1.4632621969212778e-05
{'loss': 0.0002, 'grad_norm': 0.015480278059840202, 'learning_rate': 1.092057761732852e-05, 'epoch': 2.44}
対照学習損失: 3.7323035940062255e-05
対照学習損失: 3.185739478794858e-05
{'loss': 0.0, 'grad_norm': 0.0008048266172409058, 'learning_rate': 1.0902527075812274e-05, 'epoch': 2.45}
対照学習損失: 1.1384299796191044e-05
対照学習損失: 1.9311151845613495e-05
{'loss': 0.0, 'grad_norm': 0.0008774036541581154, 'learning_rate': 1.088447653429603e-05, 'epoch': 2.45}
対照学習損失: 1.8961041860166006e-05
対照学習損失: 4.924103632220067e-05
{'loss': 0.0, 'grad_norm': 0.0010558495996519923, 'learning_rate': 1.0866425992779784e-05, 'epoch': 2.45}
対照学習損失: 4.917449041386135e-05
対照学習損失: 0.00011868921865243465
{'loss': 0.0001, 'grad_norm': 0.0033832918852567673, 'learning_rate': 1.084837545126354e-05, 'epoch': 2.45}
対照学習損失: 0.0003902333846781403
対照学習損失: 1.4408890820050146e-05
{'loss': 0.0002, 'grad_norm': 0.010981733910739422, 'learning_rate': 1.0830324909747293e-05, 'epoch': 2.45}
対照学習損失: 1.2546504876809195e-05
対照学習損失: 3.527587614371441e-05
{'loss': 0.0, 'grad_norm': 0.0006176048773340881, 'learning_rate': 1.0812274368231047e-05, 'epoch': 2.45}
対照学習損失: 0.000524291186593473
対照学習損失: 1.3559872968471609e-05
{'loss': 0.0003, 'grad_norm': 0.03167438879609108, 'learning_rate': 1.07942238267148e-05, 'epoch': 2.45}
対照学習損失: 4.9095513531938195e-05
対照学習損失: 0.00025860167806968093
{'loss': 0.0002, 'grad_norm': 0.007594145368784666, 'learning_rate': 1.0776173285198556e-05, 'epoch': 2.45}
対照学習損失: 0.0007050737622193992
対照学習損失: 1.0751004083431326e-05
{'loss': 0.0004, 'grad_norm': 0.020493896678090096, 'learning_rate': 1.0758122743682312e-05, 'epoch': 2.45}
対照学習損失: 2.0048866645083763e-05
対照学習損失: 0.00010872895654756576
{'loss': 0.0001, 'grad_norm': 0.004825976677238941, 'learning_rate': 1.0740072202166066e-05, 'epoch': 2.45}
対照学習損失: 0.00017302727792412043
対照学習損失: 0.006258551962673664
{'loss': 0.0032, 'grad_norm': 0.18798980116844177, 'learning_rate': 1.072202166064982e-05, 'epoch': 2.45}
対照学習損失: 1.83424235729035e-05
対照学習損失: 0.0006542420014739037
{'loss': 0.0003, 'grad_norm': 0.023059122264385223, 'learning_rate': 1.0703971119133573e-05, 'epoch': 2.46}
対照学習損失: 2.5509612896712497e-05
対照学習損失: 7.204619123513112e-06
{'loss': 0.0, 'grad_norm': 0.0005431980243884027, 'learning_rate': 1.068592057761733e-05, 'epoch': 2.46}
対照学習損失: 3.2637024560244754e-05
対照学習損失: 1.627121673664078e-05
{'loss': 0.0, 'grad_norm': 0.000972106063272804, 'learning_rate': 1.0667870036101084e-05, 'epoch': 2.46}
対照学習損失: 2.8279599064262584e-05
対照学習損失: 4.360439197625965e-05
{'loss': 0.0, 'grad_norm': 0.0014582306612282991, 'learning_rate': 1.0649819494584838e-05, 'epoch': 2.46}
対照学習損失: 2.928678077296354e-05
対照学習損失: 1.2308104487601668e-05
{'loss': 0.0, 'grad_norm': 0.0005746838869526982, 'learning_rate': 1.0631768953068592e-05, 'epoch': 2.46}
対照学習損失: 5.754264930146746e-05
対照学習損失: 5.8915509725920856e-05
{'loss': 0.0001, 'grad_norm': 0.0020844275131821632, 'learning_rate': 1.0613718411552347e-05, 'epoch': 2.46}
対照学習損失: 2.0718171072076075e-05
対照学習損失: 1.696444087428972e-05
{'loss': 0.0, 'grad_norm': 0.0008211550884880126, 'learning_rate': 1.0595667870036103e-05, 'epoch': 2.46}
対照学習損失: 0.00012204845552332699
対照学習損失: 1.9221373804612085e-05
{'loss': 0.0001, 'grad_norm': 0.0057334438897669315, 'learning_rate': 1.0577617328519857e-05, 'epoch': 2.46}
対照学習損失: 8.4475519543048e-05
対照学習損失: 3.228160858270712e-05
{'loss': 0.0001, 'grad_norm': 0.0026314465794712305, 'learning_rate': 1.055956678700361e-05, 'epoch': 2.46}
対照学習損失: 1.1227853974560276e-05
対照学習損失: 1.4595383618143387e-05
{'loss': 0.0, 'grad_norm': 0.0002782712981570512, 'learning_rate': 1.0541516245487364e-05, 'epoch': 2.46}
対照学習損失: 1.0825488971022423e-05
対照学習損失: 2.6404144591651857e-05
{'loss': 0.0, 'grad_norm': 0.0005194395198486745, 'learning_rate': 1.052346570397112e-05, 'epoch': 2.46}
対照学習損失: 9.4099832494976e-06
対照学習損失: 1.031147075991612e-05
{'loss': 0.0, 'grad_norm': 0.0002201359166065231, 'learning_rate': 1.0505415162454874e-05, 'epoch': 2.47}
対照学習損失: 2.4727567506488413e-05
対照学習損失: 1.0326377378078178e-05
{'loss': 0.0, 'grad_norm': 0.0004326725611463189, 'learning_rate': 1.0487364620938629e-05, 'epoch': 2.47}
対照学習損失: 0.00014978167018853128
対照学習損失: 7.852857379475608e-06
{'loss': 0.0001, 'grad_norm': 0.0036841228138655424, 'learning_rate': 1.0469314079422383e-05, 'epoch': 2.47}
対照学習損失: 8.381814950553235e-06
対照学習損失: 0.00028128334088250995
{'loss': 0.0001, 'grad_norm': 0.009553322568535805, 'learning_rate': 1.0451263537906138e-05, 'epoch': 2.47}
対照学習損失: 0.00010351256059948355
対照学習損失: 1.4684589586977381e-05
{'loss': 0.0001, 'grad_norm': 0.0035321093164384365, 'learning_rate': 1.0433212996389892e-05, 'epoch': 2.47}
対照学習損失: 2.549455348344054e-05
対照学習損失: 0.0001155973513959907
{'loss': 0.0001, 'grad_norm': 0.0029350079130381346, 'learning_rate': 1.0415162454873646e-05, 'epoch': 2.47}
対照学習損失: 2.4786229914752766e-05
対照学習損失: 2.8094842491555028e-05
{'loss': 0.0, 'grad_norm': 0.0007364785415120423, 'learning_rate': 1.0397111913357402e-05, 'epoch': 2.47}
対照学習損失: 0.0004068067064508796
対照学習損失: 0.0016885901568457484
{'loss': 0.001, 'grad_norm': 0.04411379247903824, 'learning_rate': 1.0379061371841155e-05, 'epoch': 2.47}
対照学習損失: 8.099559636320919e-05
対照学習損失: 0.0016306319739669561
{'loss': 0.0009, 'grad_norm': 0.04075958952307701, 'learning_rate': 1.0361010830324911e-05, 'epoch': 2.47}
対照学習損失: 5.84866211283952e-06
対照学習損失: 5.938896720181219e-05
{'loss': 0.0, 'grad_norm': 0.0019240685505792499, 'learning_rate': 1.0342960288808665e-05, 'epoch': 2.47}
対照学習損失: 2.0562689314829186e-05
対照学習損失: 5.3718385970569216e-06
{'loss': 0.0, 'grad_norm': 0.00047293188981711864, 'learning_rate': 1.0324909747292419e-05, 'epoch': 2.48}
対照学習損失: 1.8208691471954808e-05
対照学習損失: 7.662936695851386e-05
{'loss': 0.0, 'grad_norm': 0.0016794996336102486, 'learning_rate': 1.0306859205776172e-05, 'epoch': 2.48}
対照学習損失: 6.679120269836858e-05
対照学習損失: 5.386748853197787e-06
{'loss': 0.0, 'grad_norm': 0.0019156162161380053, 'learning_rate': 1.028880866425993e-05, 'epoch': 2.48}
対照学習損失: 6.884289632580476e-06
対照学習損失: 1.329140013694996e-05
{'loss': 0.0, 'grad_norm': 0.00026205641916021705, 'learning_rate': 1.0270758122743683e-05, 'epoch': 2.48}
対照学習損失: 0.00023034756304696202
対照学習損失: 4.3849071516888216e-05
{'loss': 0.0001, 'grad_norm': 0.00963572971522808, 'learning_rate': 1.0252707581227437e-05, 'epoch': 2.48}
対照学習損失: 2.506799137336202e-05
対照学習損失: 4.758854993269779e-05
{'loss': 0.0, 'grad_norm': 0.001628679921850562, 'learning_rate': 1.0234657039711191e-05, 'epoch': 2.48}
対照学習損失: 0.00010224138532066718
対照学習損失: 0.00019694509683176875
{'loss': 0.0001, 'grad_norm': 0.010904315859079361, 'learning_rate': 1.0216606498194947e-05, 'epoch': 2.48}
対照学習損失: 1.3976923582958989e-05
対照学習損失: 5.9447844250826165e-05
{'loss': 0.0, 'grad_norm': 0.00239878729917109, 'learning_rate': 1.0198555956678702e-05, 'epoch': 2.48}
対照学習損失: 1.5757617802591994e-05
対照学習損失: 1.2337945008766837e-05
{'loss': 0.0, 'grad_norm': 0.0003691932070069015, 'learning_rate': 1.0180505415162456e-05, 'epoch': 2.48}
対照学習損失: 2.2096006432548165e-05
対照学習損失: 6.215140456333756e-05
{'loss': 0.0, 'grad_norm': 0.0011476136278361082, 'learning_rate': 1.016245487364621e-05, 'epoch': 2.48}
対照学習損失: 7.2418702075083274e-06
対照学習損失: 5.236396100372076e-05
{'loss': 0.0, 'grad_norm': 0.0008964426233433187, 'learning_rate': 1.0144404332129964e-05, 'epoch': 2.48}
対照学習損失: 1.073620660463348e-05
対照学習損失: 8.901767432689667e-05
{'loss': 0.0, 'grad_norm': 0.002125787315890193, 'learning_rate': 1.0126353790613719e-05, 'epoch': 2.49}
対照学習損失: 1.3552382370107807e-05
対照学習損失: 1.0929836207651533e-05
{'loss': 0.0, 'grad_norm': 0.0002619471924845129, 'learning_rate': 1.0108303249097473e-05, 'epoch': 2.49}
対照学習損失: 1.413345307810232e-05
対照学習損失: 3.9625894714845344e-05
{'loss': 0.0, 'grad_norm': 0.0010406309738755226, 'learning_rate': 1.0090252707581228e-05, 'epoch': 2.49}
対照学習損失: 7.323803401959594e-06
対照学習損失: 1.4252629625843838e-05
{'loss': 0.0, 'grad_norm': 0.0002893459750339389, 'learning_rate': 1.0072202166064982e-05, 'epoch': 2.49}
対照学習損失: 2.920496444858145e-05
対照学習損失: 8.341269131051376e-05
{'loss': 0.0001, 'grad_norm': 0.0014630700461566448, 'learning_rate': 1.0054151624548738e-05, 'epoch': 2.49}
対照学習損失: 0.0004182570264674723
対照学習損失: 2.2961077775107697e-05
{'loss': 0.0002, 'grad_norm': 0.02134765312075615, 'learning_rate': 1.0036101083032491e-05, 'epoch': 2.49}
対照学習損失: 1.6218447854043916e-05
対照学習損失: 1.1272553820163012e-05
{'loss': 0.0, 'grad_norm': 0.0004881557251792401, 'learning_rate': 1.0018050541516245e-05, 'epoch': 2.49}
対照学習損失: 0.001295317430049181
対照学習損失: 7.737799023743719e-05
{'loss': 0.0007, 'grad_norm': 0.024887727573513985, 'learning_rate': 1e-05, 'epoch': 2.49}
対照学習損失: 1.0803226359712426e-05
対照学習損失: 2.2864769562147558e-05
{'loss': 0.0, 'grad_norm': 0.000586754409596324, 'learning_rate': 9.981949458483755e-06, 'epoch': 2.49}
対照学習損失: 0.002638430567458272
対照学習損失: 2.8183238100609742e-05
{'loss': 0.0013, 'grad_norm': 0.08238841593265533, 'learning_rate': 9.96389891696751e-06, 'epoch': 2.49}
対照学習損失: 1.2181421880086418e-05
対照学習損失: 0.0008174704853445292
{'loss': 0.0004, 'grad_norm': 0.04373186454176903, 'learning_rate': 9.945848375451264e-06, 'epoch': 2.49}
対照学習損失: 1.6666246665408835e-05
対照学習損失: 1.4863433534628712e-05
{'loss': 0.0, 'grad_norm': 0.00041304773185402155, 'learning_rate': 9.927797833935018e-06, 'epoch': 2.5}
対照学習損失: 5.315892485668883e-05
対照学習損失: 4.578620792017318e-05
{'loss': 0.0, 'grad_norm': 0.0022688789758831263, 'learning_rate': 9.909747292418773e-06, 'epoch': 2.5}
対照学習損失: 0.007347323931753635
対照学習損失: 5.945529665041249e-06
{'loss': 0.0037, 'grad_norm': 0.12245412170886993, 'learning_rate': 9.891696750902527e-06, 'epoch': 2.5}
対照学習損失: 3.636777546489611e-05
対照学習損失: 2.6328038075007498e-05
{'loss': 0.0, 'grad_norm': 0.0011932258494198322, 'learning_rate': 9.873646209386283e-06, 'epoch': 2.5}
対照学習損失: 9.660529030952603e-05
対照学習損失: 8.240272109105717e-06
{'loss': 0.0001, 'grad_norm': 0.0034185012336820364, 'learning_rate': 9.855595667870036e-06, 'epoch': 2.5}
対照学習損失: 5.9604244597721845e-06
対照学習損失: 5.256693111732602e-05
{'loss': 0.0, 'grad_norm': 0.0011370383435860276, 'learning_rate': 9.83754512635379e-06, 'epoch': 2.5}
対照学習損失: 0.00016155392222572118
対照学習損失: 1.1533104043337516e-05
{'loss': 0.0001, 'grad_norm': 0.004489319398999214, 'learning_rate': 9.819494584837546e-06, 'epoch': 2.5}
対照学習損失: 1.067646371666342e-05
対照学習損失: 2.9759514291072264e-05
{'loss': 0.0, 'grad_norm': 0.0013014266733080149, 'learning_rate': 9.801444043321301e-06, 'epoch': 2.5}
対照学習損失: 1.53326091094641e-05
対照学習損失: 4.19398165831808e-05
{'loss': 0.0, 'grad_norm': 0.001554126967675984, 'learning_rate': 9.783393501805055e-06, 'epoch': 2.5}
対照学習損失: 2.1888485207455233e-05
対照学習損失: 0.00010621953697409481
{'loss': 0.0001, 'grad_norm': 0.011770403012633324, 'learning_rate': 9.765342960288809e-06, 'epoch': 2.5}
対照学習損失: 7.4579729698598385e-06
対照学習損失: 5.752271681558341e-05
{'loss': 0.0, 'grad_norm': 0.0022712741047143936, 'learning_rate': 9.747292418772563e-06, 'epoch': 2.5}
対照学習損失: 0.0006629321142099798
対照学習損失: 9.733373735798523e-05
{'loss': 0.0004, 'grad_norm': 0.07337683439254761, 'learning_rate': 9.729241877256318e-06, 'epoch': 2.51}
対照学習損失: 1.4304743672255427e-05
対照学習損失: 3.369984915480018e-05
{'loss': 0.0, 'grad_norm': 0.0012594035360962152, 'learning_rate': 9.711191335740074e-06, 'epoch': 2.51}
対照学習損失: 2.0219042198732495e-05
対照学習損失: 1.8476854165783152e-05
{'loss': 0.0, 'grad_norm': 0.000514662591740489, 'learning_rate': 9.693140794223828e-06, 'epoch': 2.51}
対照学習損失: 5.321408389136195e-05
対照学習損失: 0.00011007775901816785
{'loss': 0.0001, 'grad_norm': 0.0026714988052845, 'learning_rate': 9.675090252707581e-06, 'epoch': 2.51}
対照学習損失: 1.6443198546767235e-05
対照学習損失: 3.307130828034133e-05
{'loss': 0.0, 'grad_norm': 0.000832421996165067, 'learning_rate': 9.657039711191337e-06, 'epoch': 2.51}
対照学習損失: 1.401419831381645e-05
対照学習損失: 6.931330426596105e-05
{'loss': 0.0, 'grad_norm': 0.0023329921532422304, 'learning_rate': 9.63898916967509e-06, 'epoch': 2.51}
対照学習損失: 1.9437546143308282e-05
対照学習損失: 9.463331662118435e-05
{'loss': 0.0001, 'grad_norm': 0.011727923527359962, 'learning_rate': 9.620938628158845e-06, 'epoch': 2.51}
対照学習損失: 1.0557365385466255e-05
対照学習損失: 3.373410436324775e-05
{'loss': 0.0, 'grad_norm': 0.000569663243368268, 'learning_rate': 9.6028880866426e-06, 'epoch': 2.51}
対照学習損失: 6.742700861650519e-06
対照学習損失: 0.00013477196625899523
{'loss': 0.0001, 'grad_norm': 0.002996502909809351, 'learning_rate': 9.584837545126354e-06, 'epoch': 2.51}
対照学習損失: 4.045651621709112e-06
対照学習損失: 5.103609055367997e-06
{'loss': 0.0, 'grad_norm': 8.237335714511573e-05, 'learning_rate': 9.56678700361011e-06, 'epoch': 2.51}
対照学習損失: 2.6246474590152502e-05
対照学習損失: 1.5153901586018037e-05
{'loss': 0.0, 'grad_norm': 0.0010881592752411962, 'learning_rate': 9.548736462093863e-06, 'epoch': 2.51}
対照学習損失: 2.1330024537746795e-05
対照学習損失: 2.589572250144556e-05
{'loss': 0.0, 'grad_norm': 0.0007660218398086727, 'learning_rate': 9.530685920577617e-06, 'epoch': 2.52}
対照学習損失: 4.7785870265215635e-05
対照学習損失: 2.587429480627179e-05
{'loss': 0.0, 'grad_norm': 0.0012809342006221414, 'learning_rate': 9.512635379061373e-06, 'epoch': 2.52}
対照学習損失: 6.236085482669296e-06
対照学習損失: 1.2390049960231408e-05
{'loss': 0.0, 'grad_norm': 0.00022834347328171134, 'learning_rate': 9.494584837545126e-06, 'epoch': 2.52}
対照学習損失: 9.11187999008689e-06
対照学習損失: 3.444550384301692e-05
{'loss': 0.0, 'grad_norm': 0.0010966658592224121, 'learning_rate': 9.476534296028882e-06, 'epoch': 2.52}
対照学習損失: 0.00028963538352400064
対照学習損失: 0.00014032436592970043
{'loss': 0.0002, 'grad_norm': 0.0077707357704639435, 'learning_rate': 9.458483754512636e-06, 'epoch': 2.52}
対照学習損失: 2.6938314476865344e-05
対照学習損失: 8.836292181513272e-06
{'loss': 0.0, 'grad_norm': 0.0006506968638859689, 'learning_rate': 9.44043321299639e-06, 'epoch': 2.52}
対照学習損失: 4.749368963530287e-05
対照学習損失: 5.483600943989586e-06
{'loss': 0.0, 'grad_norm': 0.00172079773619771, 'learning_rate': 9.422382671480145e-06, 'epoch': 2.52}
対照学習損失: 0.00010153358743991703
対照学習損失: 2.7527237762114964e-05
{'loss': 0.0001, 'grad_norm': 0.0030173282139003277, 'learning_rate': 9.4043321299639e-06, 'epoch': 2.52}
対照学習損失: 1.3231774573796429e-05
対照学習損失: 0.00011783210356952623
{'loss': 0.0001, 'grad_norm': 0.0027899215929210186, 'learning_rate': 9.386281588447654e-06, 'epoch': 2.52}
対照学習損失: 2.2127396732685156e-05
対照学習損失: 8.59040483192075e-06
{'loss': 0.0, 'grad_norm': 0.00035733362892642617, 'learning_rate': 9.368231046931408e-06, 'epoch': 2.52}
対照学習損失: 1.2479589713620953e-05
対照学習損失: 6.936430509085767e-06
{'loss': 0.0, 'grad_norm': 0.00020776514429599047, 'learning_rate': 9.350180505415162e-06, 'epoch': 2.52}
対照学習損失: 5.395813786890358e-05
対照学習損失: 2.4444681912427768e-05
{'loss': 0.0, 'grad_norm': 0.0012660316424444318, 'learning_rate': 9.332129963898917e-06, 'epoch': 2.53}
対照学習損失: 2.4563047190895304e-05
対照学習損失: 0.0011577357072383165
{'loss': 0.0006, 'grad_norm': 0.018367473036050797, 'learning_rate': 9.314079422382673e-06, 'epoch': 2.53}
対照学習損失: 2.9800645279465243e-05
対照学習損失: 4.4129137677373365e-05
{'loss': 0.0, 'grad_norm': 0.0022905226796865463, 'learning_rate': 9.296028880866427e-06, 'epoch': 2.53}
対照学習損失: 3.0977076676208526e-05
対照学習損失: 5.592130764853209e-05
{'loss': 0.0, 'grad_norm': 0.001984509639441967, 'learning_rate': 9.27797833935018e-06, 'epoch': 2.53}
対照学習損失: 6.742705409124028e-06
対照学習損失: 3.9842125261202455e-05
{'loss': 0.0, 'grad_norm': 0.0007799454033374786, 'learning_rate': 9.259927797833934e-06, 'epoch': 2.53}
対照学習損失: 0.00028299877885729074
対照学習損失: 1.989210068131797e-05
{'loss': 0.0002, 'grad_norm': 0.008505145087838173, 'learning_rate': 9.24187725631769e-06, 'epoch': 2.53}
対照学習損失: 3.152502904413268e-05
対照学習損失: 1.5600744518451393e-05
{'loss': 0.0, 'grad_norm': 0.0015979526797309518, 'learning_rate': 9.223826714801445e-06, 'epoch': 2.53}
対照学習損失: 7.053154695313424e-05
対照学習損失: 1.2866876204498112e-05
{'loss': 0.0, 'grad_norm': 0.0013458761386573315, 'learning_rate': 9.2057761732852e-06, 'epoch': 2.53}
対照学習損失: 0.00014469931193161756
対照学習損失: 9.753633639775217e-05
{'loss': 0.0001, 'grad_norm': 0.004669188987463713, 'learning_rate': 9.187725631768953e-06, 'epoch': 2.53}
対照学習損失: 1.666629577812273e-05
対照学習損失: 1.7433756511309184e-05
{'loss': 0.0, 'grad_norm': 0.0006001560250297189, 'learning_rate': 9.169675090252709e-06, 'epoch': 2.53}
対照学習損失: 2.2872207409818657e-05
対照学習損失: 1.5980867829057388e-05
{'loss': 0.0, 'grad_norm': 0.000520259840413928, 'learning_rate': 9.151624548736462e-06, 'epoch': 2.53}
対照学習損失: 1.601804979145527e-05
対照学習損失: 0.0036105664912611246
{'loss': 0.0018, 'grad_norm': 0.09384935349225998, 'learning_rate': 9.133574007220216e-06, 'epoch': 2.54}
対照学習損失: 1.12055513454834e-05
対照学習損失: 1.9809922378044575e-05
{'loss': 0.0, 'grad_norm': 0.0005009185406379402, 'learning_rate': 9.115523465703972e-06, 'epoch': 2.54}
対照学習損失: 3.154269506921992e-05
対照学習損失: 8.438384247710928e-05
{'loss': 0.0001, 'grad_norm': 0.002252123551443219, 'learning_rate': 9.097472924187726e-06, 'epoch': 2.54}
対照学習損失: 1.3880252481612843e-05
対照学習損失: 1.0013393875851762e-05
{'loss': 0.0, 'grad_norm': 0.0002732096763793379, 'learning_rate': 9.079422382671481e-06, 'epoch': 2.54}
対照学習損失: 0.00031831362866796553
対照学習損失: 5.811399205413181e-06
{'loss': 0.0002, 'grad_norm': 0.01046418771147728, 'learning_rate': 9.061371841155235e-06, 'epoch': 2.54}
対照学習損失: 7.528594142058864e-05
対照学習損失: 1.81487193913199e-05
{'loss': 0.0, 'grad_norm': 0.0025431064423173666, 'learning_rate': 9.043321299638989e-06, 'epoch': 2.54}
対照学習損失: 1.037107540469151e-05
対照学習損失: 3.627875048550777e-05
{'loss': 0.0, 'grad_norm': 0.0009082521428354084, 'learning_rate': 9.025270758122744e-06, 'epoch': 2.54}
対照学習損失: 1.7322288840659894e-05
対照学習損失: 0.00010082269727718085
{'loss': 0.0001, 'grad_norm': 0.0029525719583034515, 'learning_rate': 9.0072202166065e-06, 'epoch': 2.54}
対照学習損失: 0.00023667346977163106
対照学習損失: 1.611531843082048e-05
{'loss': 0.0001, 'grad_norm': 0.006812158040702343, 'learning_rate': 8.989169675090254e-06, 'epoch': 2.54}
対照学習損失: 3.160308551741764e-05
対照学習損失: 1.6956788385869004e-05
{'loss': 0.0, 'grad_norm': 0.000636474578641355, 'learning_rate': 8.971119133574007e-06, 'epoch': 2.54}
対照学習損失: 0.00010953964374493808
対照学習損失: 1.1548216207302175e-05
{'loss': 0.0001, 'grad_norm': 0.0020718099549412727, 'learning_rate': 8.953068592057761e-06, 'epoch': 2.54}
対照学習損失: 1.70537932717707e-05
対照学習損失: 1.1540810191945639e-05
{'loss': 0.0, 'grad_norm': 0.00044939442886970937, 'learning_rate': 8.935018050541517e-06, 'epoch': 2.55}
対照学習損失: 9.387641512148548e-06
対照学習損失: 0.00019666632579173893
{'loss': 0.0001, 'grad_norm': 0.007257501129060984, 'learning_rate': 8.916967509025272e-06, 'epoch': 2.55}
対照学習損失: 5.329514533514157e-05
対照学習損失: 1.2345341929176357e-05
{'loss': 0.0, 'grad_norm': 0.0011210972443223, 'learning_rate': 8.898916967509026e-06, 'epoch': 2.55}
対照学習損失: 9.484416659688577e-05
対照学習損失: 0.007225523702800274
{'loss': 0.0037, 'grad_norm': 0.1424478143453598, 'learning_rate': 8.88086642599278e-06, 'epoch': 2.55}
対照学習損失: 4.234612424625084e-05
対照学習損失: 1.2069785043422598e-05
{'loss': 0.0, 'grad_norm': 0.0006876448169350624, 'learning_rate': 8.862815884476534e-06, 'epoch': 2.55}
対照学習損失: 2.4681887225597166e-05
対照学習損失: 1.2442092156561557e-05
{'loss': 0.0, 'grad_norm': 0.0004835984145756811, 'learning_rate': 8.84476534296029e-06, 'epoch': 2.55}
対照学習損失: 0.00010289868077961728
対照学習損失: 1.5533842088188976e-05
{'loss': 0.0001, 'grad_norm': 0.003463056869804859, 'learning_rate': 8.826714801444045e-06, 'epoch': 2.55}
対照学習損失: 2.0405965187819675e-05
対照学習損失: 5.885154678253457e-05
{'loss': 0.0, 'grad_norm': 0.005225270986557007, 'learning_rate': 8.808664259927798e-06, 'epoch': 2.55}
対照学習損失: 2.2537089535035193e-05
対照学習損失: 2.1665757230948657e-05
{'loss': 0.0, 'grad_norm': 0.0005095893866382539, 'learning_rate': 8.790613718411552e-06, 'epoch': 2.55}
対照学習損失: 0.033479005098342896
対照学習損失: 1.2039582543366123e-05
{'loss': 0.0167, 'grad_norm': 0.6438382267951965, 'learning_rate': 8.772563176895308e-06, 'epoch': 2.55}
対照学習損失: 4.40897565567866e-05
対照学習損失: 3.200559876859188e-05
{'loss': 0.0, 'grad_norm': 0.002016568323597312, 'learning_rate': 8.754512635379062e-06, 'epoch': 2.55}
対照学習損失: 0.0008165168110281229
対照学習損失: 5.371836778067518e-06
{'loss': 0.0004, 'grad_norm': 0.023204848170280457, 'learning_rate': 8.736462093862817e-06, 'epoch': 2.56}
対照学習損失: 2.671658330655191e-05
対照学習損失: 4.776812420459464e-05
{'loss': 0.0, 'grad_norm': 0.0009415860404260457, 'learning_rate': 8.718411552346571e-06, 'epoch': 2.56}
対照学習損失: 1.5206129319267347e-05
対照学習損失: 6.226818368304521e-05
{'loss': 0.0, 'grad_norm': 0.0030315183103084564, 'learning_rate': 8.700361010830325e-06, 'epoch': 2.56}
対照学習損失: 3.775448931264691e-05
対照学習損失: 1.3962161574454512e-05
{'loss': 0.0, 'grad_norm': 0.0009474461548961699, 'learning_rate': 8.68231046931408e-06, 'epoch': 2.56}
対照学習損失: 1.249442902917508e-05
対照学習損失: 4.534644904197194e-05
{'loss': 0.0, 'grad_norm': 0.001392196281813085, 'learning_rate': 8.664259927797834e-06, 'epoch': 2.56}
対照学習損失: 0.0009444045135751367
対照学習損失: 6.004848182783462e-05
{'loss': 0.0005, 'grad_norm': 0.019917847588658333, 'learning_rate': 8.646209386281588e-06, 'epoch': 2.56}
対照学習損失: 9.722801223688293e-06
対照学習損失: 4.5151573431212455e-05
{'loss': 0.0, 'grad_norm': 0.0011242383625358343, 'learning_rate': 8.628158844765343e-06, 'epoch': 2.56}
対照学習損失: 7.765563350403681e-05
対照学習損失: 9.737719665281475e-06
{'loss': 0.0, 'grad_norm': 0.006314466707408428, 'learning_rate': 8.610108303249099e-06, 'epoch': 2.56}
対照学習損失: 1.5168310710578226e-05
対照学習損失: 1.4401517546502873e-05
{'loss': 0.0, 'grad_norm': 0.0003955564461648464, 'learning_rate': 8.592057761732853e-06, 'epoch': 2.56}
対照学習損失: 3.0201326808310114e-05
対照学習損失: 0.001544626196846366
{'loss': 0.0008, 'grad_norm': 0.0414511039853096, 'learning_rate': 8.574007220216607e-06, 'epoch': 2.56}
対照学習損失: 2.663457053131424e-05
対照学習損失: 2.7273990781395696e-05
{'loss': 0.0, 'grad_norm': 0.0010928476694971323, 'learning_rate': 8.55595667870036e-06, 'epoch': 2.56}
対照学習損失: 1.4096129234530963e-05
対照学習損失: 1.1287458619335666e-05
{'loss': 0.0, 'grad_norm': 0.00029532474582083523, 'learning_rate': 8.537906137184116e-06, 'epoch': 2.57}
対照学習損失: 2.7587126169237308e-05
対照学習損失: 6.591719284188002e-05
{'loss': 0.0, 'grad_norm': 0.0015191128477454185, 'learning_rate': 8.519855595667871e-06, 'epoch': 2.57}
対照学習損失: 2.361605038458947e-05
対照学習損失: 0.0001266702456632629
{'loss': 0.0001, 'grad_norm': 0.003649465274065733, 'learning_rate': 8.501805054151625e-06, 'epoch': 2.57}
対照学習損失: 7.962668314576149e-05
対照学習損失: 2.7245590899838135e-05
{'loss': 0.0001, 'grad_norm': 0.00275779259391129, 'learning_rate': 8.483754512635379e-06, 'epoch': 2.57}
対照学習損失: 5.7358414778718725e-05
対照学習損失: 3.497748912195675e-05
{'loss': 0.0, 'grad_norm': 0.0014075394719839096, 'learning_rate': 8.465703971119133e-06, 'epoch': 2.57}
対照学習損失: 2.5136421754723415e-05
対照学習損失: 0.00023667560890316963
{'loss': 0.0001, 'grad_norm': 0.015264519490301609, 'learning_rate': 8.447653429602888e-06, 'epoch': 2.57}
対照学習損失: 0.00021005207963753492
対照学習損失: 1.2993474229006097e-05
{'loss': 0.0001, 'grad_norm': 0.006639599334448576, 'learning_rate': 8.429602888086644e-06, 'epoch': 2.57}
対照学習損失: 1.2248481652932242e-05
対照学習損失: 1.6651514670229517e-05
{'loss': 0.0, 'grad_norm': 0.0003465462941676378, 'learning_rate': 8.411552346570398e-06, 'epoch': 2.57}
対照学習損失: 1.873030123533681e-05
対照学習損失: 1.684548260527663e-05
{'loss': 0.0, 'grad_norm': 0.0004096168850082904, 'learning_rate': 8.393501805054152e-06, 'epoch': 2.57}
対照学習損失: 3.0445833544945344e-05
対照学習損失: 0.008785810321569443
{'loss': 0.0044, 'grad_norm': 0.307643324136734, 'learning_rate': 8.375451263537907e-06, 'epoch': 2.57}
対照学習損失: 1.9467777747195214e-05
対照学習損失: 3.688007154778461e-06
{'loss': 0.0, 'grad_norm': 0.00043164531234651804, 'learning_rate': 8.357400722021661e-06, 'epoch': 2.57}
対照学習損失: 8.102995343506336e-05
対照学習損失: 1.559377597004641e-05
{'loss': 0.0, 'grad_norm': 0.0024711794685572386, 'learning_rate': 8.339350180505416e-06, 'epoch': 2.58}
対照学習損失: 0.00024169919197447598
対照学習損失: 4.3430158257251605e-05
{'loss': 0.0001, 'grad_norm': 0.004953334107995033, 'learning_rate': 8.32129963898917e-06, 'epoch': 2.58}
対照学習損失: 0.00033006566809490323
対照学習損失: 7.200387335615233e-05
{'loss': 0.0002, 'grad_norm': 0.009209555573761463, 'learning_rate': 8.303249097472924e-06, 'epoch': 2.58}
対照学習損失: 6.697868957417086e-05
対照学習損失: 5.59856416657567e-05
{'loss': 0.0001, 'grad_norm': 0.002314138226211071, 'learning_rate': 8.28519855595668e-06, 'epoch': 2.58}
対照学習損失: 4.395460564410314e-05
対照学習損失: 0.00013092787412460893
{'loss': 0.0001, 'grad_norm': 0.009327552281320095, 'learning_rate': 8.267148014440433e-06, 'epoch': 2.58}
対照学習損失: 3.9040896808728576e-06
対照学習損失: 5.1590748626040295e-05
{'loss': 0.0, 'grad_norm': 0.0008705653017386794, 'learning_rate': 8.249097472924189e-06, 'epoch': 2.58}
対照学習損失: 1.4424016626435332e-05
対照学習損失: 4.6521348849637434e-05
{'loss': 0.0, 'grad_norm': 0.0023799557238817215, 'learning_rate': 8.231046931407943e-06, 'epoch': 2.58}
対照学習損失: 0.003444662084802985
対照学習損失: 5.222821528150234e-06
{'loss': 0.0017, 'grad_norm': 0.10579292476177216, 'learning_rate': 8.212996389891696e-06, 'epoch': 2.58}
対照学習損失: 3.1379680876852944e-05
対照学習損失: 2.1605845176964067e-05
{'loss': 0.0, 'grad_norm': 0.0013609641464427114, 'learning_rate': 8.194945848375452e-06, 'epoch': 2.58}
対照学習損失: 3.774886135943234e-05
対照学習損失: 0.0008310475386679173
{'loss': 0.0004, 'grad_norm': 0.01895316317677498, 'learning_rate': 8.176895306859206e-06, 'epoch': 2.58}
対照学習損失: 5.137651533004828e-05
対照学習損失: 2.641577520989813e-05
{'loss': 0.0, 'grad_norm': 0.0012526349164545536, 'learning_rate': 8.15884476534296e-06, 'epoch': 2.59}
対照学習損失: 9.21621631277958e-06
対照学習損失: 0.00012866347969975322
{'loss': 0.0001, 'grad_norm': 0.002938733668997884, 'learning_rate': 8.140794223826715e-06, 'epoch': 2.59}
対照学習損失: 0.00013046622916590422
対照学習損失: 3.027419552381616e-05
{'loss': 0.0001, 'grad_norm': 0.0034117186442017555, 'learning_rate': 8.12274368231047e-06, 'epoch': 2.59}
対照学習損失: 5.355106623028405e-05
対照学習損失: 4.09747299272567e-05
{'loss': 0.0, 'grad_norm': 0.002606236143037677, 'learning_rate': 8.104693140794224e-06, 'epoch': 2.59}
対照学習損失: 0.00015487833297811449
対照学習損失: 1.831310146371834e-05
{'loss': 0.0001, 'grad_norm': 0.00326743652112782, 'learning_rate': 8.086642599277978e-06, 'epoch': 2.59}
対照学習損失: 6.146684427221771e-06
対照学習損失: 5.348476770450361e-05
{'loss': 0.0, 'grad_norm': 0.0017925174906849861, 'learning_rate': 8.068592057761732e-06, 'epoch': 2.59}
対照学習損失: 0.0008851095335558057
対照学習損失: 1.0534882676438428e-05
{'loss': 0.0004, 'grad_norm': 0.02269023284316063, 'learning_rate': 8.050541516245488e-06, 'epoch': 2.59}
対照学習損失: 6.966231012484059e-06
対照学習損失: 5.759272426075768e-06
{'loss': 0.0, 'grad_norm': 0.00012044939649058506, 'learning_rate': 8.032490974729243e-06, 'epoch': 2.59}
対照学習損失: 7.420713245664956e-06
対照学習損失: 0.00013356261479202658
{'loss': 0.0001, 'grad_norm': 0.0038130050525069237, 'learning_rate': 8.014440433212997e-06, 'epoch': 2.59}
対照学習損失: 7.274420204339549e-05
対照学習損失: 2.6715353669715114e-05
{'loss': 0.0, 'grad_norm': 0.0030275529716163874, 'learning_rate': 7.99638989169675e-06, 'epoch': 2.59}
対照学習損失: 7.502662811020855e-06
対照学習損失: 1.5772402548464015e-05
{'loss': 0.0, 'grad_norm': 0.0005797771736979485, 'learning_rate': 7.978339350180505e-06, 'epoch': 2.59}
対照学習損失: 2.905532528529875e-05
対照学習損失: 2.4473240046063438e-05
{'loss': 0.0, 'grad_norm': 0.0006881050649099052, 'learning_rate': 7.96028880866426e-06, 'epoch': 2.6}
対照学習損失: 0.00011103024735348299
対照学習損失: 1.627893652766943e-05
{'loss': 0.0001, 'grad_norm': 0.002619249979034066, 'learning_rate': 7.942238267148016e-06, 'epoch': 2.6}
対照学習損失: 3.771635238081217e-05
対照学習損失: 6.265900992730167e-06
{'loss': 0.0, 'grad_norm': 0.0008425225969403982, 'learning_rate': 7.92418772563177e-06, 'epoch': 2.6}
対照学習損失: 0.00010101142106577754
対照学習損失: 1.4602856026613154e-05
{'loss': 0.0001, 'grad_norm': 0.00690583186224103, 'learning_rate': 7.906137184115523e-06, 'epoch': 2.6}
対照学習損失: 3.268010914325714e-05
対照学習損失: 2.8189431759528816e-05
{'loss': 0.0, 'grad_norm': 0.0026935560163110495, 'learning_rate': 7.888086642599279e-06, 'epoch': 2.6}
対照学習損失: 2.082363425870426e-05
対照学習損失: 2.8389549697749317e-05
{'loss': 0.0, 'grad_norm': 0.0009369515464641154, 'learning_rate': 7.870036101083033e-06, 'epoch': 2.6}
対照学習損失: 0.0009607132524251938
対照学習損失: 7.606967301398981e-06
{'loss': 0.0005, 'grad_norm': 0.02846653014421463, 'learning_rate': 7.851985559566788e-06, 'epoch': 2.6}
対照学習損失: 5.871034773008432e-06
対照学習損失: 6.290083547355607e-05
{'loss': 0.0, 'grad_norm': 0.0016729800263419747, 'learning_rate': 7.833935018050542e-06, 'epoch': 2.6}
対照学習損失: 5.924091601627879e-05
対照学習損失: 1.0289026249665767e-05
{'loss': 0.0, 'grad_norm': 0.0011001320090144873, 'learning_rate': 7.815884476534296e-06, 'epoch': 2.6}
対照学習損失: 0.006533816456794739
対照学習損失: 1.2494409929786343e-05
{'loss': 0.0033, 'grad_norm': 0.1356765180826187, 'learning_rate': 7.797833935018051e-06, 'epoch': 2.6}
対照学習損失: 1.3209546523285098e-05
対照学習損失: 2.5278059183619916e-05
{'loss': 0.0, 'grad_norm': 0.0006044207257218659, 'learning_rate': 7.779783393501805e-06, 'epoch': 2.6}
対照学習損失: 5.13339500685106e-06
対照学習損失: 0.0009199639316648245
{'loss': 0.0005, 'grad_norm': 0.020924976095557213, 'learning_rate': 7.76173285198556e-06, 'epoch': 2.61}
対照学習損失: 7.186127913882956e-05
対照学習損失: 9.380157280247658e-06
{'loss': 0.0, 'grad_norm': 0.0016783932223916054, 'learning_rate': 7.743682310469314e-06, 'epoch': 2.61}
対照学習損失: 9.648243576521054e-06
対照学習損失: 7.498916966142133e-05
{'loss': 0.0, 'grad_norm': 0.0016967944102361798, 'learning_rate': 7.72563176895307e-06, 'epoch': 2.61}
対照学習損失: 4.143088517594151e-05
対照学習損失: 1.997391154873185e-05
{'loss': 0.0, 'grad_norm': 0.0017045316053554416, 'learning_rate': 7.707581227436824e-06, 'epoch': 2.61}
対照学習損失: 1.1607873602770269e-05
対照学習損失: 3.5604709410108626e-05
{'loss': 0.0, 'grad_norm': 0.0005842903628945351, 'learning_rate': 7.689530685920577e-06, 'epoch': 2.61}
対照学習損失: 2.5620391170377843e-05
対照学習損失: 4.20917131123133e-05
{'loss': 0.0, 'grad_norm': 0.0022784105967730284, 'learning_rate': 7.671480144404331e-06, 'epoch': 2.61}
対照学習損失: 5.574705937760882e-05
対照学習損失: 1.0147352440981194e-05
{'loss': 0.0, 'grad_norm': 0.0015680573415011168, 'learning_rate': 7.653429602888087e-06, 'epoch': 2.61}
対照学習損失: 0.0002793726744130254
対照学習損失: 0.00012709408474620432
{'loss': 0.0002, 'grad_norm': 0.009225535206496716, 'learning_rate': 7.635379061371842e-06, 'epoch': 2.61}
対照学習損失: 2.137464252882637e-05
対照学習損失: 4.3821703002322465e-05
{'loss': 0.0, 'grad_norm': 0.0007730450015515089, 'learning_rate': 7.617328519855596e-06, 'epoch': 2.61}
対照学習損失: 0.00010618504893500358
対照学習損失: 1.2337899534031749e-05
{'loss': 0.0001, 'grad_norm': 0.002978435019031167, 'learning_rate': 7.59927797833935e-06, 'epoch': 2.61}
対照学習損失: 1.74933738890104e-05
対照学習損失: 5.298279211274348e-05
{'loss': 0.0, 'grad_norm': 0.0034961991477757692, 'learning_rate': 7.581227436823105e-06, 'epoch': 2.61}
対照学習損失: 5.796505320176948e-06
対照学習損失: 6.787421170884045e-06
{'loss': 0.0, 'grad_norm': 0.00024233137082774192, 'learning_rate': 7.56317689530686e-06, 'epoch': 2.62}
対照学習損失: 7.899559568613768e-05
対照学習損失: 0.00017715661670081317
{'loss': 0.0001, 'grad_norm': 0.012402268126606941, 'learning_rate': 7.545126353790615e-06, 'epoch': 2.62}
対照学習損失: 8.433951734332368e-06
対照学習損失: 3.863512029056437e-05
{'loss': 0.0, 'grad_norm': 0.0007441723137162626, 'learning_rate': 7.527075812274369e-06, 'epoch': 2.62}
対照学習損失: 1.4342074791784398e-05
対照学習損失: 3.3753989555407315e-05
{'loss': 0.0, 'grad_norm': 0.002970999339595437, 'learning_rate': 7.509025270758123e-06, 'epoch': 2.62}
対照学習損失: 5.058964597992599e-05
対照学習損失: 9.998488167184405e-06
{'loss': 0.0, 'grad_norm': 0.004251754377037287, 'learning_rate': 7.490974729241877e-06, 'epoch': 2.62}
対照学習損失: 2.2805352273280732e-05
対照学習損失: 8.539637929061428e-05
{'loss': 0.0001, 'grad_norm': 0.002324888715520501, 'learning_rate': 7.472924187725632e-06, 'epoch': 2.62}
対照学習損失: 8.687045919941738e-06
対照学習損失: 9.909128493745811e-06
{'loss': 0.0, 'grad_norm': 0.00024132498947437853, 'learning_rate': 7.454873646209387e-06, 'epoch': 2.62}
対照学習損失: 1.8014856323134154e-05
対照学習損失: 1.0222004675597418e-05
{'loss': 0.0, 'grad_norm': 0.00038606583257205784, 'learning_rate': 7.436823104693141e-06, 'epoch': 2.62}
対照学習損失: 7.260408165166155e-05
対照学習損失: 4.214591172058135e-05
{'loss': 0.0001, 'grad_norm': 0.002256290754303336, 'learning_rate': 7.418772563176896e-06, 'epoch': 2.62}
対照学習損失: 6.669729918939993e-05
対照学習損失: 0.00046341188135556877
{'loss': 0.0003, 'grad_norm': 0.03147472068667412, 'learning_rate': 7.4007220216606496e-06, 'epoch': 2.62}
対照学習損失: 1.4826126061961986e-05
対照学習損失: 6.296059291344136e-05
{'loss': 0.0, 'grad_norm': 0.0014338535256683826, 'learning_rate': 7.382671480144404e-06, 'epoch': 2.62}
対照学習損失: 4.5634260459337384e-05
対照学習損失: 4.880107553617563e-06
{'loss': 0.0, 'grad_norm': 0.0011081021511927247, 'learning_rate': 7.36462093862816e-06, 'epoch': 2.63}
対照学習損失: 2.079384466924239e-05
対照学習損失: 1.1898413504241034e-05
{'loss': 0.0, 'grad_norm': 0.0003723464033100754, 'learning_rate': 7.346570397111914e-06, 'epoch': 2.63}
対照学習損失: 1.5370034816442057e-05
対照学習損失: 2.8764370654243976e-05
{'loss': 0.0, 'grad_norm': 0.000819987675640732, 'learning_rate': 7.328519855595668e-06, 'epoch': 2.63}
対照学習損失: 1.1458869266789407e-05
対照学習損失: 5.473280543810688e-05
{'loss': 0.0, 'grad_norm': 0.0010345058981329203, 'learning_rate': 7.310469314079423e-06, 'epoch': 2.63}
対照学習損失: 0.000320466875564307
対照学習損失: 2.130063876393251e-05
{'loss': 0.0002, 'grad_norm': 0.00833070743829012, 'learning_rate': 7.292418772563177e-06, 'epoch': 2.63}
対照学習損失: 3.5975990613223985e-05
対照学習損失: 1.1026737411157228e-05
{'loss': 0.0, 'grad_norm': 0.0016965511022135615, 'learning_rate': 7.274368231046932e-06, 'epoch': 2.63}
対照学習損失: 1.4773708244320005e-05
対照学習損失: 0.0001853782159741968
{'loss': 0.0001, 'grad_norm': 0.005466966424137354, 'learning_rate': 7.256317689530687e-06, 'epoch': 2.63}
対照学習損失: 1.4692159311380237e-05
対照学習損失: 4.1770912503125146e-05
{'loss': 0.0, 'grad_norm': 0.0009157946333289146, 'learning_rate': 7.238267148014441e-06, 'epoch': 2.63}
対照学習損失: 1.0467786523804534e-05
対照学習損失: 0.0001334284315817058
{'loss': 0.0001, 'grad_norm': 0.0034979302436113358, 'learning_rate': 7.220216606498195e-06, 'epoch': 2.63}
対照学習損失: 6.486522033810616e-05
対照学習損失: 4.208302198094316e-05
{'loss': 0.0001, 'grad_norm': 0.0019370688823983073, 'learning_rate': 7.202166064981949e-06, 'epoch': 2.63}
対照学習損失: 1.986278766707983e-05
対照学習損失: 6.414907147700433e-06
{'loss': 0.0, 'grad_norm': 0.00028504079091362655, 'learning_rate': 7.184115523465704e-06, 'epoch': 2.63}
対照学習損失: 7.5771508818434086e-06
対照学習損失: 2.6149988116230816e-05
{'loss': 0.0, 'grad_norm': 0.0004822607443202287, 'learning_rate': 7.166064981949459e-06, 'epoch': 2.64}
対照学習損失: 8.561814320273697e-05
対照学習損失: 0.0012271883897483349
{'loss': 0.0007, 'grad_norm': 0.04810585081577301, 'learning_rate': 7.148014440433214e-06, 'epoch': 2.64}
対照学習損失: 3.6854911741102114e-05
対照学習損失: 0.00021095143165439367
{'loss': 0.0001, 'grad_norm': 0.0048688724637031555, 'learning_rate': 7.129963898916968e-06, 'epoch': 2.64}
対照学習損失: 1.9682700440171175e-05
対照学習損失: 0.0003172551223542541
{'loss': 0.0002, 'grad_norm': 0.00656962301582098, 'learning_rate': 7.1119133574007225e-06, 'epoch': 2.64}
対照学習損失: 1.475181124988012e-05
対照学習損失: 0.00027547910576686263
{'loss': 0.0001, 'grad_norm': 0.010418212972581387, 'learning_rate': 7.093862815884476e-06, 'epoch': 2.64}
対照学習損失: 4.359392914921045e-05
対照学習損失: 0.000424819765612483
{'loss': 0.0002, 'grad_norm': 0.014357078820466995, 'learning_rate': 7.075812274368232e-06, 'epoch': 2.64}
対照学習損失: 2.9280683975230204e-06
対照学習損失: 0.00010040783672593534
{'loss': 0.0001, 'grad_norm': 0.003686733776703477, 'learning_rate': 7.0577617328519865e-06, 'epoch': 2.64}
対照学習損失: 8.734998846193776e-05
対照学習損失: 1.1384167009964585e-05
{'loss': 0.0, 'grad_norm': 0.002269790042191744, 'learning_rate': 7.03971119133574e-06, 'epoch': 2.64}
対照学習損失: 6.5787653511506505e-06
対照学習損失: 9.372635759063996e-06
{'loss': 0.0, 'grad_norm': 0.0002625638444442302, 'learning_rate': 7.021660649819495e-06, 'epoch': 2.64}
対照学習損失: 0.00017431082960683852
対照学習損失: 8.11361132946331e-06
{'loss': 0.0001, 'grad_norm': 0.012681144289672375, 'learning_rate': 7.003610108303249e-06, 'epoch': 2.64}
対照学習損失: 6.631040014326572e-05
対照学習損失: 5.8193203585688025e-05
{'loss': 0.0001, 'grad_norm': 0.002137207193300128, 'learning_rate': 6.9855595667870035e-06, 'epoch': 2.64}
対照学習損失: 2.1866591850994155e-05
対照学習損失: 0.001266344217583537
{'loss': 0.0006, 'grad_norm': 0.07075292617082596, 'learning_rate': 6.967509025270759e-06, 'epoch': 2.65}
対照学習損失: 5.1536084356484935e-05
対照学習損失: 0.00010583121911622584
{'loss': 0.0001, 'grad_norm': 0.0030229338444769382, 'learning_rate': 6.949458483754513e-06, 'epoch': 2.65}
対照学習損失: 1.97729477804387e-05
対照学習損失: 2.752103864622768e-05
{'loss': 0.0, 'grad_norm': 0.0006086891517043114, 'learning_rate': 6.9314079422382674e-06, 'epoch': 2.65}
対照学習損失: 2.0376090105855837e-05
対照学習損失: 0.000212105514947325
{'loss': 0.0001, 'grad_norm': 0.007450920529663563, 'learning_rate': 6.913357400722022e-06, 'epoch': 2.65}
対照学習損失: 3.647942503448576e-05
対照学習損失: 4.5703935029450804e-05
{'loss': 0.0, 'grad_norm': 0.002800572430714965, 'learning_rate': 6.895306859205776e-06, 'epoch': 2.65}
対照学習損失: 6.176668830448762e-05
対照学習損失: 2.7221753043704666e-05
{'loss': 0.0, 'grad_norm': 0.002768733771517873, 'learning_rate': 6.8772563176895314e-06, 'epoch': 2.65}
対照学習損失: 1.5727633581263945e-05
対照学習損失: 5.4537918003916275e-06
{'loss': 0.0, 'grad_norm': 0.00028509157709777355, 'learning_rate': 6.859205776173286e-06, 'epoch': 2.65}
対照学習損失: 1.058711131918244e-05
対照学習損失: 0.0001393230922985822
{'loss': 0.0001, 'grad_norm': 0.0038542146794497967, 'learning_rate': 6.84115523465704e-06, 'epoch': 2.65}
対照学習損失: 1.9757329937419854e-05
対照学習損失: 8.070539479376748e-05
{'loss': 0.0001, 'grad_norm': 0.002553623402491212, 'learning_rate': 6.823104693140795e-06, 'epoch': 2.65}
対照学習損失: 4.244723095325753e-05
対照学習損失: 7.481951615773141e-05
{'loss': 0.0001, 'grad_norm': 0.00742107629776001, 'learning_rate': 6.805054151624548e-06, 'epoch': 2.65}
対照学習損失: 3.352749672558275e-06
対照学習損失: 0.00026255863485857844
{'loss': 0.0001, 'grad_norm': 0.006502439733594656, 'learning_rate': 6.787003610108304e-06, 'epoch': 2.65}
対照学習損失: 5.667972072842531e-05
対照学習損失: 2.422074248897843e-05
{'loss': 0.0, 'grad_norm': 0.0018616330344229937, 'learning_rate': 6.768953068592059e-06, 'epoch': 2.66}
対照学習損失: 9.625940037949476e-06
対照学習損失: 2.6849325877265073e-05
{'loss': 0.0, 'grad_norm': 0.0007116618799045682, 'learning_rate': 6.750902527075812e-06, 'epoch': 2.66}
対照学習損失: 2.941058664873708e-05
対照学習損失: 0.00010538398782955483
{'loss': 0.0001, 'grad_norm': 0.012054217979311943, 'learning_rate': 6.732851985559567e-06, 'epoch': 2.66}
対照学習損失: 0.00011993148655164987
対照学習損失: 7.033272595435847e-06
{'loss': 0.0001, 'grad_norm': 0.005497440695762634, 'learning_rate': 6.714801444043321e-06, 'epoch': 2.66}
対照学習損失: 9.132853301707655e-05
対照学習損失: 1.554146729176864e-05
{'loss': 0.0001, 'grad_norm': 0.005183422937989235, 'learning_rate': 6.6967509025270755e-06, 'epoch': 2.66}
対照学習損失: 9.787173621589318e-05
対照学習損失: 1.1279931641183794e-05
{'loss': 0.0001, 'grad_norm': 0.003879798110574484, 'learning_rate': 6.678700361010831e-06, 'epoch': 2.66}
対照学習損失: 0.00032142980489879847
対照学習損失: 2.7207244784221984e-05
{'loss': 0.0002, 'grad_norm': 0.010173024609684944, 'learning_rate': 6.660649819494586e-06, 'epoch': 2.66}
対照学習損失: 0.0004502740630414337
対照学習損失: 6.28466805210337e-05
{'loss': 0.0003, 'grad_norm': 0.01373522449284792, 'learning_rate': 6.6425992779783395e-06, 'epoch': 2.66}
対照学習損失: 1.787334076652769e-05
対照学習損失: 1.3582191968453117e-05
{'loss': 0.0, 'grad_norm': 0.0003451392112765461, 'learning_rate': 6.624548736462094e-06, 'epoch': 2.66}
対照学習損失: 0.00017599436978343874
対照学習損失: 2.093534749292303e-05
{'loss': 0.0001, 'grad_norm': 0.005692583974450827, 'learning_rate': 6.606498194945848e-06, 'epoch': 2.66}
対照学習損失: 9.976183719118126e-06
対照学習損失: 8.292367056128569e-06
{'loss': 0.0, 'grad_norm': 0.00028881881735287607, 'learning_rate': 6.5884476534296035e-06, 'epoch': 2.66}
対照学習損失: 9.004184539662674e-05
対照学習損失: 4.999313205189537e-06
{'loss': 0.0, 'grad_norm': 0.002110186731442809, 'learning_rate': 6.570397111913358e-06, 'epoch': 2.67}
対照学習損失: 3.3029242331394926e-05
対照学習損失: 9.506860806141049e-06
{'loss': 0.0, 'grad_norm': 0.0007028638501651585, 'learning_rate': 6.552346570397112e-06, 'epoch': 2.67}
対照学習損失: 1.4237504728953354e-05
対照学習損失: 0.002388490131124854
{'loss': 0.0012, 'grad_norm': 0.04795381426811218, 'learning_rate': 6.534296028880867e-06, 'epoch': 2.67}
対照学習損失: 1.8252796508022584e-05
対照学習損失: 2.3937205696711317e-05
{'loss': 0.0, 'grad_norm': 0.000589297735132277, 'learning_rate': 6.5162454873646205e-06, 'epoch': 2.67}
対照学習損失: 2.4146198484231718e-05
対照学習損失: 2.9569668186013587e-05
{'loss': 0.0, 'grad_norm': 0.0007157258223742247, 'learning_rate': 6.498194945848375e-06, 'epoch': 2.67}
対照学習損失: 1.7121044947998598e-05
対照学習損失: 1.4520840522891376e-05
{'loss': 0.0, 'grad_norm': 0.00039061374263837934, 'learning_rate': 6.480144404332131e-06, 'epoch': 2.67}
対照学習損失: 3.5482509701978415e-05
対照学習損失: 7.31638829165604e-06
{'loss': 0.0, 'grad_norm': 0.0009398797410540283, 'learning_rate': 6.462093862815885e-06, 'epoch': 2.67}
対照学習損失: 0.0004226616001687944
対照学習損失: 0.002182398224249482
{'loss': 0.0013, 'grad_norm': 0.05725390091538429, 'learning_rate': 6.444043321299639e-06, 'epoch': 2.67}
対照学習損失: 4.446457023732364e-05
対照学習損失: 2.045129622274544e-05
{'loss': 0.0, 'grad_norm': 0.0010196482762694359, 'learning_rate': 6.425992779783394e-06, 'epoch': 2.67}
対照学習損失: 0.0002825301489792764
対照学習損失: 0.0010316252009943128
{'loss': 0.0007, 'grad_norm': 0.07537897676229477, 'learning_rate': 6.407942238267148e-06, 'epoch': 2.67}
対照学習損失: 2.7194560061616357e-06
対照学習損失: 1.2322872862569056e-05
{'loss': 0.0, 'grad_norm': 0.00020132699864916503, 'learning_rate': 6.389891696750903e-06, 'epoch': 2.67}
対照学習損失: 1.2419777704053558e-05
対照学習損失: 6.42234408587683e-06
{'loss': 0.0, 'grad_norm': 0.00024192521232180297, 'learning_rate': 6.371841155234658e-06, 'epoch': 2.68}
対照学習損失: 3.181233842042275e-05
対照学習損失: 4.7721798182465136e-05
{'loss': 0.0, 'grad_norm': 0.0013691228814423084, 'learning_rate': 6.353790613718412e-06, 'epoch': 2.68}
対照学習損失: 3.88357657357119e-05
対照学習損失: 1.572049586684443e-05
{'loss': 0.0, 'grad_norm': 0.0007143961847759783, 'learning_rate': 6.335740072202166e-06, 'epoch': 2.68}
対照学習損失: 1.9825431081699207e-05
対照学習損失: 5.9003097703680396e-05
{'loss': 0.0, 'grad_norm': 0.0013286677422001958, 'learning_rate': 6.31768953068592e-06, 'epoch': 2.68}
対照学習損失: 5.4239962992141955e-06
対照学習損失: 1.6375925042666495e-05
{'loss': 0.0, 'grad_norm': 0.0002869909512810409, 'learning_rate': 6.299638989169675e-06, 'epoch': 2.68}
対照学習損失: 8.47108822199516e-06
対照学習損失: 0.0004173276829533279
{'loss': 0.0002, 'grad_norm': 0.025468893349170685, 'learning_rate': 6.28158844765343e-06, 'epoch': 2.68}
対照学習損失: 1.562353099870961e-05
対照学習損失: 2.22242197196465e-05
{'loss': 0.0, 'grad_norm': 0.00041396301821805537, 'learning_rate': 6.263537906137185e-06, 'epoch': 2.68}
対照学習損失: 5.672492989106104e-05
対照学習損失: 5.409597360994667e-05
{'loss': 0.0001, 'grad_norm': 0.0016403571935370564, 'learning_rate': 6.245487364620939e-06, 'epoch': 2.68}
対照学習損失: 1.7254677004530095e-05
対照学習損失: 1.2889295248896815e-05
{'loss': 0.0, 'grad_norm': 0.0009547375375404954, 'learning_rate': 6.227436823104693e-06, 'epoch': 2.68}
対照学習損失: 5.3007817768957466e-05
対照学習損失: 1.4081339031690732e-05
{'loss': 0.0, 'grad_norm': 0.0015171122504398227, 'learning_rate': 6.209386281588448e-06, 'epoch': 2.68}
対照学習損失: 2.4651892090332694e-05
対照学習損失: 4.537385393632576e-06
{'loss': 0.0, 'grad_norm': 0.0006978390156291425, 'learning_rate': 6.191335740072203e-06, 'epoch': 2.69}
対照学習損失: 7.397659646812826e-05
対照学習損失: 5.78282488277182e-05
{'loss': 0.0001, 'grad_norm': 0.0030321672093123198, 'learning_rate': 6.173285198555957e-06, 'epoch': 2.69}
対照学習損失: 4.991861260350561e-06
対照学習損失: 0.00019934450392611325
{'loss': 0.0001, 'grad_norm': 0.004551758524030447, 'learning_rate': 6.155234657039711e-06, 'epoch': 2.69}
対照学習損失: 0.00012035715189995244
対照学習損失: 2.0480549210333265e-05
{'loss': 0.0001, 'grad_norm': 0.0025635745842009783, 'learning_rate': 6.137184115523466e-06, 'epoch': 2.69}
対照学習損失: 3.7324785807868466e-05
対照学習損失: 8.062208507908508e-05
{'loss': 0.0001, 'grad_norm': 0.0019485761877149343, 'learning_rate': 6.1191335740072206e-06, 'epoch': 2.69}
対照学習損失: 5.562661317526363e-05
対照学習損失: 5.9029880503658205e-05
{'loss': 0.0001, 'grad_norm': 0.00481815543025732, 'learning_rate': 6.101083032490975e-06, 'epoch': 2.69}
対照学習損失: 4.3372554500820115e-05
対照学習損失: 0.00011741896742023528
{'loss': 0.0001, 'grad_norm': 0.0039007957093417645, 'learning_rate': 6.083032490974729e-06, 'epoch': 2.69}
対照学習損失: 7.383484899037285e-06
対照学習損失: 2.131523797288537e-05
{'loss': 0.0, 'grad_norm': 0.00043106707744300365, 'learning_rate': 6.0649819494584845e-06, 'epoch': 2.69}
対照学習損失: 2.9129045287845656e-05
対照学習損失: 4.2637249862309545e-05
{'loss': 0.0, 'grad_norm': 0.0009185413364320993, 'learning_rate': 6.046931407942238e-06, 'epoch': 2.69}
対照学習損失: 2.1098785509821028e-05
対照学習損失: 8.404089385294355e-06
{'loss': 0.0, 'grad_norm': 0.000952943810261786, 'learning_rate': 6.028880866425993e-06, 'epoch': 2.69}
対照学習損失: 0.00023334955039899796
対照学習損失: 2.5025687136803754e-05
{'loss': 0.0001, 'grad_norm': 0.012400471605360508, 'learning_rate': 6.010830324909748e-06, 'epoch': 2.69}
対照学習損失: 2.346841210965067e-05
対照学習損失: 7.763384928693995e-06
{'loss': 0.0, 'grad_norm': 0.000450280582299456, 'learning_rate': 5.9927797833935015e-06, 'epoch': 2.7}
対照学習損失: 1.2941409295308404e-05
対照学習損失: 3.7103768590895925e-06
{'loss': 0.0, 'grad_norm': 0.0002737817703746259, 'learning_rate': 5.974729241877257e-06, 'epoch': 2.7}
対照学習損失: 0.0005849342560395598
対照学習損失: 0.0002051689225481823
{'loss': 0.0004, 'grad_norm': 0.02319316193461418, 'learning_rate': 5.956678700361011e-06, 'epoch': 2.7}
対照学習損失: 6.371834024321288e-05
対照学習損失: 2.4048124032560736e-05
{'loss': 0.0, 'grad_norm': 0.0019375570118427277, 'learning_rate': 5.9386281588447655e-06, 'epoch': 2.7}
対照学習損失: 8.56060523801716e-06
対照学習損失: 1.6286378013319336e-05
{'loss': 0.0, 'grad_norm': 0.0003140704066026956, 'learning_rate': 5.92057761732852e-06, 'epoch': 2.7}
対照学習損失: 1.8446855392539874e-05
対照学習損失: 3.976750303991139e-05
{'loss': 0.0, 'grad_norm': 0.0016651476034894586, 'learning_rate': 5.902527075812275e-06, 'epoch': 2.7}
対照学習損失: 6.140267214505002e-05
対照学習損失: 2.1732481400249526e-05
{'loss': 0.0, 'grad_norm': 0.003553332295268774, 'learning_rate': 5.884476534296029e-06, 'epoch': 2.7}
対照学習損失: 2.7490943466546014e-05
対照学習損失: 0.00023096430231817067
{'loss': 0.0001, 'grad_norm': 0.007256597746163607, 'learning_rate': 5.866425992779784e-06, 'epoch': 2.7}
対照学習損失: 0.00498206727206707
対照学習損失: 1.1779132364608813e-05
{'loss': 0.0025, 'grad_norm': 0.1105637326836586, 'learning_rate': 5.848375451263538e-06, 'epoch': 2.7}
対照学習損失: 5.625154244626174e-06
対照学習損失: 1.7299624232691713e-05
{'loss': 0.0, 'grad_norm': 0.00046112266136333346, 'learning_rate': 5.830324909747293e-06, 'epoch': 2.7}
対照学習損失: 2.0123123249504715e-05
対照学習損失: 2.014590791077353e-05
{'loss': 0.0, 'grad_norm': 0.0011524379951879382, 'learning_rate': 5.812274368231047e-06, 'epoch': 2.7}
対照学習損失: 2.3490496459999122e-05
対照学習損失: 0.00010177645162912086
{'loss': 0.0001, 'grad_norm': 0.010656118392944336, 'learning_rate': 5.794223826714801e-06, 'epoch': 2.71}
対照学習損失: 1.70014682225883e-05
対照学習損失: 1.2755201169056818e-05
{'loss': 0.0, 'grad_norm': 0.00040058238664641976, 'learning_rate': 5.776173285198557e-06, 'epoch': 2.71}
対照学習損失: 7.085416200425243e-06
対照学習損失: 0.00013550980656873435
{'loss': 0.0001, 'grad_norm': 0.008093365468084812, 'learning_rate': 5.7581227436823104e-06, 'epoch': 2.71}
対照学習損失: 9.60807446972467e-05
対照学習損失: 2.6276931748725474e-05
{'loss': 0.0001, 'grad_norm': 0.003539507510140538, 'learning_rate': 5.740072202166065e-06, 'epoch': 2.71}
対照学習損失: 6.829317862866446e-05
対照学習損失: 1.823098136810586e-05
{'loss': 0.0, 'grad_norm': 0.0023083421401679516, 'learning_rate': 5.72202166064982e-06, 'epoch': 2.71}
対照学習損失: 1.0542266863922123e-05
対照学習損失: 0.0002874612982850522
{'loss': 0.0001, 'grad_norm': 0.006599568296223879, 'learning_rate': 5.7039711191335744e-06, 'epoch': 2.71}
対照学習損失: 0.0020654371473938227
対照学習損失: 0.00016459148901049048
{'loss': 0.0011, 'grad_norm': 0.06599923223257065, 'learning_rate': 5.685920577617329e-06, 'epoch': 2.71}
対照学習損失: 1.1823929526144639e-05
対照学習損失: 2.2924599761608988e-05
{'loss': 0.0, 'grad_norm': 0.0006255079060792923, 'learning_rate': 5.667870036101084e-06, 'epoch': 2.71}
対照学習損失: 0.00038418674375861883
対照学習損失: 1.55338548211148e-05
{'loss': 0.0002, 'grad_norm': 0.009039966389536858, 'learning_rate': 5.649819494584838e-06, 'epoch': 2.71}
対照学習損失: 4.732679371954873e-05
対照学習損失: 0.00010216651571681723
{'loss': 0.0001, 'grad_norm': 0.0027869408950209618, 'learning_rate': 5.631768953068592e-06, 'epoch': 2.71}
対照学習損失: 3.4966789826285094e-05
対照学習損失: 3.608701081248e-05
{'loss': 0.0, 'grad_norm': 0.0013392408145591617, 'learning_rate': 5.613718411552347e-06, 'epoch': 2.71}
対照学習損失: 6.367416062857956e-05
対照学習損失: 5.7890729294740595e-06
{'loss': 0.0, 'grad_norm': 0.0019596219062805176, 'learning_rate': 5.595667870036101e-06, 'epoch': 2.72}
対照学習損失: 0.00025422091130167246
対照学習損失: 5.545514795812778e-05
{'loss': 0.0002, 'grad_norm': 0.007829276844859123, 'learning_rate': 5.577617328519856e-06, 'epoch': 2.72}
対照学習損失: 6.037427738192491e-05
対照学習損失: 4.717899355455302e-05
{'loss': 0.0001, 'grad_norm': 0.004559839144349098, 'learning_rate': 5.55956678700361e-06, 'epoch': 2.72}
対照学習損失: 0.00016066354874055833
対照学習損失: 7.911711873020977e-05
{'loss': 0.0001, 'grad_norm': 0.0054194931872189045, 'learning_rate': 5.5415162454873656e-06, 'epoch': 2.72}
対照学習損失: 0.0012563219061121345
対照学習損失: 9.66312381933676e-06
{'loss': 0.0006, 'grad_norm': 0.03495953232049942, 'learning_rate': 5.523465703971119e-06, 'epoch': 2.72}
対照学習損失: 2.1590922187897377e-05
対照学習損失: 7.495181307604071e-06
{'loss': 0.0, 'grad_norm': 0.0004068537673447281, 'learning_rate': 5.505415162454874e-06, 'epoch': 2.72}
対照学習損失: 0.0012335475767031312
対照学習損失: 0.0002817896893247962
{'loss': 0.0008, 'grad_norm': 0.04287761449813843, 'learning_rate': 5.487364620938629e-06, 'epoch': 2.72}
対照学習損失: 1.11905719677452e-05
対照学習損失: 6.779940122214612e-06
{'loss': 0.0, 'grad_norm': 0.00020321623014751822, 'learning_rate': 5.4693140794223825e-06, 'epoch': 2.72}
対照学習損失: 1.0460462362971157e-05
対照学習損失: 5.6722696172073483e-05
{'loss': 0.0, 'grad_norm': 0.0009943246841430664, 'learning_rate': 5.451263537906137e-06, 'epoch': 2.72}
対照学習損失: 0.00023438739299308509
対照学習損失: 8.567544864490628e-05
{'loss': 0.0002, 'grad_norm': 0.005753211211413145, 'learning_rate': 5.433212996389892e-06, 'epoch': 2.72}
対照学習損失: 3.21520165016409e-05
対照学習損失: 0.00025672902120277286
{'loss': 0.0001, 'grad_norm': 0.006490444764494896, 'learning_rate': 5.4151624548736465e-06, 'epoch': 2.72}
対照学習損失: 5.599110954790376e-05
対照学習損失: 4.416343290358782e-05
{'loss': 0.0001, 'grad_norm': 0.0015328541630879045, 'learning_rate': 5.3971119133574e-06, 'epoch': 2.73}
対照学習損失: 1.732215423544403e-05
対照学習損失: 0.0002503689029254019
{'loss': 0.0001, 'grad_norm': 0.007651896215975285, 'learning_rate': 5.379061371841156e-06, 'epoch': 2.73}
対照学習損失: 2.0458437575143762e-05
対照学習損失: 9.782519555301405e-06
{'loss': 0.0, 'grad_norm': 0.0003792153438553214, 'learning_rate': 5.36101083032491e-06, 'epoch': 2.73}
対照学習損失: 1.9176983187207952e-05
対照学習損失: 0.000287578790448606
{'loss': 0.0002, 'grad_norm': 0.009229060262441635, 'learning_rate': 5.342960288808665e-06, 'epoch': 2.73}
対照学習損失: 1.2501484889071435e-05
対照学習損失: 1.5884172171354294e-05
{'loss': 0.0, 'grad_norm': 0.0004113799368496984, 'learning_rate': 5.324909747292419e-06, 'epoch': 2.73}
対照学習損失: 6.325489721348276e-06
対照学習損失: 2.267132185806986e-05
{'loss': 0.0, 'grad_norm': 0.0003287143772467971, 'learning_rate': 5.306859205776174e-06, 'epoch': 2.73}
対照学習損失: 1.7381575162289664e-05
対照学習損失: 5.1624949264805764e-05
{'loss': 0.0, 'grad_norm': 0.0013308635680004954, 'learning_rate': 5.288808664259928e-06, 'epoch': 2.73}
対照学習損失: 3.438895146246068e-05
対照学習損失: 2.346042856515851e-05
{'loss': 0.0, 'grad_norm': 0.0010842251358553767, 'learning_rate': 5.270758122743682e-06, 'epoch': 2.73}
対照学習損失: 8.955516022979282e-06
対照学習損失: 6.139239758340409e-06
{'loss': 0.0, 'grad_norm': 0.00020581771968863904, 'learning_rate': 5.252707581227437e-06, 'epoch': 2.73}
対照学習損失: 1.3418027265288401e-05
対照学習損失: 2.0629362552426755e-05
{'loss': 0.0, 'grad_norm': 0.0011505450820550323, 'learning_rate': 5.2346570397111915e-06, 'epoch': 2.73}
対照学習損失: 4.550653829937801e-05
対照学習損失: 1.3939253221906256e-05
{'loss': 0.0, 'grad_norm': 0.0027998280711472034, 'learning_rate': 5.216606498194946e-06, 'epoch': 2.73}
対照学習損失: 6.884269168949686e-06
対照学習損失: 1.4453911717282608e-05
{'loss': 0.0, 'grad_norm': 0.00023427717678714544, 'learning_rate': 5.198555956678701e-06, 'epoch': 2.74}
対照学習損失: 0.00022893043933436275
対照学習損失: 0.00013582194515038282
{'loss': 0.0002, 'grad_norm': 0.008966493420302868, 'learning_rate': 5.1805054151624555e-06, 'epoch': 2.74}
対照学習損失: 7.405798896797933e-06
対照学習損失: 2.815122570609674e-05
{'loss': 0.0, 'grad_norm': 0.0007844596402719617, 'learning_rate': 5.162454873646209e-06, 'epoch': 2.74}
対照学習損失: 2.8995034881518222e-05
対照学習損失: 9.234040044248104e-05
{'loss': 0.0001, 'grad_norm': 0.002859783824533224, 'learning_rate': 5.144404332129965e-06, 'epoch': 2.74}
対照学習損失: 2.3542786948382854e-05
対照学習損失: 0.0011391312582418323
{'loss': 0.0006, 'grad_norm': 0.0494404211640358, 'learning_rate': 5.126353790613719e-06, 'epoch': 2.74}
対照学習損失: 1.5176006854744628e-05
対照学習損失: 5.707426316803321e-05
{'loss': 0.0, 'grad_norm': 0.001513456809334457, 'learning_rate': 5.108303249097473e-06, 'epoch': 2.74}
対照学習損失: 6.263551767915487e-05
対照学習損失: 4.573488695314154e-05
{'loss': 0.0001, 'grad_norm': 0.0017467543948441744, 'learning_rate': 5.090252707581228e-06, 'epoch': 2.74}
対照学習損失: 6.355302957672393e-06
対照学習損失: 5.122418951941654e-05
{'loss': 0.0, 'grad_norm': 0.0009706538403406739, 'learning_rate': 5.072202166064982e-06, 'epoch': 2.74}
対照学習損失: 5.292976857163012e-05
対照学習損失: 7.428140634146985e-06
{'loss': 0.0, 'grad_norm': 0.0013395356945693493, 'learning_rate': 5.054151624548736e-06, 'epoch': 2.74}
対照学習損失: 6.202217628015205e-05
対照学習損失: 1.595136745891068e-05
{'loss': 0.0, 'grad_norm': 0.004210571758449078, 'learning_rate': 5.036101083032491e-06, 'epoch': 2.74}
対照学習損失: 0.0006355372024700046
対照学習損失: 0.00011577800614759326
{'loss': 0.0004, 'grad_norm': 0.014433112926781178, 'learning_rate': 5.018050541516246e-06, 'epoch': 2.74}
対照学習損失: 1.9087827240582556e-05
対照学習損失: 7.69636517361505e-06
{'loss': 0.0, 'grad_norm': 0.0003852503141388297, 'learning_rate': 5e-06, 'epoch': 2.75}
対照学習損失: 0.00014542693679686636
対照学習損失: 3.2782447760837385e-06
{'loss': 0.0001, 'grad_norm': 0.0027647772803902626, 'learning_rate': 4.981949458483755e-06, 'epoch': 2.75}
対照学習損失: 0.055377453565597534
対照学習損失: 2.726038292166777e-05
{'loss': 0.0277, 'grad_norm': 0.9782900214195251, 'learning_rate': 4.963898916967509e-06, 'epoch': 2.75}
対照学習損失: 0.0012976345606148243
対照学習損失: 1.359666021016892e-05
{'loss': 0.0007, 'grad_norm': 0.04302551969885826, 'learning_rate': 4.9458483754512636e-06, 'epoch': 2.75}
対照学習損失: 2.5926514354068786e-05
対照学習損失: 7.882302452344447e-05
{'loss': 0.0001, 'grad_norm': 0.00200252840295434, 'learning_rate': 4.927797833935018e-06, 'epoch': 2.75}
対照学習損失: 2.4191092961700633e-05
対照学習損失: 6.013087113387883e-05
{'loss': 0.0, 'grad_norm': 0.0017166583565995097, 'learning_rate': 4.909747292418773e-06, 'epoch': 2.75}
対照学習損失: 9.558292367728427e-05
対照学習損失: 0.0002152894448954612
{'loss': 0.0002, 'grad_norm': 0.008944093249738216, 'learning_rate': 4.8916967509025275e-06, 'epoch': 2.75}
対照学習損失: 2.3414400857291184e-05
対照学習損失: 5.4197575082071126e-05
{'loss': 0.0, 'grad_norm': 0.0011183011811226606, 'learning_rate': 4.873646209386281e-06, 'epoch': 2.75}
対照学習損失: 6.565508374478668e-05
対照学習損失: 0.0006125220097601414
{'loss': 0.0003, 'grad_norm': 0.016689978539943695, 'learning_rate': 4.855595667870037e-06, 'epoch': 2.75}
対照学習損失: 7.435594852722716e-06
対照学習損失: 1.5802155758137815e-05
{'loss': 0.0, 'grad_norm': 0.000494480540510267, 'learning_rate': 4.837545126353791e-06, 'epoch': 2.75}
対照学習損失: 6.072161340853199e-06
対照学習損失: 4.511533916229382e-05
{'loss': 0.0, 'grad_norm': 0.0007995208143256605, 'learning_rate': 4.819494584837545e-06, 'epoch': 2.75}
対照学習損失: 0.0009822056163102388
対照学習損失: 8.806474397715647e-06
{'loss': 0.0005, 'grad_norm': 0.03283527120947838, 'learning_rate': 4.8014440433213e-06, 'epoch': 2.76}
対照学習損失: 1.9318646081956103e-05
対照学習損失: 9.901455632643774e-05
{'loss': 0.0001, 'grad_norm': 0.006543061230331659, 'learning_rate': 4.783393501805055e-06, 'epoch': 2.76}
対照学習損失: 1.3984551515022758e-05
対照学習損失: 0.00011719412577804178
{'loss': 0.0001, 'grad_norm': 0.008754451759159565, 'learning_rate': 4.7653429602888085e-06, 'epoch': 2.76}
対照学習損失: 7.0631003836751916e-06
対照学習損失: 4.683489532908425e-05
{'loss': 0.0, 'grad_norm': 0.004490249324589968, 'learning_rate': 4.747292418772563e-06, 'epoch': 2.76}
対照学習損失: 6.903595931362361e-05
対照学習損失: 2.0375888198032044e-05
{'loss': 0.0, 'grad_norm': 0.0021865873131901026, 'learning_rate': 4.729241877256318e-06, 'epoch': 2.76}
対照学習損失: 1.0393402590125334e-05
対照学習損失: 6.116895292507252e-06
{'loss': 0.0, 'grad_norm': 0.00020235370902810246, 'learning_rate': 4.7111913357400725e-06, 'epoch': 2.76}
対照学習損失: 2.9309017918421887e-05
対照学習損失: 4.8317539039999247e-05
{'loss': 0.0, 'grad_norm': 0.0019256514497101307, 'learning_rate': 4.693140794223827e-06, 'epoch': 2.76}
対照学習損失: 6.109416972321924e-06
対照学習損失: 1.0944713721983135e-05
{'loss': 0.0, 'grad_norm': 0.0002101585705531761, 'learning_rate': 4.675090252707581e-06, 'epoch': 2.76}
対照学習損失: 4.0525468648411334e-05
対照学習損失: 2.421326280455105e-05
{'loss': 0.0, 'grad_norm': 0.0012377280509099364, 'learning_rate': 4.6570397111913365e-06, 'epoch': 2.76}
対照学習損失: 1.2069634976796806e-05
対照学習損失: 1.403662099619396e-05
{'loss': 0.0, 'grad_norm': 0.0003194365999661386, 'learning_rate': 4.63898916967509e-06, 'epoch': 2.76}
対照学習損失: 9.361227421322837e-05
対照学習損失: 5.2260744269005954e-05
{'loss': 0.0001, 'grad_norm': 0.0025765197351574898, 'learning_rate': 4.620938628158845e-06, 'epoch': 2.76}
対照学習損失: 1.0408161870145705e-05
対照学習損失: 0.00030029178014956415
{'loss': 0.0002, 'grad_norm': 0.009794835932552814, 'learning_rate': 4.6028880866426e-06, 'epoch': 2.77}
対照学習損失: 0.00032230588840320706
対照学習損失: 0.00034484147909097373
{'loss': 0.0003, 'grad_norm': 0.01912549138069153, 'learning_rate': 4.584837545126354e-06, 'epoch': 2.77}
対照学習損失: 7.433761493302882e-05
対照学習損失: 0.0014832564629614353
{'loss': 0.0008, 'grad_norm': 0.04486098140478134, 'learning_rate': 4.566787003610108e-06, 'epoch': 2.77}
対照学習損失: 1.4140949133434333e-05
対照学習損失: 1.6755993783590384e-05
{'loss': 0.0, 'grad_norm': 0.00039841991383582354, 'learning_rate': 4.548736462093863e-06, 'epoch': 2.77}
対照学習損失: 2.0555358787532896e-05
対照学習損失: 1.6592039173701778e-05
{'loss': 0.0, 'grad_norm': 0.0004873981815762818, 'learning_rate': 4.5306859205776174e-06, 'epoch': 2.77}
対照学習損失: 8.165760846168268e-06
対照学習損失: 4.9375888920621946e-05
{'loss': 0.0, 'grad_norm': 0.0019181497627869248, 'learning_rate': 4.512635379061372e-06, 'epoch': 2.77}
対照学習損失: 1.6621823306195438e-05
対照学習損失: 2.0987219613743946e-05
{'loss': 0.0, 'grad_norm': 0.0008248733356595039, 'learning_rate': 4.494584837545127e-06, 'epoch': 2.77}
対照学習損失: 0.0001031676511047408
対照学習損失: 9.998550012824126e-06
{'loss': 0.0001, 'grad_norm': 0.002827474381774664, 'learning_rate': 4.476534296028881e-06, 'epoch': 2.77}
対照学習損失: 0.00019594204786699265
対照学習損失: 9.600407065590844e-05
{'loss': 0.0001, 'grad_norm': 0.010968494229018688, 'learning_rate': 4.458483754512636e-06, 'epoch': 2.77}
対照学習損失: 6.0200272855581716e-06
対照学習損失: 0.00020593080262187868
{'loss': 0.0001, 'grad_norm': 0.006124864332377911, 'learning_rate': 4.44043321299639e-06, 'epoch': 2.77}
対照学習損失: 1.4654894584964495e-05
対照学習損失: 0.00033632948179729283
{'loss': 0.0002, 'grad_norm': 0.02735038660466671, 'learning_rate': 4.422382671480145e-06, 'epoch': 2.77}
対照学習損失: 2.3080938262864947e-05
対照学習損失: 1.6778110875748098e-05
{'loss': 0.0, 'grad_norm': 0.0005898625240661204, 'learning_rate': 4.404332129963899e-06, 'epoch': 2.78}
対照学習損失: 2.1285193724907003e-05
対照学習損失: 0.0009687574347481132
{'loss': 0.0005, 'grad_norm': 0.033873528242111206, 'learning_rate': 4.386281588447654e-06, 'epoch': 2.78}
対照学習損失: 2.2648238882538863e-05
対照学習損失: 6.538480374729261e-05
{'loss': 0.0, 'grad_norm': 0.0016558935167267919, 'learning_rate': 4.3682310469314086e-06, 'epoch': 2.78}
対照学習損失: 3.863408710458316e-05
対照学習損失: 5.3989475418347865e-05
{'loss': 0.0, 'grad_norm': 0.005334803834557533, 'learning_rate': 4.350180505415162e-06, 'epoch': 2.78}
対照学習損失: 5.366021650843322e-05
対照学習損失: 0.0001294161193072796
{'loss': 0.0001, 'grad_norm': 0.0030386934522539377, 'learning_rate': 4.332129963898917e-06, 'epoch': 2.78}
対照学習損失: 3.236224802094512e-05
対照学習損失: 7.468892727047205e-05
{'loss': 0.0001, 'grad_norm': 0.004731482360512018, 'learning_rate': 4.314079422382672e-06, 'epoch': 2.78}
対照学習損失: 0.000105242186691612
対照学習損失: 6.973699328227667e-06
{'loss': 0.0001, 'grad_norm': 0.002818122273311019, 'learning_rate': 4.296028880866426e-06, 'epoch': 2.78}
対照学習損失: 7.271673894138075e-06
対照学習損失: 0.0006292221369221807
{'loss': 0.0003, 'grad_norm': 0.02178078703582287, 'learning_rate': 4.27797833935018e-06, 'epoch': 2.78}
対照学習損失: 9.223720553563908e-06
対照学習損失: 2.568106538092252e-05
{'loss': 0.0, 'grad_norm': 0.0005650999373756349, 'learning_rate': 4.259927797833936e-06, 'epoch': 2.78}
対照学習損失: 0.0012187778484076262
対照学習損失: 8.366943802684546e-06
{'loss': 0.0006, 'grad_norm': 0.054684579372406006, 'learning_rate': 4.2418772563176895e-06, 'epoch': 2.78}
対照学習損失: 0.0002492665662430227
対照学習損失: 1.045301542035304e-05
{'loss': 0.0001, 'grad_norm': 0.010927975177764893, 'learning_rate': 4.223826714801444e-06, 'epoch': 2.78}
対照学習損失: 7.51173502067104e-05
対照学習損失: 1.4341977475851309e-05
{'loss': 0.0, 'grad_norm': 0.0038573029451072216, 'learning_rate': 4.205776173285199e-06, 'epoch': 2.79}
対照学習損失: 3.069446756853722e-05
対照学習損失: 0.0013147684512659907
{'loss': 0.0007, 'grad_norm': 0.04977034404873848, 'learning_rate': 4.1877256317689535e-06, 'epoch': 2.79}
対照学習損失: 0.00016007249359972775
対照学習損失: 5.163535752217285e-05
{'loss': 0.0001, 'grad_norm': 0.006714529823511839, 'learning_rate': 4.169675090252708e-06, 'epoch': 2.79}
対照学習損失: 1.4684683264931664e-05
対照学習損失: 1.4096182894718368e-05
{'loss': 0.0, 'grad_norm': 0.0006851027137599885, 'learning_rate': 4.151624548736462e-06, 'epoch': 2.79}
対照学習損失: 1.3887534805689938e-05
対照学習損失: 1.4989871488069184e-05
{'loss': 0.0, 'grad_norm': 0.00041067786514759064, 'learning_rate': 4.133574007220217e-06, 'epoch': 2.79}
対照学習損失: 7.212944910861552e-05
対照学習損失: 1.6927035176195204e-05
{'loss': 0.0, 'grad_norm': 0.0024701403453946114, 'learning_rate': 4.115523465703971e-06, 'epoch': 2.79}
対照学習損失: 2.968785702250898e-05
対照学習損失: 2.5195637135766447e-05
{'loss': 0.0, 'grad_norm': 0.0019092262955382466, 'learning_rate': 4.097472924187726e-06, 'epoch': 2.79}
対照学習損失: 1.493063973612152e-05
対照学習損失: 8.345866081072018e-05
{'loss': 0.0, 'grad_norm': 0.0018631231505423784, 'learning_rate': 4.07942238267148e-06, 'epoch': 2.79}
対照学習損失: 0.0003506805223878473
対照学習損失: 5.520819286175538e-06
{'loss': 0.0002, 'grad_norm': 0.03513596951961517, 'learning_rate': 4.061371841155235e-06, 'epoch': 2.79}
対照学習損失: 8.519108087057248e-05
対照学習損失: 1.2092021279386245e-05
{'loss': 0.0, 'grad_norm': 0.0029880739748477936, 'learning_rate': 4.043321299638989e-06, 'epoch': 2.79}
対照学習損失: 1.5444613381987438e-05
対照学習損失: 6.517654401250184e-05
{'loss': 0.0, 'grad_norm': 0.0024548773653805256, 'learning_rate': 4.025270758122744e-06, 'epoch': 2.8}
対照学習損失: 2.8235681384103373e-05
対照学習損失: 7.68894278735388e-06
{'loss': 0.0, 'grad_norm': 0.0005415616324171424, 'learning_rate': 4.0072202166064985e-06, 'epoch': 2.8}
対照学習損失: 1.061681268765824e-05
対照学習損失: 2.511306229280308e-05
{'loss': 0.0, 'grad_norm': 0.0006368367467075586, 'learning_rate': 3.989169675090252e-06, 'epoch': 2.8}
対照学習損失: 1.0482857760507613e-05
対照学習損失: 3.9297545299632475e-05
{'loss': 0.0, 'grad_norm': 0.0008846932323649526, 'learning_rate': 3.971119133574008e-06, 'epoch': 2.8}
対照学習損失: 1.7470927559770644e-05
対照学習損失: 0.0003639615315478295
{'loss': 0.0002, 'grad_norm': 0.00963070522993803, 'learning_rate': 3.953068592057762e-06, 'epoch': 2.8}
対照学習損失: 3.688027618409251e-06
対照学習損失: 0.00014153568190522492
{'loss': 0.0001, 'grad_norm': 0.003024414647370577, 'learning_rate': 3.935018050541516e-06, 'epoch': 2.8}
対照学習損失: 9.76758383330889e-06
対照学習損失: 2.3326101654674858e-05
{'loss': 0.0, 'grad_norm': 0.0005760038620792329, 'learning_rate': 3.916967509025271e-06, 'epoch': 2.8}
対照学習損失: 1.1093619832536206e-05
対照学習損失: 8.055524085648358e-05
{'loss': 0.0, 'grad_norm': 0.0027361796237528324, 'learning_rate': 3.898916967509026e-06, 'epoch': 2.8}
対照学習損失: 2.397456410108134e-05
対照学習損失: 2.3594111553393304e-05
{'loss': 0.0, 'grad_norm': 0.0013301742728799582, 'learning_rate': 3.88086642599278e-06, 'epoch': 2.8}
対照学習損失: 2.8427422876120545e-05
対照学習損失: 0.00021381625265348703
{'loss': 0.0001, 'grad_norm': 0.011357397772371769, 'learning_rate': 3.862815884476535e-06, 'epoch': 2.8}
対照学習損失: 9.745202987687662e-06
対照学習損失: 5.108998084324412e-05
{'loss': 0.0, 'grad_norm': 0.0014341064961627126, 'learning_rate': 3.844765342960289e-06, 'epoch': 2.8}
対照学習損失: 8.903356501832604e-06
対照学習損失: 6.467773346230388e-05
{'loss': 0.0, 'grad_norm': 0.001188755384646356, 'learning_rate': 3.826714801444043e-06, 'epoch': 2.81}
対照学習損失: 8.580832945881411e-05
対照学習損失: 1.388764849252766e-05
{'loss': 0.0, 'grad_norm': 0.0029106037691235542, 'learning_rate': 3.808664259927798e-06, 'epoch': 2.81}
対照学習損失: 0.0011819226201623678
対照学習損失: 1.2509256521298084e-05
{'loss': 0.0006, 'grad_norm': 0.05430421233177185, 'learning_rate': 3.7906137184115523e-06, 'epoch': 2.81}
対照学習損失: 8.806493497104384e-06
対照学習損失: 4.8292837163899094e-05
{'loss': 0.0, 'grad_norm': 0.0013801264576613903, 'learning_rate': 3.7725631768953074e-06, 'epoch': 2.81}
対照学習損失: 7.286625987035222e-06
対照学習損失: 0.00014385825488716364
{'loss': 0.0001, 'grad_norm': 0.004369370639324188, 'learning_rate': 3.7545126353790616e-06, 'epoch': 2.81}
対照学習損失: 0.00039684530929662287
対照学習損失: 0.0001158149098046124
{'loss': 0.0003, 'grad_norm': 0.012379801832139492, 'learning_rate': 3.736462093862816e-06, 'epoch': 2.81}
対照学習損失: 1.887179314508103e-05
対照学習損失: 8.024199814826716e-06
{'loss': 0.0, 'grad_norm': 0.00034050288377329707, 'learning_rate': 3.7184115523465705e-06, 'epoch': 2.81}
対照学習損失: 6.740623211953789e-05
対照学習損失: 4.515787441050634e-05
{'loss': 0.0001, 'grad_norm': 0.0022367474157363176, 'learning_rate': 3.7003610108303248e-06, 'epoch': 2.81}
対照学習損失: 2.573285746620968e-05
対照学習損失: 8.240240276791155e-06
{'loss': 0.0, 'grad_norm': 0.0015139851020649076, 'learning_rate': 3.68231046931408e-06, 'epoch': 2.81}
対照学習損失: 0.00019367145432624966
対照学習損失: 8.263991185231134e-05
{'loss': 0.0001, 'grad_norm': 0.016137314960360527, 'learning_rate': 3.664259927797834e-06, 'epoch': 2.81}
対照学習損失: 2.2230618924368173e-05
対照学習損失: 2.3326789232669398e-05
{'loss': 0.0, 'grad_norm': 0.0009186833631247282, 'learning_rate': 3.6462093862815884e-06, 'epoch': 2.81}
対照学習損失: 0.00028486971859820187
対照学習損失: 2.209741433034651e-05
{'loss': 0.0002, 'grad_norm': 0.009456484578549862, 'learning_rate': 3.6281588447653434e-06, 'epoch': 2.82}
対照学習損失: 0.00047002994688227773
対照学習損失: 6.988331733737141e-05
{'loss': 0.0003, 'grad_norm': 0.015538240782916546, 'learning_rate': 3.6101083032490977e-06, 'epoch': 2.82}
対照学習損失: 9.044839316629805e-06
対照学習損失: 4.865207301918417e-06
{'loss': 0.0, 'grad_norm': 0.00019362461171112955, 'learning_rate': 3.592057761732852e-06, 'epoch': 2.82}
対照学習損失: 0.00011371327855158597
対照学習損失: 4.2306437535444275e-05
{'loss': 0.0001, 'grad_norm': 0.0030363178811967373, 'learning_rate': 3.574007220216607e-06, 'epoch': 2.82}
対照学習損失: 8.076361154962797e-06
対照学習損失: 7.022292993497103e-05
{'loss': 0.0, 'grad_norm': 0.002137307543307543, 'learning_rate': 3.5559566787003613e-06, 'epoch': 2.82}
対照学習損失: 5.904881254537031e-05
対照学習損失: 0.00013762603339273483
{'loss': 0.0001, 'grad_norm': 0.004118176177144051, 'learning_rate': 3.537906137184116e-06, 'epoch': 2.82}
対照学習損失: 9.134127139986958e-06
対照学習損失: 6.633431621594355e-05
{'loss': 0.0, 'grad_norm': 0.0016403853660449386, 'learning_rate': 3.51985559566787e-06, 'epoch': 2.82}
対照学習損失: 0.00043548300163820386
対照学習損失: 1.735161458782386e-05
{'loss': 0.0002, 'grad_norm': 0.022875066846609116, 'learning_rate': 3.5018050541516244e-06, 'epoch': 2.82}
対照学習損失: 8.231132960645482e-05
対照学習損失: 2.7669253540807404e-05
{'loss': 0.0001, 'grad_norm': 0.0038566356524825096, 'learning_rate': 3.4837545126353795e-06, 'epoch': 2.82}
対照学習損失: 0.00015359051758423448
対照学習損失: 2.3408067136188038e-05
{'loss': 0.0001, 'grad_norm': 0.010115319862961769, 'learning_rate': 3.4657039711191337e-06, 'epoch': 2.82}
対照学習損失: 1.7739472241373733e-05
対照学習損失: 0.00014728025416843593
{'loss': 0.0001, 'grad_norm': 0.003471882315352559, 'learning_rate': 3.447653429602888e-06, 'epoch': 2.82}
対照学習損失: 4.989296940038912e-05
対照学習損失: 1.087770488084061e-05
{'loss': 0.0, 'grad_norm': 0.0012126128422096372, 'learning_rate': 3.429602888086643e-06, 'epoch': 2.83}
対照学習損失: 8.292448910651729e-06
対照学習損失: 2.8718284738715738e-05
{'loss': 0.0, 'grad_norm': 0.0009529853705316782, 'learning_rate': 3.4115523465703973e-06, 'epoch': 2.83}
対照学習損失: 2.3355847588391043e-05
対照学習損失: 9.097055226447992e-06
{'loss': 0.0, 'grad_norm': 0.00046906655188649893, 'learning_rate': 3.393501805054152e-06, 'epoch': 2.83}
対照学習損失: 0.051964446902275085
対照学習損失: 5.461247383209411e-06
{'loss': 0.026, 'grad_norm': 1.1726181507110596, 'learning_rate': 3.375451263537906e-06, 'epoch': 2.83}
対照学習損失: 8.977840479929e-06
対照学習損失: 9.640357893658802e-05
{'loss': 0.0001, 'grad_norm': 0.0028064928483217955, 'learning_rate': 3.3574007220216604e-06, 'epoch': 2.83}
対照学習損失: 1.4230353372113314e-05
対照学習損失: 3.521511098369956e-05
{'loss': 0.0, 'grad_norm': 0.0013229070464149117, 'learning_rate': 3.3393501805054155e-06, 'epoch': 2.83}
対照学習損失: 8.642544344183989e-06
対照学習損失: 0.00016685709124431014
{'loss': 0.0001, 'grad_norm': 0.007347804959863424, 'learning_rate': 3.3212996389891698e-06, 'epoch': 2.83}
対照学習損失: 0.0002572500961832702
対照学習損失: 9.27584642340662e-06
{'loss': 0.0001, 'grad_norm': 0.008353750221431255, 'learning_rate': 3.303249097472924e-06, 'epoch': 2.83}
対照学習損失: 8.751501445658505e-05
対照学習損失: 4.267716940375976e-05
{'loss': 0.0001, 'grad_norm': 0.0027469447813928127, 'learning_rate': 3.285198555956679e-06, 'epoch': 2.83}
対照学習損失: 3.008901148859877e-05
対照学習損失: 3.176049722242169e-05
{'loss': 0.0, 'grad_norm': 0.000986853032372892, 'learning_rate': 3.2671480144404333e-06, 'epoch': 2.83}
対照学習損失: 3.856770126731135e-05
対照学習損失: 0.00035262093297205865
{'loss': 0.0002, 'grad_norm': 0.01655268482863903, 'learning_rate': 3.2490974729241876e-06, 'epoch': 2.83}
対照学習損失: 8.542318391846493e-05
対照学習損失: 4.87555589643307e-05
{'loss': 0.0001, 'grad_norm': 0.0024491753429174423, 'learning_rate': 3.2310469314079427e-06, 'epoch': 2.84}
対照学習損失: 0.00010632724297465757
対照学習損失: 2.6448144126334228e-05
{'loss': 0.0001, 'grad_norm': 0.002846726216375828, 'learning_rate': 3.212996389891697e-06, 'epoch': 2.84}
対照学習損失: 7.00063246767968e-05
対照学習損失: 8.232231630245224e-05
{'loss': 0.0001, 'grad_norm': 0.0023840542417019606, 'learning_rate': 3.1949458483754516e-06, 'epoch': 2.84}
対照学習損失: 6.539468449773267e-05
対照学習損失: 6.131778718554415e-06
{'loss': 0.0, 'grad_norm': 0.00603075698018074, 'learning_rate': 3.176895306859206e-06, 'epoch': 2.84}
対照学習損失: 0.002481894800439477
対照学習損失: 4.584073758451268e-05
{'loss': 0.0013, 'grad_norm': 0.1071159839630127, 'learning_rate': 3.15884476534296e-06, 'epoch': 2.84}
対照学習損失: 4.550228550215252e-05
対照学習損失: 4.969133442500606e-05
{'loss': 0.0, 'grad_norm': 0.005599904805421829, 'learning_rate': 3.140794223826715e-06, 'epoch': 2.84}
対照学習損失: 0.00010051946446765214
対照学習損失: 8.659715240355581e-05
{'loss': 0.0001, 'grad_norm': 0.0030103966128081083, 'learning_rate': 3.1227436823104694e-06, 'epoch': 2.84}
対照学習損失: 0.000198241657926701
対照学習損失: 0.0001617914967937395
{'loss': 0.0002, 'grad_norm': 0.015985975041985512, 'learning_rate': 3.104693140794224e-06, 'epoch': 2.84}
対照学習損失: 1.9512381186359562e-05
対照学習損失: 0.00015805396833457053
{'loss': 0.0001, 'grad_norm': 0.005284399259835482, 'learning_rate': 3.0866425992779787e-06, 'epoch': 2.84}
対照学習損失: 1.1972882930422202e-05
対照学習損失: 7.595804345328361e-05
{'loss': 0.0, 'grad_norm': 0.001672550686635077, 'learning_rate': 3.068592057761733e-06, 'epoch': 2.84}
対照学習損失: 7.185649155871943e-05
対照学習損失: 0.00014157910482026637
{'loss': 0.0001, 'grad_norm': 0.0065337312407791615, 'learning_rate': 3.0505415162454876e-06, 'epoch': 2.84}
対照学習損失: 1.6882240743143484e-05
対照学習損失: 1.6807895008241758e-05
{'loss': 0.0, 'grad_norm': 0.0011110133491456509, 'learning_rate': 3.0324909747292423e-06, 'epoch': 2.85}
対照学習損失: 6.012578978698002e-06
対照学習損失: 1.8133916455553845e-05
{'loss': 0.0, 'grad_norm': 0.0006422753795050085, 'learning_rate': 3.0144404332129965e-06, 'epoch': 2.85}
対照学習損失: 0.0003499116573948413
対照学習損失: 5.349474577087676e-06
{'loss': 0.0002, 'grad_norm': 0.013096357695758343, 'learning_rate': 2.9963898916967508e-06, 'epoch': 2.85}
対照学習損失: 3.6375364288687706e-05
対照学習損失: 5.468683866638457e-06
{'loss': 0.0, 'grad_norm': 0.0016799818258732557, 'learning_rate': 2.9783393501805054e-06, 'epoch': 2.85}
対照学習損失: 1.2575990695040673e-05
対照学習損失: 2.016039979935158e-05
{'loss': 0.0, 'grad_norm': 0.000586845853831619, 'learning_rate': 2.96028880866426e-06, 'epoch': 2.85}
対照学習損失: 6.334403587970883e-05
対照学習損失: 4.29895999332075e-06
{'loss': 0.0, 'grad_norm': 0.001973460428416729, 'learning_rate': 2.9422382671480143e-06, 'epoch': 2.85}
対照学習損失: 2.5442655896767974e-05
対照学習損失: 5.207769208936952e-05
{'loss': 0.0, 'grad_norm': 0.0014449970331043005, 'learning_rate': 2.924187725631769e-06, 'epoch': 2.85}
対照学習損失: 6.140838377177715e-05
対照学習損失: 2.267035233671777e-05
{'loss': 0.0, 'grad_norm': 0.003215193282812834, 'learning_rate': 2.9061371841155237e-06, 'epoch': 2.85}
対照学習損失: 1.705396789475344e-05
対照学習損失: 2.6522746338741854e-05
{'loss': 0.0, 'grad_norm': 0.0005378862260840833, 'learning_rate': 2.8880866425992783e-06, 'epoch': 2.85}
対照学習損失: 2.1606207155855373e-05
対照学習損失: 9.022373888001312e-06
{'loss': 0.0, 'grad_norm': 0.0003651111328508705, 'learning_rate': 2.8700361010830326e-06, 'epoch': 2.85}
対照学習損失: 1.1883441402460448e-05
対照学習損失: 0.0007684074807912111
{'loss': 0.0004, 'grad_norm': 0.027179216966032982, 'learning_rate': 2.8519855595667872e-06, 'epoch': 2.85}
対照学習損失: 0.0002751579449977726
対照学習損失: 2.135997056029737e-05
{'loss': 0.0001, 'grad_norm': 0.024488884955644608, 'learning_rate': 2.833935018050542e-06, 'epoch': 2.86}
対照学習損失: 0.00018873233057092875
対照学習損失: 1.748567410686519e-05
{'loss': 0.0001, 'grad_norm': 0.008296054787933826, 'learning_rate': 2.815884476534296e-06, 'epoch': 2.86}
対照学習損失: 5.1778057240881026e-05
対照学習損失: 3.447995550232008e-05
{'loss': 0.0, 'grad_norm': 0.0017900716047734022, 'learning_rate': 2.7978339350180504e-06, 'epoch': 2.86}
対照学習損失: 3.878506686305627e-05
対照学習損失: 0.0002756213943939656
{'loss': 0.0002, 'grad_norm': 0.005921924486756325, 'learning_rate': 2.779783393501805e-06, 'epoch': 2.86}
対照学習損失: 0.0003252003516536206
対照学習損失: 4.575585262500681e-05
{'loss': 0.0002, 'grad_norm': 0.012096458114683628, 'learning_rate': 2.7617328519855597e-06, 'epoch': 2.86}
対照学習損失: 2.722214958339464e-05
対照学習損失: 8.43396355776349e-06
{'loss': 0.0, 'grad_norm': 0.0006726151914335787, 'learning_rate': 2.7436823104693144e-06, 'epoch': 2.86}
対照学習損失: 0.0002188479556934908
対照学習損失: 1.370124300592579e-05
{'loss': 0.0001, 'grad_norm': 0.006066306494176388, 'learning_rate': 2.7256317689530686e-06, 'epoch': 2.86}
対照学習損失: 2.9097602237015963e-05
対照学習損失: 3.818569894065149e-05
{'loss': 0.0, 'grad_norm': 0.00131592049729079, 'learning_rate': 2.7075812274368233e-06, 'epoch': 2.86}
対照学習損失: 0.015150250867009163
対照学習損失: 1.786604661901947e-05
{'loss': 0.0076, 'grad_norm': 1.3951479196548462, 'learning_rate': 2.689530685920578e-06, 'epoch': 2.86}
対照学習損失: 0.00031940487679094076
対照学習損失: 3.944738273276016e-05
{'loss': 0.0002, 'grad_norm': 0.012147380039095879, 'learning_rate': 2.6714801444043326e-06, 'epoch': 2.86}
対照学習損失: 5.135868559591472e-05
対照学習損失: 0.0005402814713306725
{'loss': 0.0003, 'grad_norm': 0.017887119203805923, 'learning_rate': 2.653429602888087e-06, 'epoch': 2.86}
対照学習損失: 9.010900976136327e-05
対照学習損失: 5.136706749908626e-05
{'loss': 0.0001, 'grad_norm': 0.004434152971953154, 'learning_rate': 2.635379061371841e-06, 'epoch': 2.87}
対照学習損失: 8.700362377567217e-05
対照学習損失: 3.981017289333977e-05
{'loss': 0.0001, 'grad_norm': 0.002618782687932253, 'learning_rate': 2.6173285198555957e-06, 'epoch': 2.87}
対照学習損失: 0.0003234801406506449
対照学習損失: 3.8748250517528504e-05
{'loss': 0.0002, 'grad_norm': 0.025033140555024147, 'learning_rate': 2.5992779783393504e-06, 'epoch': 2.87}
対照学習損失: 1.979569424293004e-05
対照学習損失: 2.096439311571885e-05
{'loss': 0.0, 'grad_norm': 0.0006187267717905343, 'learning_rate': 2.5812274368231046e-06, 'epoch': 2.87}
対照学習損失: 2.0957746528438292e-05
対照学習損失: 2.0324298020568676e-05
{'loss': 0.0, 'grad_norm': 0.0010044975206255913, 'learning_rate': 2.5631768953068593e-06, 'epoch': 2.87}
対照学習損失: 1.2330327990639489e-05
対照学習損失: 6.139243851066567e-06
{'loss': 0.0, 'grad_norm': 0.00034897594014182687, 'learning_rate': 2.545126353790614e-06, 'epoch': 2.87}
対照学習損失: 0.00012601456546690315
対照学習損失: 0.0004959629732184112
{'loss': 0.0003, 'grad_norm': 0.014895879663527012, 'learning_rate': 2.527075812274368e-06, 'epoch': 2.87}
対照学習損失: 1.072870145435445e-05
対照学習損失: 2.3266280550160445e-05
{'loss': 0.0, 'grad_norm': 0.000651111186016351, 'learning_rate': 2.509025270758123e-06, 'epoch': 2.87}
対照学習損失: 6.087090696382802e-06
対照学習損失: 0.0001594281493453309
{'loss': 0.0001, 'grad_norm': 0.003246717154979706, 'learning_rate': 2.4909747292418775e-06, 'epoch': 2.87}
対照学習損失: 8.970384442363866e-06
対照学習損失: 4.037131293443963e-05
{'loss': 0.0, 'grad_norm': 0.0014247896615415812, 'learning_rate': 2.4729241877256318e-06, 'epoch': 2.87}
対照学習損失: 1.0698733603931032e-05
対照学習損失: 5.245171450951602e-06
{'loss': 0.0, 'grad_norm': 0.00020470710296649486, 'learning_rate': 2.4548736462093864e-06, 'epoch': 2.87}
対照学習損失: 1.363429328193888e-05
対照学習損失: 0.0001513542520115152
{'loss': 0.0001, 'grad_norm': 0.0036114794202148914, 'learning_rate': 2.4368231046931407e-06, 'epoch': 2.88}
対照学習損失: 2.1776824723929167e-05
対照学習損失: 0.0006843406008556485
{'loss': 0.0004, 'grad_norm': 0.03114303946495056, 'learning_rate': 2.4187725631768953e-06, 'epoch': 2.88}
対照学習損失: 0.003253795439377427
対照学習損失: 7.383989577647299e-05
{'loss': 0.0017, 'grad_norm': 0.10904335230588913, 'learning_rate': 2.40072202166065e-06, 'epoch': 2.88}
対照学習損失: 1.1399253708077595e-05
対照学習損失: 0.0001654511725064367
{'loss': 0.0001, 'grad_norm': 0.006352621130645275, 'learning_rate': 2.3826714801444043e-06, 'epoch': 2.88}
対照学習損失: 3.1060411856742576e-05
対照学習損失: 0.00016105934628285468
{'loss': 0.0001, 'grad_norm': 0.0042052241042256355, 'learning_rate': 2.364620938628159e-06, 'epoch': 2.88}
対照学習損失: 6.726825085934252e-05
対照学習損失: 0.00013549314462579787
{'loss': 0.0001, 'grad_norm': 0.005100879818201065, 'learning_rate': 2.3465703971119136e-06, 'epoch': 2.88}
対照学習損失: 8.948042705014814e-06
対照学習損失: 1.2002482435491402e-05
{'loss': 0.0, 'grad_norm': 0.0003115422441624105, 'learning_rate': 2.3285198555956682e-06, 'epoch': 2.88}
対照学習損失: 3.0276643883553334e-05
対照学習損失: 2.4413635401288047e-05
{'loss': 0.0, 'grad_norm': 0.0007682078867219388, 'learning_rate': 2.3104693140794225e-06, 'epoch': 2.88}
対照学習損失: 4.3291031033732e-05
対照学習損失: 0.00019849659292958677
{'loss': 0.0001, 'grad_norm': 0.004293251782655716, 'learning_rate': 2.292418772563177e-06, 'epoch': 2.88}
対照学習損失: 1.0661486157914624e-05
対照学習損失: 2.0152721845079213e-05
{'loss': 0.0, 'grad_norm': 0.0007572912727482617, 'learning_rate': 2.2743682310469314e-06, 'epoch': 2.88}
対照学習損失: 7.656835077796131e-05
対照学習損失: 0.00011682330659823492
{'loss': 0.0001, 'grad_norm': 0.006281102541834116, 'learning_rate': 2.256317689530686e-06, 'epoch': 2.88}
対照学習損失: 8.433831681031734e-06
対照学習損失: 3.875999027513899e-05
{'loss': 0.0, 'grad_norm': 0.0020195466931909323, 'learning_rate': 2.2382671480144403e-06, 'epoch': 2.89}
対照学習損失: 5.0597533117979765e-05
対照学習損失: 0.00010196049697697163
{'loss': 0.0001, 'grad_norm': 0.0038493573665618896, 'learning_rate': 2.220216606498195e-06, 'epoch': 2.89}
対照学習損失: 1.1384316167095676e-05
対照学習損失: 3.691342499223538e-05
{'loss': 0.0, 'grad_norm': 0.0011866921558976173, 'learning_rate': 2.2021660649819496e-06, 'epoch': 2.89}
対照学習損失: 2.6850742870010436e-05
対照学習損失: 0.0003154033038299531
{'loss': 0.0002, 'grad_norm': 0.010147235356271267, 'learning_rate': 2.1841155234657043e-06, 'epoch': 2.89}
対照学習損失: 0.00026331108529120684
対照学習損失: 8.128505214699544e-06
{'loss': 0.0001, 'grad_norm': 0.007091281469911337, 'learning_rate': 2.1660649819494585e-06, 'epoch': 2.89}
対照学習損失: 6.556830339832231e-05
対照学習損失: 8.970433555077761e-06
{'loss': 0.0, 'grad_norm': 0.0021172803826630116, 'learning_rate': 2.148014440433213e-06, 'epoch': 2.89}
対照学習損失: 4.865943992626853e-05
対照学習損失: 9.238588972948492e-06
{'loss': 0.0, 'grad_norm': 0.0026320277247577906, 'learning_rate': 2.129963898916968e-06, 'epoch': 2.89}
対照学習損失: 2.0107892851228826e-05
対照学習損失: 5.030662941862829e-05
{'loss': 0.0, 'grad_norm': 0.004604654386639595, 'learning_rate': 2.111913357400722e-06, 'epoch': 2.89}
対照学習損失: 4.673889998230152e-05
対照学習損失: 8.098695616354235e-06
{'loss': 0.0, 'grad_norm': 0.0011174647370353341, 'learning_rate': 2.0938628158844768e-06, 'epoch': 2.89}
対照学習損失: 1.6539393982384354e-05
対照学習損失: 2.6238554710289463e-05
{'loss': 0.0, 'grad_norm': 0.000690692977514118, 'learning_rate': 2.075812274368231e-06, 'epoch': 2.89}
対照学習損失: 0.00012783281272277236
対照学習損失: 0.00012308753503020853
{'loss': 0.0001, 'grad_norm': 0.004734416026622057, 'learning_rate': 2.0577617328519857e-06, 'epoch': 2.9}
対照学習損失: 9.22373237699503e-06
対照学習損失: 1.1957930837525055e-05
{'loss': 0.0, 'grad_norm': 0.000253165460890159, 'learning_rate': 2.03971119133574e-06, 'epoch': 2.9}
対照学習損失: 4.570438613882288e-05
対照学習損失: 2.7938087441725656e-05
{'loss': 0.0, 'grad_norm': 0.0010704952292144299, 'learning_rate': 2.0216606498194946e-06, 'epoch': 2.9}
対照学習損失: 1.3611728718387894e-05
対照学習損失: 3.391970312804915e-05
{'loss': 0.0, 'grad_norm': 0.0006782221025787294, 'learning_rate': 2.0036101083032492e-06, 'epoch': 2.9}
対照学習損失: 1.5250804608513135e-05
対照学習損失: 1.4222357094695326e-05
{'loss': 0.0, 'grad_norm': 0.000386293773772195, 'learning_rate': 1.985559566787004e-06, 'epoch': 2.9}
対照学習損失: 0.00036385099519975483
対照学習損失: 2.898036291298922e-05
{'loss': 0.0002, 'grad_norm': 0.011867506429553032, 'learning_rate': 1.967509025270758e-06, 'epoch': 2.9}
対照学習損失: 3.986300725955516e-05
対照学習損失: 1.4684756024507806e-05
{'loss': 0.0, 'grad_norm': 0.0019199036760255694, 'learning_rate': 1.949458483754513e-06, 'epoch': 2.9}
対照学習損失: 0.001377592678181827
対照学習損失: 2.137483716069255e-05
{'loss': 0.0007, 'grad_norm': 0.041684456169605255, 'learning_rate': 1.9314079422382675e-06, 'epoch': 2.9}
対照学習損失: 2.3416323529090732e-05
対照学習損失: 0.0004846893425565213
{'loss': 0.0003, 'grad_norm': 0.019326437264680862, 'learning_rate': 1.9133574007220217e-06, 'epoch': 2.9}
対照学習損失: 9.629673149902374e-05
対照学習損失: 3.501761057123076e-06
{'loss': 0.0, 'grad_norm': 0.008028660900890827, 'learning_rate': 1.8953068592057762e-06, 'epoch': 2.9}
対照学習損失: 1.2203782716824207e-05
対照学習損失: 0.00034241462708450854
{'loss': 0.0002, 'grad_norm': 0.011044672690331936, 'learning_rate': 1.8772563176895308e-06, 'epoch': 2.9}
対照学習損失: 7.227667811093852e-05
対照学習損失: 1.4520850527333096e-05
{'loss': 0.0, 'grad_norm': 0.0015974376583471894, 'learning_rate': 1.8592057761732853e-06, 'epoch': 2.91}
対照学習損失: 0.0002503081923350692
対照学習損失: 0.00011870599701069295
{'loss': 0.0002, 'grad_norm': 0.007683188188821077, 'learning_rate': 1.84115523465704e-06, 'epoch': 2.91}
対照学習損失: 1.5482004528166726e-05
対照学習損失: 1.4371919860423077e-05
{'loss': 0.0, 'grad_norm': 0.0003265590057708323, 'learning_rate': 1.8231046931407942e-06, 'epoch': 2.91}
対照学習損失: 0.00027853873325511813
対照学習損失: 5.848653927387204e-06
{'loss': 0.0001, 'grad_norm': 0.020768357440829277, 'learning_rate': 1.8050541516245488e-06, 'epoch': 2.91}
対照学習損失: 7.115248990885448e-06
対照学習損失: 1.4982833818066865e-05
{'loss': 0.0, 'grad_norm': 0.000300232000881806, 'learning_rate': 1.7870036101083035e-06, 'epoch': 2.91}
対照学習損失: 2.1308023860910907e-05
対照学習損失: 9.647160186432302e-05
{'loss': 0.0001, 'grad_norm': 0.002633023774251342, 'learning_rate': 1.768953068592058e-06, 'epoch': 2.91}
対照学習損失: 2.5413020921405405e-05
対照学習損失: 6.504307748400606e-06
{'loss': 0.0, 'grad_norm': 0.00041396525921300054, 'learning_rate': 1.7509025270758122e-06, 'epoch': 2.91}
対照学習損失: 9.58136806730181e-05
対照学習損失: 9.68561835179571e-06
{'loss': 0.0001, 'grad_norm': 0.003417117753997445, 'learning_rate': 1.7328519855595669e-06, 'epoch': 2.91}
対照学習損失: 9.24045525607653e-05
対照学習損失: 3.4479075111448765e-05
{'loss': 0.0001, 'grad_norm': 0.001980734756216407, 'learning_rate': 1.7148014440433215e-06, 'epoch': 2.91}
対照学習損失: 4.86529097543098e-05
対照学習損失: 4.0118684410117567e-05
{'loss': 0.0, 'grad_norm': 0.0015628136461600661, 'learning_rate': 1.696750902527076e-06, 'epoch': 2.91}
対照学習損失: 6.787432539567817e-06
対照学習損失: 6.166805542306975e-05
{'loss': 0.0, 'grad_norm': 0.0009670605068095028, 'learning_rate': 1.6787003610108302e-06, 'epoch': 2.91}
対照学習損失: 0.00015938497381284833
対照学習損失: 3.6211535189067945e-05
{'loss': 0.0001, 'grad_norm': 0.004754169378429651, 'learning_rate': 1.6606498194945849e-06, 'epoch': 2.92}
対照学習損失: 0.00023793693981133401
対照学習損失: 8.903379239200149e-06
{'loss': 0.0001, 'grad_norm': 0.005923969205468893, 'learning_rate': 1.6425992779783395e-06, 'epoch': 2.92}
対照学習損失: 2.6507586881052703e-05
対照学習損失: 1.6316218534484506e-05
{'loss': 0.0, 'grad_norm': 0.0007151652243919671, 'learning_rate': 1.6245487364620938e-06, 'epoch': 2.92}
対照学習損失: 9.730353667691816e-06
対照学習損失: 0.0004394447023514658
{'loss': 0.0002, 'grad_norm': 0.015214035287499428, 'learning_rate': 1.6064981949458485e-06, 'epoch': 2.92}
対照学習損失: 6.45215914119035e-06
対照学習損失: 0.0033782869577407837
{'loss': 0.0017, 'grad_norm': 0.09795816242694855, 'learning_rate': 1.588447653429603e-06, 'epoch': 2.92}
対照学習損失: 1.4118653780315071e-05
対照学習損失: 1.232293652719818e-05
{'loss': 0.0, 'grad_norm': 0.0002866670547518879, 'learning_rate': 1.5703971119133576e-06, 'epoch': 2.92}
対照学習損失: 9.073255932889879e-05
対照学習損失: 2.188895086874254e-05
{'loss': 0.0001, 'grad_norm': 0.0023074031341820955, 'learning_rate': 1.552346570397112e-06, 'epoch': 2.92}
対照学習損失: 2.0033856344525702e-05
対照学習損失: 2.416063580312766e-05
{'loss': 0.0, 'grad_norm': 0.0006295416387729347, 'learning_rate': 1.5342960288808665e-06, 'epoch': 2.92}
対照学習損失: 2.8762760848621838e-05
対照学習損失: 8.933073331718333e-06
{'loss': 0.0, 'grad_norm': 0.0015640899073332548, 'learning_rate': 1.5162454873646211e-06, 'epoch': 2.92}
対照学習損失: 4.325441477703862e-05
対照学習損失: 0.0002543301088735461
{'loss': 0.0001, 'grad_norm': 0.010074499994516373, 'learning_rate': 1.4981949458483754e-06, 'epoch': 2.92}
対照学習損失: 0.00013022315397392958
対照学習損失: 1.1831243682536297e-05
{'loss': 0.0001, 'grad_norm': 0.009710729122161865, 'learning_rate': 1.48014440433213e-06, 'epoch': 2.92}
対照学習損失: 4.2582913010846823e-05
対照学習損失: 2.0733776182169095e-05
{'loss': 0.0, 'grad_norm': 0.000832935213111341, 'learning_rate': 1.4620938628158845e-06, 'epoch': 2.93}
対照学習損失: 0.0015069172950461507
対照学習損失: 1.622601121198386e-05
{'loss': 0.0008, 'grad_norm': 0.03842056170105934, 'learning_rate': 1.4440433212996392e-06, 'epoch': 2.93}
対照学習損失: 8.866009011399001e-06
対照学習損失: 0.0001331428502453491
{'loss': 0.0001, 'grad_norm': 0.0033314917236566544, 'learning_rate': 1.4259927797833936e-06, 'epoch': 2.93}
対照学習損失: 0.00786503590643406
対照学習損失: 2.117348776664585e-05
{'loss': 0.0039, 'grad_norm': 0.19369930028915405, 'learning_rate': 1.407942238267148e-06, 'epoch': 2.93}
対照学習損失: 9.387649697600864e-06
対照学習損失: 0.000476223009172827
{'loss': 0.0002, 'grad_norm': 0.008516956120729446, 'learning_rate': 1.3898916967509025e-06, 'epoch': 2.93}
対照学習損失: 5.736901584896259e-06
対照学習損失: 0.00012430603965185583
{'loss': 0.0001, 'grad_norm': 0.0035945959389209747, 'learning_rate': 1.3718411552346572e-06, 'epoch': 2.93}
対照学習損失: 0.001881423406302929
対照学習損失: 5.9380745369708166e-06
{'loss': 0.0009, 'grad_norm': 0.07237061858177185, 'learning_rate': 1.3537906137184116e-06, 'epoch': 2.93}
対照学習損失: 1.5295558114303276e-05
対照学習損失: 1.206968863698421e-05
{'loss': 0.0, 'grad_norm': 0.0003461987944319844, 'learning_rate': 1.3357400722021663e-06, 'epoch': 2.93}
対照学習損失: 0.00011694659042404965
対照学習損失: 8.776613867667038e-06
{'loss': 0.0001, 'grad_norm': 0.003147429320961237, 'learning_rate': 1.3176895306859205e-06, 'epoch': 2.93}
対照学習損失: 4.352955147624016e-05
対照学習損失: 2.6737885491456836e-05
{'loss': 0.0, 'grad_norm': 0.0013048278633505106, 'learning_rate': 1.2996389891696752e-06, 'epoch': 2.93}
対照学習損失: 1.2762588085024618e-05
対照学習損失: 2.3632303054910153e-05
{'loss': 0.0, 'grad_norm': 0.0004294264072086662, 'learning_rate': 1.2815884476534297e-06, 'epoch': 2.93}
対照学習損失: 1.9654100469779223e-05
対照学習損失: 0.0006103626219555736
{'loss': 0.0003, 'grad_norm': 0.01983710750937462, 'learning_rate': 1.263537906137184e-06, 'epoch': 2.94}
対照学習損失: 0.00044149596942588687
対照学習損失: 0.0021941722370684147
{'loss': 0.0013, 'grad_norm': 0.07343519479036331, 'learning_rate': 1.2454873646209388e-06, 'epoch': 2.94}
対照学習損失: 0.002003819914534688
対照学習損失: 7.726172043476254e-06
{'loss': 0.001, 'grad_norm': 0.07390958070755005, 'learning_rate': 1.2274368231046932e-06, 'epoch': 2.94}
対照学習損失: 3.2303069019690156e-05
対照学習損失: 1.0706176908570342e-05
{'loss': 0.0, 'grad_norm': 0.0007256758981384337, 'learning_rate': 1.2093862815884477e-06, 'epoch': 2.94}
対照学習損失: 0.00024012532958295196
対照学習損失: 6.465005571953952e-05
{'loss': 0.0002, 'grad_norm': 0.008082900196313858, 'learning_rate': 1.1913357400722021e-06, 'epoch': 2.94}
対照学習損失: 5.326098471414298e-05
対照学習損失: 3.180386920575984e-05
{'loss': 0.0, 'grad_norm': 0.001437315484508872, 'learning_rate': 1.1732851985559568e-06, 'epoch': 2.94}
対照学習損失: 9.804843102756422e-06
対照学習損失: 4.389979221741669e-05
{'loss': 0.0, 'grad_norm': 0.0015616965247318149, 'learning_rate': 1.1552346570397112e-06, 'epoch': 2.94}
対照学習損失: 8.9481174654793e-05
対照学習損失: 0.00011136009561596438
{'loss': 0.0001, 'grad_norm': 0.008792540989816189, 'learning_rate': 1.1371841155234657e-06, 'epoch': 2.94}
対照学習損失: 7.569716217403766e-06
対照学習損失: 2.337811929464806e-05
{'loss': 0.0, 'grad_norm': 0.0004900105413980782, 'learning_rate': 1.1191335740072201e-06, 'epoch': 2.94}
対照学習損失: 8.8288170445594e-06
対照学習損失: 2.2537044060300104e-05
{'loss': 0.0, 'grad_norm': 0.0005328365368768573, 'learning_rate': 1.1010830324909748e-06, 'epoch': 2.94}
対照学習損失: 5.781613253930118e-06
対照学習損失: 2.1464198653120548e-05
{'loss': 0.0, 'grad_norm': 0.000399667740566656, 'learning_rate': 1.0830324909747293e-06, 'epoch': 2.94}
対照学習損失: 2.9092308977851644e-05
対照学習損失: 0.0003097538137808442
{'loss': 0.0002, 'grad_norm': 0.012424156069755554, 'learning_rate': 1.064981949458484e-06, 'epoch': 2.95}
対照学習損失: 9.961233445210382e-06
対照学習損失: 5.4906297009438276e-05
{'loss': 0.0, 'grad_norm': 0.0016771270893514156, 'learning_rate': 1.0469314079422384e-06, 'epoch': 2.95}
対照学習損失: 1.5079302102094516e-05
対照学習損失: 9.240456711268052e-05
{'loss': 0.0001, 'grad_norm': 0.003216077573597431, 'learning_rate': 1.0288808664259928e-06, 'epoch': 2.95}
対照学習損失: 1.0177282092627138e-05
対照学習損失: 1.5086459825397469e-05
{'loss': 0.0, 'grad_norm': 0.0009426942560821772, 'learning_rate': 1.0108303249097473e-06, 'epoch': 2.95}
対照学習損失: 3.9059985283529386e-05
対照学習損失: 3.790461414610036e-05
{'loss': 0.0, 'grad_norm': 0.0016292048385366797, 'learning_rate': 9.92779783393502e-07, 'epoch': 2.95}
対照学習損失: 4.855060615227558e-05
対照学習損失: 0.001536101452074945
{'loss': 0.0008, 'grad_norm': 0.04284515604376793, 'learning_rate': 9.747292418772564e-07, 'epoch': 2.95}
対照学習損失: 7.465405360562727e-06
対照学習損失: 1.9683917344082147e-05
{'loss': 0.0, 'grad_norm': 0.0004246864700689912, 'learning_rate': 9.566787003610109e-07, 'epoch': 2.95}
対照学習損失: 4.194663233647589e-06
対照学習損失: 5.5012664233800024e-05
{'loss': 0.0, 'grad_norm': 0.0016011596890166402, 'learning_rate': 9.386281588447654e-07, 'epoch': 2.95}
対照学習損失: 6.746046710759401e-05
対照学習損失: 2.670188041520305e-05
{'loss': 0.0, 'grad_norm': 0.0017216770211234689, 'learning_rate': 9.2057761732852e-07, 'epoch': 2.95}
対照学習損失: 0.000234323539189063
対照学習損失: 2.477011366863735e-05
{'loss': 0.0001, 'grad_norm': 0.007443216163665056, 'learning_rate': 9.025270758122744e-07, 'epoch': 2.95}
対照学習損失: 4.038198312628083e-06
対照学習損失: 3.0118986614979804e-05
{'loss': 0.0, 'grad_norm': 0.0021954572293907404, 'learning_rate': 8.84476534296029e-07, 'epoch': 2.95}
対照学習損失: 2.3230360966408625e-05
対照学習損失: 2.8899159588036127e-05
{'loss': 0.0, 'grad_norm': 0.0005943991709500551, 'learning_rate': 8.664259927797834e-07, 'epoch': 2.96}
対照学習損失: 1.2762557162204757e-05
対照学習損失: 0.00022438084124587476
{'loss': 0.0001, 'grad_norm': 0.005792274139821529, 'learning_rate': 8.48375451263538e-07, 'epoch': 2.96}
対照学習損失: 0.0003740313113667071
対照学習損失: 2.9651755539816804e-05
{'loss': 0.0002, 'grad_norm': 0.014730414375662804, 'learning_rate': 8.303249097472924e-07, 'epoch': 2.96}
対照学習損失: 1.2061968845955562e-05
対照学習損失: 6.928918992343824e-06
{'loss': 0.0, 'grad_norm': 0.0003133228456135839, 'learning_rate': 8.122743682310469e-07, 'epoch': 2.96}
対照学習損失: 2.1941068553132936e-05
対照学習損失: 7.240164268296212e-05
{'loss': 0.0, 'grad_norm': 0.0032637803815305233, 'learning_rate': 7.942238267148015e-07, 'epoch': 2.96}
対照学習損失: 0.00013420643517747521
対照学習損失: 3.2443360396428034e-05
{'loss': 0.0001, 'grad_norm': 0.0038537951186299324, 'learning_rate': 7.76173285198556e-07, 'epoch': 2.96}
対照学習損失: 1.2881704606115818e-05
対照学習損失: 0.003090867307037115
{'loss': 0.0016, 'grad_norm': 0.1261528879404068, 'learning_rate': 7.581227436823106e-07, 'epoch': 2.96}
対照学習損失: 0.0001416967570548877
対照学習損失: 2.0346951714600436e-05
{'loss': 0.0001, 'grad_norm': 0.004197315778583288, 'learning_rate': 7.40072202166065e-07, 'epoch': 2.96}
対照学習損失: 4.4767133658751845e-05
対照学習損失: 1.539187906018924e-05
{'loss': 0.0, 'grad_norm': 0.0011322483187541366, 'learning_rate': 7.220216606498196e-07, 'epoch': 2.96}
対照学習損失: 1.1078780516982079e-05
対照学習損失: 1.032628097163979e-05
{'loss': 0.0, 'grad_norm': 0.0002675534342415631, 'learning_rate': 7.03971119133574e-07, 'epoch': 2.96}
対照学習損失: 3.9040819501678925e-06
対照学習損失: 7.435584848280996e-06
{'loss': 0.0, 'grad_norm': 0.00012179822806501761, 'learning_rate': 6.859205776173286e-07, 'epoch': 2.96}
対照学習損失: 6.371366907842457e-05
対照学習損失: 0.005472247488796711
{'loss': 0.0028, 'grad_norm': 0.5209981203079224, 'learning_rate': 6.678700361010831e-07, 'epoch': 2.97}
対照学習損失: 1.8864275261876173e-05
対照学習損失: 0.00021826293959748
{'loss': 0.0001, 'grad_norm': 0.0054449662566185, 'learning_rate': 6.498194945848376e-07, 'epoch': 2.97}
対照学習損失: 6.988547738728812e-06
対照学習損失: 0.00037369964411482215
{'loss': 0.0002, 'grad_norm': 0.013337159529328346, 'learning_rate': 6.31768953068592e-07, 'epoch': 2.97}
対照学習損失: 0.00011076944065280259
対照学習損失: 0.0002502065326552838
{'loss': 0.0002, 'grad_norm': 0.01398232951760292, 'learning_rate': 6.137184115523466e-07, 'epoch': 2.97}
対照学習損失: 1.2479476936277933e-05
対照学習損失: 5.237727691564942e-06
{'loss': 0.0, 'grad_norm': 0.00023196589609142393, 'learning_rate': 5.956678700361011e-07, 'epoch': 2.97}
対照学習損失: 1.125770177168306e-05
対照学習損失: 3.218975689378567e-05
{'loss': 0.0, 'grad_norm': 0.0014590087812393904, 'learning_rate': 5.776173285198556e-07, 'epoch': 2.97}
対照学習損失: 2.74307112704264e-05
対照学習損失: 3.143999492749572e-05
{'loss': 0.0, 'grad_norm': 0.0016766159096732736, 'learning_rate': 5.595667870036101e-07, 'epoch': 2.97}
対照学習損失: 1.939327921718359e-05
対照学習損失: 9.491904165770393e-06
{'loss': 0.0, 'grad_norm': 0.00033722451189532876, 'learning_rate': 5.415162454873646e-07, 'epoch': 2.97}
対照学習損失: 2.2209167582332157e-05
対照学習損失: 3.905714402208105e-05
{'loss': 0.0, 'grad_norm': 0.0010378430597484112, 'learning_rate': 5.234657039711192e-07, 'epoch': 2.97}
対照学習損失: 1.9705861632246524e-05
対照学習損失: 7.963847747305408e-05
{'loss': 0.0, 'grad_norm': 0.0020110218320041895, 'learning_rate': 5.054151624548736e-07, 'epoch': 2.97}
対照学習損失: 0.0017740706680342555
対照学習損失: 0.00016087954281829298
{'loss': 0.001, 'grad_norm': 0.05732284113764763, 'learning_rate': 4.873646209386282e-07, 'epoch': 2.97}
対照学習損失: 2.0242177924956195e-05
対照学習損失: 8.717084710951895e-06
{'loss': 0.0, 'grad_norm': 0.0003912851389031857, 'learning_rate': 4.693140794223827e-07, 'epoch': 2.98}
対照学習損失: 3.04707427858375e-05
対照学習損失: 2.335614772164263e-05
{'loss': 0.0, 'grad_norm': 0.0007280844729393721, 'learning_rate': 4.512635379061372e-07, 'epoch': 2.98}
対照学習損失: 4.803460615221411e-05
対照学習損失: 1.0005799595091958e-05
{'loss': 0.0, 'grad_norm': 0.0011558852856978774, 'learning_rate': 4.332129963898917e-07, 'epoch': 2.98}
対照学習損失: 2.4240685888798907e-05
対照学習損失: 9.597842290531844e-05
{'loss': 0.0001, 'grad_norm': 0.002798918867483735, 'learning_rate': 4.151624548736462e-07, 'epoch': 2.98}
対照学習損失: 0.0001167032532976009
対照学習損失: 5.86928945267573e-05
{'loss': 0.0001, 'grad_norm': 0.0024614178109914064, 'learning_rate': 3.971119133574007e-07, 'epoch': 2.98}
対照学習損失: 2.270113691338338e-05
対照学習損失: 3.255573028582148e-05
{'loss': 0.0, 'grad_norm': 0.0008408346911892295, 'learning_rate': 3.790613718411553e-07, 'epoch': 2.98}
対照学習損失: 0.00010223769641015679
対照学習損失: 0.00011256967991357669
{'loss': 0.0001, 'grad_norm': 0.009091584011912346, 'learning_rate': 3.610108303249098e-07, 'epoch': 2.98}
対照学習損失: 1.3589408808911685e-05
対照学習損失: 0.00011803409870481119
{'loss': 0.0001, 'grad_norm': 0.0021920388098806143, 'learning_rate': 3.429602888086643e-07, 'epoch': 2.98}
対照学習損失: 3.389058110769838e-05
対照学習損失: 0.00020430059521459043
{'loss': 0.0001, 'grad_norm': 0.006503842771053314, 'learning_rate': 3.249097472924188e-07, 'epoch': 2.98}
対照学習損失: 2.3676606360822916e-05
対照学習損失: 6.565623334608972e-05
{'loss': 0.0, 'grad_norm': 0.004605701193213463, 'learning_rate': 3.068592057761733e-07, 'epoch': 2.98}
対照学習損失: 1.7620013750274666e-05
対照学習損失: 1.0475390809006058e-05
{'loss': 0.0, 'grad_norm': 0.0004517201450653374, 'learning_rate': 2.888086642599278e-07, 'epoch': 2.98}
対照学習損失: 4.574637841869844e-06
対照学習損失: 4.2321549699408934e-05
{'loss': 0.0, 'grad_norm': 0.0008056609076447785, 'learning_rate': 2.707581227436823e-07, 'epoch': 2.99}
対照学習損失: 1.5369418179034255e-05
対照学習損失: 1.8580965843284503e-05
{'loss': 0.0, 'grad_norm': 0.0005322292563505471, 'learning_rate': 2.527075812274368e-07, 'epoch': 2.99}
対照学習損失: 6.4670293795643374e-06
対照学習損失: 6.776793452445418e-05
{'loss': 0.0, 'grad_norm': 0.0024169115349650383, 'learning_rate': 2.3465703971119135e-07, 'epoch': 2.99}
対照学習損失: 4.409634129842743e-05
対照学習損失: 0.0005087918252684176
{'loss': 0.0003, 'grad_norm': 0.032333288341760635, 'learning_rate': 2.1660649819494586e-07, 'epoch': 2.99}
対照学習損失: 3.477443533483893e-05
対照学習損失: 2.772649168036878e-05
{'loss': 0.0, 'grad_norm': 0.0012891462538391352, 'learning_rate': 1.9855595667870036e-07, 'epoch': 2.99}
対照学習損失: 1.645789780013729e-05
対照学習損失: 2.710279295570217e-05
{'loss': 0.0, 'grad_norm': 0.0008678593439981341, 'learning_rate': 1.805054151624549e-07, 'epoch': 2.99}
対照学習損失: 0.00010685109737096354
対照学習損失: 0.00012622540816664696
{'loss': 0.0001, 'grad_norm': 0.006861288566142321, 'learning_rate': 1.624548736462094e-07, 'epoch': 2.99}
対照学習損失: 0.00014786502288188785
対照学習損失: 1.0430627298774198e-05
{'loss': 0.0001, 'grad_norm': 0.004878437612205744, 'learning_rate': 1.444043321299639e-07, 'epoch': 2.99}
対照学習損失: 1.0869951438507996e-05
対照学習損失: 1.87527130037779e-05
{'loss': 0.0, 'grad_norm': 0.0003969809622503817, 'learning_rate': 1.263537906137184e-07, 'epoch': 2.99}
対照学習損失: 9.59614226303529e-06
対照学習損失: 5.0877533794846386e-05
{'loss': 0.0, 'grad_norm': 0.0011010309681296349, 'learning_rate': 1.0830324909747293e-07, 'epoch': 2.99}
対照学習損失: 5.1734139560721815e-05
対照学習損失: 4.243512375978753e-05
{'loss': 0.0, 'grad_norm': 0.0021425967570394278, 'learning_rate': 9.025270758122745e-08, 'epoch': 2.99}
対照学習損失: 3.061828829231672e-05
対照学習損失: 8.57552822708385e-06
{'loss': 0.0, 'grad_norm': 0.0006508735241368413, 'learning_rate': 7.220216606498195e-08, 'epoch': 3.0}
対照学習損失: 1.8289900253876112e-05
対照学習損失: 0.0003423070884309709
{'loss': 0.0002, 'grad_norm': 0.009824337437748909, 'learning_rate': 5.4151624548736464e-08, 'epoch': 3.0}
対照学習損失: 5.027820952818729e-05
対照学習損失: 0.00015413729124702513
{'loss': 0.0001, 'grad_norm': 0.0037913438864052296, 'learning_rate': 3.6101083032490976e-08, 'epoch': 3.0}
対照学習損失: 3.339600516483188e-05
対照学習損失: 0.00020069186575710773
{'loss': 0.0001, 'grad_norm': 0.01451488584280014, 'learning_rate': 1.8050541516245488e-08, 'epoch': 3.0}
対照学習損失: 4.6938184823375195e-06
対照学習損失: 5.447341754916124e-05
対照学習損失: 1.0013312930823304e-05
対照学習損失: 5.900799351366004e-06
対照学習損失: 3.2302719773724675e-05
対照学習損失: 3.3229471227969043e-06
対照学習損失: 9.758333908393979e-05
対照学習損失: 3.5911693885282148e-06
対照学習損失: 2.327370020793751e-05
対照学習損失: 0.00018702067609410733
対照学習損失: 2.3394788968289504e-06
対照学習損失: 1.6017955204006284e-05
対照学習損失: 1.9669475932460045e-06
対照学習損失: 0.001540690427646041
対照学習損失: 7.2418861236656085e-06
対照学習損失: 2.3692784907325404e-06
対照学習損失: 3.635856046457775e-06
対照学習損失: 1.8565864593256265e-05
対照学習損失: 3.4868594411818776e-06
対照学習損失: 0.00046493267291225493
対照学習損失: 0.0003201838117092848
対照学習損失: 1.4140850908006541e-05
対照学習損失: 1.5795213812452857e-06
対照学習損失: 2.5493509383522905e-05
対照学習損失: 4.813050054508494e-06
対照学習損失: 1.162269472843036e-05
対照学習損失: 2.9355205697356723e-06
対照学習損失: 1.5795208128110971e-06
対照学習損失: 3.5017469599551987e-06
対照学習損失: 1.9371459529793356e-06
対照学習損失: 0.00295957550406456
対照学習損失: 3.933888365281746e-06
対照学習損失: 1.728531515254872e-06
対照学習損失: 2.9889117286074907e-05
対照学習損失: 0.00036298029590398073
対照学習損失: 2.026553147516097e-06
対照学習損失: 2.5033900783455465e-06
対照学習損失: 0.000136162867420353
対照学習損失: 7.4057797974091955e-06
対照学習損失: 2.443782477712375e-06
対照学習損失: 3.099263267358765e-05
対照学習損失: 1.1801526852650568e-05
対照学習損失: 0.0003509356756694615
対照学習損失: 6.735261649737367e-06
対照学習損失: 2.5927961360139307e-06
対照学習損失: 0.00016993765893857926
対照学習損失: 3.591164841054706e-06
対照学習損失: 1.9668193999677896e-05
対照学習損失: 3.6060669117432553e-06
対照学習損失: 1.1473886161184055e-06
対照学習損失: 2.0861562006757595e-06
対照学習損失: 0.0001091831290978007
対照学習損失: 1.564550620969385e-05
対照学習損失: 5.736913408327382e-06
対照学習損失: 9.760105967870913e-06
対照学習損失: 5.8640413044486195e-05
対照学習損失: 6.794839464419056e-06
対照学習損失: 2.2335905669024214e-05
対照学習損失: 1.4408914466912393e-05
対照学習損失: 7.52498544898117e-06
対照学習損失: 9.134329957305454e-06
対照学習損失: 8.368212002096698e-05
対照学習損失: 0.0011589672649279237
対照学習損失: 4.872634235653095e-06
対照学習損失: 0.04165644198656082
対照学習損失: 3.397447471797932e-06
対照学習損失: 7.8975344877108e-06
対照学習損失: 7.584586001030402e-06
対照学習損失: 1.3961752301838715e-05
対照学習損失: 0.0035644301678985357
対照学習損失: 1.832839870985481e-06
対照学習損失: 1.065409196598921e-05
対照学習損失: 8.433892617176753e-06
対照学習損失: 7.808139343978837e-06
対照学習損失: 6.213699634827208e-06
対照学習損失: 7.077957889123354e-06
対照学習損失: 9.030026376422029e-06
対照学習損失: 4.784116390510462e-05
対照学習損失: 1.3321354344952852e-05
対照学習損失: 1.7434314258935046e-06
対照学習損失: 1.7732340893417131e-06
対照学習損失: 6.52666585665429e-06
対照学習損失: 2.175564077333547e-06
対照学習損失: 4.672311115427874e-05
対照学習損失: 4.0382037695962936e-06
対照学習損失: 2.861017719624215e-06
対照学習損失: 0.0003260228259023279
対照学習損失: 3.1722287531010807e-05
対照学習損失: 1.111611709347926e-05
対照学習損失: 2.0265533748897724e-06
対照学習損失: 4.9271351599600166e-05
対照学習損失: 5.721994966734201e-06
対照学習損失: 9.089515515370294e-06
対照学習損失: 4.693849859904731e-06
対照学習損失: 2.1755631678388454e-06
対照学習損失: 3.3241583878407255e-05
対照学習損失: 6.943898824829375e-06
対照学習損失: 8.2700680650305e-06
対照学習損失: 6.5266440287814476e-06
対照学習損失: 1.5615554730175063e-05
対照学習損失: 0.00017944211140275002
対照学習損失: 0.004413390066474676
対照学習損失: 1.725451511447318e-05
対照学習損失: 0.001006593694910407
対照学習損失: 1.2606228665390518e-05
対照学習損失: 0.00016111487639136612
対照学習損失: 1.3262017546367133e-06
対照学習損失: 3.561365247151116e-06
対照学習損失: 3.435853432165459e-05
対照学習損失: 0.0018933796091005206
対照学習損失: 2.1173424102016725e-05
対照学習損失: 1.2218786650919355e-05
対照学習損失: 2.90571460936917e-06
対照学習損失: 3.534171264618635e-05
対照学習損失: 6.00513430981664e-06
対照学習損失: 1.3008326277486049e-05
対照学習損失: 9.208729352394585e-06
対照学習損失: 3.2125302823260427e-05
対照学習損失: 4.90246202389244e-06
対照学習損失: 0.00017202348681166768
対照学習損失: 3.251260204706341e-05
対照学習損失: 4.798125701199751e-06
対照学習損失: 1.3709055792787694e-06
対照学習損失: 7.271718459378462e-06
対照学習損失: 8.582989721617196e-06
対照学習損失: 2.8534264856716618e-05
対照学習損失: 5.498457994690398e-06
対照学習損失: 3.3080418688768987e-06
対照学習損失: 0.0003790202608797699
対照学習損失: 4.768346570926951e-06
対照学習損失: 1.4796510185988154e-05
対照学習損失: 1.0937250408460386e-05
対照学習損失: 0.0050049456767737865
対照学習損失: 1.5795189938216936e-06
対照学習損失: 4.589537638821639e-06
対照学習損失: 1.0013363862526603e-05
対照学習損失: 4.09717540605925e-05
対照学習損失: 6.884292361064581e-06
対照学習損失: 5.662401235895231e-06
対照学習損失: 3.4150976716773584e-05
対照学習損失: 8.94068989509833e-07
対照学習損失: 8.582952432334423e-06
対照学習損失: 3.218639221813646e-06
対照学習損失: 5.78162962483475e-06
対照学習損失: 8.582897862652317e-06
対照学習損失: 0.0001846498780651018
対照学習損失: 5.423985385277774e-06
対照学習損失: 4.7930749133229256e-05
対照学習損失: 1.0549864782660734e-05
対照学習損失: 1.952048478415236e-06
対照学習損失: 8.866109055816196e-06
対照学習損失: 0.00037341041024774313
対照学習損失: 1.1205424925719853e-05
対照学習損失: 1.7969909094972536e-05
対照学習損失: 1.7552534700371325e-05
対照学習損失: 4.276602339814417e-06
対照学習損失: 2.413981292193057e-06
対照学習損失: 1.1458823792054318e-05
対照学習損失: 3.069631929975003e-06
対照学習損失: 3.464332621661015e-05
対照学習損失: 2.8312019821896683e-06
対照学習損失: 4.395809355628444e-06
対照学習損失: 9.566475455358159e-06
対照学習損失: 4.306389655539533e-06
対照学習損失: 2.339477759960573e-06
対照学習損失: 8.49346997711109e-06
対照学習損失: 2.2351689494826132e-06
対照学習損失: 1.740407242323272e-05
対照学習損失: 8.255166903836653e-06
対照学習損失: 7.57090310798958e-05
対照学習損失: 3.6458601243793964e-05
対照学習損失: 1.9028013412025757e-05
対照学習損失: 1.7434319943276932e-06
対照学習損失: 5.155778580956394e-06
対照学習損失: 4.276611434761435e-06
対照学習損失: 7.241823823278537e-06
対照学習損失: 9.849444722931366e-06
対照学習損失: 3.2931434361671563e-06
対照学習損失: 1.4468313565885182e-05
対照学習損失: 3.039827106476878e-06
対照学習損失: 0.00033504943712614477
対照学習損失: 1.9340775907039642e-05
対照学習損失: 1.2740144484268967e-05
対照学習損失: 6.243546977202641e-06
対照学習損失: 0.00018483359599485993
対照学習損失: 2.3394754862238187e-06
対照学習損失: 7.212069249362685e-06
対照学習損失: 1.251696403414826e-06
対照学習損失: 0.0010610929457470775
対照学習損失: 7.778335202601738e-06
対照学習損失: 2.8908150397910504e-06
対照学習損失: 0.0022917548194527626
対照学習損失: 1.801439248083625e-05
対照学習損失: 5.5133950809249654e-06
対照学習損失: 4.827436350751668e-05
対照学習損失: 8.31475972518092e-06
対照学習損失: 2.9802179142279783e-06
対照学習損失: 3.6805720355914673e-06
対照学習損失: 6.679323996650055e-05
対照学習損失: 7.510132491006516e-06
対照学習損失: 6.139213382994058e-06
対照学習損失: 3.576265498850262e-06
対照学習損失: 9.834468983171973e-06
対照学習損失: 2.181447416660376e-05
対照学習損失: 2.9057189294690033e-06
対照学習損失: 2.62993107753573e-05
対照学習損失: 0.0003623698139563203
対照学習損失: 2.6730334866442718e-05
対照学習損失: 1.5675705071771517e-05
対照学習損失: 0.0004198387614451349
対照学習損失: 2.041455445578322e-06
対照学習損失: 7.152396392484661e-06
対照学習損失: 5.051440894021653e-06
対照学習損失: 6.377651970979059e-06
対照学習損失: 1.5243294910760596e-05
対照学習損失: 7.614351488882676e-06
対照学習損失: 0.002462028991430998
対照学習損失: 3.760714753298089e-05
対照学習損失: 1.1637514035101049e-05
対照学習損失: 2.1680063582607545e-05
対照学習損失: 3.361287963343784e-05
対照学習損失: 3.993495738541242e-06
対照学習損失: 2.7252379368292168e-05
対照学習損失: 4.380922291602474e-06
対照学習損失: 2.197830326622352e-05
対照学習損失: 2.011652895816951e-06
対照学習損失: 0.0003699497028719634
対照学習損失: 2.041455445578322e-06
対照学習損失: 6.228621259651845e-06
対照学習損失: 1.1637648640316911e-05
対照学習損失: 9.813523502089083e-05
対照学習損失: 1.780646198312752e-05
対照学習損失: 6.853910599602386e-05
対照学習損失: 1.0698871847125702e-05
対照学習損失: 3.904075128957629e-06
対照学習損失: 4.268727934686467e-05
対照学習損失: 3.053007094422355e-05
対照学習損失: 4.740753138321452e-05
対照学習損失: 5.4835913942952175e-06
対照学習損失: 1.9802489987341687e-05
対照学習損失: 0.0012660798383876681
対照学習損失: 1.801482358132489e-05
対照学習損失: 1.4975386875448748e-05
対照学習損失: 1.2292951396375429e-05
対照学習損失: 0.0016880362527444959
対照学習損失: 0.0002186986239394173
対照学習損失: 0.0002662148035597056
対照学習損失: 9.849521120486315e-06
対照学習損失: 4.470327439776156e-06
対照学習損失: 4.976952368451748e-06
対照学習損失: 4.127598003833555e-06
対照学習損失: 0.00033174536656588316
対照学習損失: 0.01311558298766613
対照学習損失: 0.001116570900194347
対照学習損失: 1.1428761354181916e-05
対照学習損失: 1.8030373212241102e-06
対照学習損失: 3.531563152137096e-06
対照学習損失: 7.212084710772615e-06
対照学習損失: 6.375034718075767e-05
対照学習損失: 1.7343963918392546e-05
対照学習損失: 6.09528724453412e-05
対照学習損失: 5.468686140375212e-06
対照学習損失: 1.0996691344189458e-05
対照学習損失: 2.568753552623093e-05
対照学習損失: 1.9132299712509848e-05
対照学習損失: 5.4543175792787224e-05
対照学習損失: 3.620968072937103e-06
対照学習損失: 2.026554284384474e-06
対照学習損失: 1.07881314761471e-05
対照学習損失: 0.00030819326639175415
対照学習損失: 3.0249275368987583e-06
対照学習損失: 0.00019951123977079988
対照学習損失: 4.485232238948811e-06
対照学習損失: 6.557891902048141e-05
対照学習損失: 3.3080395951401442e-06
対照学習損失: 7.158130028983578e-05
対照学習損失: 1.8775424450723222e-06
対照学習損失: 3.0696223802806344e-06
対照学習損失: 1.2621034329640679e-05
対照学習損失: 0.005764084868133068
対照学習損失: 3.69521658285521e-05
対照学習損失: 6.556454081874108e-06
対照学習損失: 1.54070949065499e-05
対照学習損失: 9.685750228527468e-07
対照学習損失: 2.309674073330825e-06
対照学習損失: 2.69710199063411e-06
対照学習損失: 0.00012371761840768158
対照学習損失: 4.634233846445568e-06
対照学習損失: 2.600156221888028e-05
対照学習損失: 0.00020719578606076539
対照学習損失: 5.945526481809793e-06
対照学習損失: 2.250067609566031e-06
対照学習損失: 0.0010029809782281518
対照学習損失: 1.0192198715230916e-05
対照学習損失: 6.362740350596141e-06
対照学習損失: 2.7579284505918622e-05
対照学習損失: 1.8030368664767593e-06
対照学習損失: 1.8775431271933485e-06
対照学習損失: 2.7716048407455673e-06
対照学習損失: 8.538255315215793e-06
対照学習損失: 7.495265890611336e-05
対照学習損失: 7.823035048204474e-06
対照学習損失: 2.2947729121369775e-06
対照学習損失: 3.0994349344837246e-06
対照学習損失: 3.357099194545299e-05
対照学習損失: 3.7252793845254928e-06
対照学習損失: 6.228642178029986e-06
対照学習損失: 4.7855071898084134e-05
対照学習損失: 0.00021069512877147645
対照学習損失: 1.1563128282432444e-05
対照学習損失: 1.5213726328511257e-05
対照学習損失: 1.415608835486637e-06
対照学習損失: 0.00012758973753079772
対照学習損失: 5.140863322594669e-06
対照学習損失: 4.112705482839374e-06
対照学習損失: 2.697105628612917e-06
対照学習損失: 1.7583349745109444e-06
対照学習損失: 5.5152475397335365e-05
対照学習損失: 2.5151412046398036e-05
対照学習損失: 1.2665975646086736e-06
対照学習損失: 3.501759920254699e-06
対照学習損失: 0.0026241776067763567
対照学習損失: 4.559733952191891e-06
対照学習損失: 7.688919140491635e-06
対照学習損失: 6.422279966500355e-06
対照学習損失: 1.4603119780076668e-06
対照学習損失: 0.00012760140816681087
対照学習損失: 6.139218385214917e-06
対照学習損失: 0.0035203853622078896
対照学習損失: 6.064741683076136e-06
対照学習損失: 0.0005119047709740698
対照学習損失: 3.5911657505494077e-06
対照学習損失: 7.46541718399385e-06
対照学習損失: 2.235169858977315e-06
対照学習損失: 1.543722828500904e-05
対照学習損失: 3.507613655528985e-05
対照学習損失: 1.3202192349126562e-05
対照学習損失: 1.8030384580924874e-06
対照学習損失: 5.334580691851443e-06
対照学習損失: 0.0029737779404968023
対照学習損失: 8.374355275009293e-06
対照学習損失: 8.137824625009671e-05
対照学習損失: 6.568476237589493e-05
対照学習損失: 0.0009951932588592172
対照学習損失: 7.599473519803723e-06
対照学習損失: 7.435581210302189e-06
対照学習損失: 2.5599027139833197e-05
対照学習損失: 3.889186700689606e-06
対照学習損失: 1.172693919215817e-05
対照学習損失: 6.1690334405284375e-06
対照学習損失: 7.618497329531237e-05
対照学習損失: 1.0430804877614719e-06
対照学習損失: 7.197215381893329e-06
対照学習損失: 3.547698725014925e-05
対照学習損失: 0.00015178626927081496
対照学習損失: 6.854450475657359e-06
対照学習損失: 6.675661552435486e-06
対照学習損失: 3.695466602948727e-06
対照学習損失: 0.02265620417892933
対照学習損失: 5.1855595302185975e-06
対照学習損失: 1.117558804253349e-05
対照学習損失: 2.2649733182333875e-06
対照学習損失: 7.652959175175056e-05
対照学習損失: 2.8759177439496852e-06
対照学習損失: 5.394172148953658e-06
対照学習損失: 1.746378438838292e-05
対照学習損失: 1.5795210401847726e-06
対照学習損失: 0.0027931940276175737
対照学習損失: 0.0006481739692389965
対照学習損失: 0.0012456419644877315
対照学習損失: 4.798133431904716e-06
対照学習損失: 1.8834103684639558e-05
対照学習損失: 1.527317363070324e-05
対照学習損失: 4.892676952295005e-05
対照学習損失: 7.659097718715202e-06
対照学習損失: 1.133964178734459e-05
対照学習損失: 3.732389450306073e-05
対照学習損失: 2.1069698050268926e-05
対照学習損失: 1.1920908491447335e-06
対照学習損失: 9.483082249062136e-05
対照学習損失: 1.7046120774466544e-05
対照学習損失: 4.5001256694376934e-06
対照学習損失: 0.007739030756056309
対照学習損失: 1.370886093354784e-05
対照学習損失: 1.0743515304056928e-05
対照学習損失: 4.082902705704328e-06
対照学習損失: 2.816309688569163e-06
対照学習損失: 4.8428446461912245e-06
対照学習損失: 6.213739652594086e-06
対照学習損失: 9.223693268722855e-06
対照学習損失: 2.7221823984291404e-05
対照学習損失: 1.271023393201176e-05
対照学習損失: 2.6673008051147917e-06
対照学習損失: 9.163946742773987e-06
対照学習損失: 1.7344567822874524e-05
対照学習損失: 0.001278925803489983
対照学習損失: 3.1214662158163264e-05
対照学習損失: 6.0795628087362275e-06
対照学習損失: 7.763311259623151e-06
対照学習損失: 1.117575175157981e-05
対照学習損失: 1.3709050108445808e-06
対照学習損失: 2.455587855365593e-05
対照学習損失: 1.1905789506272413e-05
対照学習損失: 0.0002392403839621693
対照学習損失: 9.059654985321686e-06
対照学習損失: 6.804692384321243e-05
対照学習損失: 2.2604204787057824e-05
対照学習損失: 4.976960553904064e-06
対照学習損失: 3.5670131182996556e-05
対照学習損失: 1.5347615772043355e-05
対照学習損失: 1.7031656170729548e-05
対照学習損失: 4.4107187022746075e-06
対照学習損失: 0.00074514199513942
対照学習損失: 1.2799905562133063e-05
対照学習損失: 6.98857729730662e-06
対照学習損失: 1.2248536222614348e-05
対照学習損失: 1.9520487057889113e-06
対照学習損失: 2.3245700049301377e-06
対照学習損失: 2.0295083231758326e-05
対照学習損失: 4.395819814817514e-06
対照学習損失: 7.645295409020036e-05
対照学習損失: 9.000180398288649e-06
対照学習損失: 1.306792637478793e-05
対照学習損失: 0.001137988525442779
対照学習損失: 4.3063982957392e-06
対照学習損失: 8.28499105409719e-06
対照学習損失: 1.5481542504858226e-05
対照学習損失: 8.076272024482023e-06
対照学習損失: 0.00013878129539079964
対照学習損失: 1.2844595403294079e-05
対照学習損失: 5.663645424647257e-05
対照学習損失: 6.154122729640221e-06
対照学習損失: 7.077946975186933e-06
対照学習損失: 8.419082405453082e-06
対照学習損失: 4.142504621995613e-06
対照学習損失: 5.543185125134187e-06
対照学習損失: 2.0697287254733965e-05
対照学習損失: 2.3005826733424328e-05
対照学習損失: 3.0067267289268784e-05
対照学習損失: 1.8328399846723187e-06
対照学習損失: 9.023238089866936e-05
対照学習損失: 9.030023647937924e-06
対照学習損失: 2.1561259927693754e-05
対照学習損失: 5.8263021855964325e-06
対照学習損失: 0.000426720071118325
対照学習損失: 5.796494406240527e-06
対照学習損失: 8.135912139550783e-06
対照学習損失: 5.208714719628915e-05
対照学習損失: 7.882654244895093e-06
対照学習損失: 1.001339296635706e-05
対照学習損失: 2.62398043560097e-05
対照学習損失: 0.0002674857387319207
対照学習損失: 0.00018858582188840955
対照学習損失: 8.32966179586947e-06
対照学習損失: 6.859528366476297e-05
対照学習損失: 8.985302883957047e-06
対照学習損失: 6.0498032325995155e-06
対照学習損失: 2.622598913148977e-06
対照学習損失: 0.00012037457781843841
対照学習損失: 0.003835252718999982
対照学習損失: 6.481891432486009e-06
対照学習損失: 3.099186869803816e-05
対照学習損失: 2.410919842077419e-05
対照学習損失: 6.735284841852263e-06
対照学習損失: 3.3676419661787804e-06
対照学習損失: 1.227805478265509e-05
対照学習損失: 4.008385076303966e-06
対照学習損失: 1.4245350939745549e-05
対照学習損失: 7.92725222709123e-06
対照学習損失: 2.8610165827558376e-06
対照学習損失: 7.498390914406627e-05
対照学習損失: 0.0008852332248352468
対照学習損失: 2.902509004343301e-05
対照学習損失: 2.27829186769668e-05
対照学習損失: 1.1920908491447335e-06
対照学習損失: 8.389196409552824e-06
対照学習損失: 1.2218940810271306e-06
対照学習損失: 0.00011503676796564832
対照学習損失: 7.152477337513119e-06
対照学習損失: 3.576251174308709e-06
対照学習損失: 3.367643785168184e-06
対照学習損失: 3.2633413411531365e-06
対照学習損失: 5.677286480931798e-06
対照学習損失: 3.829583420156268e-06
対照学習損失: 2.9802316703353426e-07
{'eval_loss': 0.0003344401193317026, 'eval_runtime': 53.9968, 'eval_samples_per_second': 71.782, 'eval_steps_per_second': 8.982, 'epoch': 3.0}
{'train_runtime': 1999.71, 'train_samples_per_second': 52.332, 'train_steps_per_second': 1.635, 'train_loss': 0.03263783143645754, 'epoch': 3.0}
